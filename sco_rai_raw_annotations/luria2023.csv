,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{luria2023, author = {Luria, Michal},
title = {Co-Design Perspectives on Algorithm Transparency Reporting: Guidelines and Prototypes},
year = {2023}}
", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-19, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,ConcernsAboutSocialMediaTransparency,Perceived_Problem,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,care about in the context of recommen-dation algorithm transparency on social media,
10,PositiveAttitudeTowardAlgorithims,Causal_Theory,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms.,
11,LowExpectationsForTransparency,Causal_Theory,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,what they imagine transparency report for recommendation algorithms would look like [...] very low expectations,
12,DesireToKnowHowPersonalDatIsUsed,Perceived_Need,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,How personal data is collected and used in suggesting content,
13,DesireForControlOverPlatform,Perceived_Need,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,what control users have over the data and what they see;,
14,DesireTokKnowWhatInformationIsCollected,Perceived_Need,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,what recommendation algorithm-related data is being gathered or shared outside the platform.,
15,LackOfInterestInAlgorithmGoals,Other_Precept,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,not interested in the goals of a particular algorithm.,
16,BeliefInUnderstanding,Causal_Theory,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,believed that they already “have a good understanding of what the goal is”,
17,SkepticismAboutBeneficiary,Causal_Theory,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,skepticism when asked about information on who benefits from an algorithm,
18,PerceptionOfInsincerity,Causal_Theory,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent,
19,DisappointmentAtPositivePortrayal,Perceived_Problem,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices.,
20,DesireForEvocativeExamples,Perceived_Need,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,Examples about how data and algorithms work on a particular platform,
21,EverydayUsers,Agent,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,everyday users,
22,Participants,Agent,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,participants,
23, , , , , ,
24,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
25,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
26,ConcernsAboutSocialMediaTransparency,constrainsAgent,EverydayUsers,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,what everyday users care about in the context of recommen-dation algorithm transparency on social media
27,PositiveAttitudeTowardAlgorithims,constrainsAgent,Participants,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms.
28,LowExpectationsForTransparency,constrainsAgent,Participants,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,"When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations"
29,DesireToKnowHowPersonalDatIsUsed,constrainsAgent,Participants,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,"Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content"
30,DesireForControlOverPlatform,constrainsAgent,Participants,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,"Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see;"
31,DesireTokKnowWhatInformationIsCollected,constrainsAgent,Participants,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,"Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform."
32,LackOfInterestInAlgorithmGoals,constrainsAgent,Participants,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,Most participants were not interested in the goals of a particular algorithm.
33,BeliefInUnderstanding,constrainsAgent,Participants,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,This is because they believed that they already “have a good understanding of what the goal is”
34,SkepticismAboutBeneficiary,constrainsAgent,Participants,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,participants expressed skepticism when asked about information on who benefits from an algorithm
35,PerceptionOfInsincerity,constrainsAgent,Participants,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent
36,DisappointmentAtPositivePortrayal,constrainsAgent,Participants,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices.
37,DesireForEvocativeExamples,constrainsAgent,Participants,"Participants were invited to participate in a remote 75-minute ses-sion with several design activities. The goal was to form a founda-tion of what everyday users care about in the context of recommen-dation algorithm transparency on social media. This foundation then served as a basis for the design of prototypes for Session 2.3.1.1 Participants. 30 participants were recruited via Prolific, an online research platform, and were compensated for their partic-ipation. We selected a diverse set of participants by age, gender,race, ability, education and political orientation. 15 participants identified as female, 14 identified as male, and one as non-binary.Seven participants identified as people of color. Seven participants reported being politically right-leaning, 20 left-leaning, and three neither. Three participants reported having a disability.3.1.2 Procedure. Session 1 was a 75-minute remote video call that consisted of three design activities: (1) Card sorting; (2) Sketching;and (3) Excerpt reflection. These activities were a mix of design approaches that allowed participants to engage in the topic of interest in different ways (e.g., reflect on existing content vs. sketch an idea). The activities were designed to prompt conversation, but also left space for unknown topics of importance that might surface as part of the co-design process.All activities were facilitated using a digital whiteboard software(“Mural”). Each participant had their own whiteboard set up for the study, and did not have access to other participants’ activity. To Ensure that the topics were understandable and legible, we piloted the designed procedure with five pilot participants who were not involved in the design of the study. Based on this pilot, we iterated on the activities’ descriptions, titles, and layout.Activity 1 - Card Sorting: Card sorting is a common design research practice that asks people to think through a topic by sorting cards—the cards serve as a tangible expression of ideas that are easy to organize, categorize and reflect on [86]. We used card sorting for participants to organize and discuss a range of topics related to recommendation algorithm transparency After a short introduction to the topic of the study, participantswere asked to organize cards on a scale from “most important for me to know about”, to “least important for me to know about” (Fig. 1)with three decks of cards: Aspects of recommendation algorithms,personal data types, and inference types. The goal was to prompt conversation about what should or should not be included in a transparency report about recommendation algorithms, based on the aspects that participants believed were important. We did so by interviewing participants throughout the activity, with questions such as “Why did you place x as most important?” or “Why do you think that y is least important for you to know about?” These types of questions are typical for card sorting activities [86].The topics that were included in the card decks were selected based on existing frameworks that laid out the topology of algorith-mic transparency ([39, 45, 62]). For example, some of the cards in the “Aspects of Recommendation Algorithms” deck included “What Are the goals of recommendation algorithms” and “How was the system developed and tested?” Using the study pilot, our research team iterated on the topics and card decks for logic and clarity. The Full list of cards and topics is attached as supplementary material.Activity 2 - Sketching a Report: Participants were asked to sketch what they would consider an ideal version of a recom-mendation algorithm transparency report. As established in prior work [43, 65] a sketching activity provides a creative outlet for participants to express their values, needs, and desires about a topic of research.Within the boundaries of digital communication, we formed an empty space on the board, with graphical elements and placeholder titles that participants could “drag and drop” for an easy start (seeFig. 2). After sketching, the facilitator asked participants to describe and explain what they had come up with (see examples in Fig. 3).Activity 3 - Excerpt Reflection: In the final activity, partici-pants were asked to read 3–4 several sentenced text excerpts related to recommendation algorithms from current social media platforms Privacy Policies pages. The goal was to identify how participants perceived current reporting practices. Here too, we followed up with questions such as “What are your impressions of the infor-mation presented here?”, and “Is there anything you would want to change about what is being shared in this text?” Excerpts per participant were randomly selected from a predefined set of 21 excerpts from large and mid-sized social media platforms. Each wasstripped of any information that would disclose the platform. The Full list of excerpts is attached as supplementary material.3.2 Session 2: Prototype FeedbackBased on Session 1 and in preparation for Session 2, we created prototypes of what a recommendation algorithm report could look like.Design researchers on the team identified patterns in Session 1, and relied on these to develop four prototypes that communicate and test new design directions for future social media recommendation algorithm transparency reports. These prototypes were created as provocations—primarily intended to generate a second conversa-tion with participants about their needs and values (as opposed to suggesting exactly how a report should look like). More details about the design process of the prototypes can be found in the supplementary materials.3.2.1 Participants. As is common in co-design processes, we in-vited all participants from Session 1 to participate in Session 2.Previous research on co-design processes suggested that it is worthwhile to include participants in multiple stages of the design process,as opposed to solely giving feedback at a single point in time [85]. Second session allowed participants to deepen their thinking about the topic and to re-examine their own ideas through tangible proto-types. A total of 16 participants (all of which participated in Session1) participated in Session 2, for additional compensation. Of those,seven participants identified as female, eight identified as male, and one as non-binary. A total of five participants were people of color. Five participants reported being politically right-leaning, ten were left-leaning, and two selected “neither.” One participant reported having a disability.3.2.2 Procedure. We used a standard methodology of ExperiencePrototyping for conducting Session 2 [13]. Experience prototyping is a method that presents prototypes of hypothetical user interfaces to people, and asks them to reflect on what they see. Further, these prototypes reflected ideas from Session 1, allowing participants to consider and validate the physical manifestation of their own and others' ideas (similar to the qualitative methodology of MemberChecking [15].) The four prototypes we generated were used as discussion probes in Session 2. All finalized prototypes are attached as supplementary material.Participants were invited to join 30-minute remote one-on-one sessions. All prototypes were presented through the whiteboard interface. Participants were encouraged to “think out loud,” and were asked open-ended follow-up questions such as: “What is your impression of the personalization of the report?” and “What do you think about the way that this information is presented?” Data from both sessions was transcribed and analyzed qualitatively using systematic thematic analysis [12]. Through an inductive process, qualitative codes were created based on about 50% of the data, until theme saturation was reached. The analysis resulted in a total of 143 codes, which were grouped into 38 high-level themes, using affinity diagramming [7]. The second half was coded deductively accordingly, and the first half re-coded as needed.The sketches that participants created in Session 1 were captured and analyzed using qualitative graphical annotation [11], where the researcher annotated graphical qualities in each sketch (referring to layout, content, structure, etc.) Most participants felt positively about the idea of social media recommendation algorithms (hereafter also referred to simply as“algorithms”) and understood that they are necessary in creating a personalized experience on social media platforms. As P24 put it,algorithms are “going to be involved no matter what...otherwise how are [social media platforms] going to be able to show me content that is relevant to me?”While algorithms themselves were met with fairly positive at-titudes, current reporting about how they work did not receive the same enthusiasm. When we asked people what they imagine transparency report for recommendation algorithms would look like, participants had very low expectations: “something that’s not really saying anything” (P10), “generic” (P15), “vague” (P4), or asP7 called it, “gobbledygook industry language”. Overall, they had a hard time conceptualizing a transparency report as something that would contain valuable information for them.Nevertheless, participants were interested in learning more about many aspects related to recommendation algorithms. For the rest of this section, we highlight topics that our findings suggest should be included and excluded, and how to best communicate them.4.1.1 What information should algorithmic transparency reports include? Participants were aware that algorithms can cause content to be hidden from them, for example, due to filtering, or if some content is shadowbanned [55]. Yet this was not a significant concern for them. Rather, they wanted to know more about what it is that they do see, and why. Within that, users pointed to three main topics that they thought should be covered in algorithmic transparency reports: (1) How personal data is collected and used in suggesting content; (2) what control users have over the data and what they see; and (3) what recommendation algorithm-related data is being gathered or shared outside the platform. How personal data is collected and used in suggesting con-tent: Of the many topics discussed, participants cared most about how their personal data is being collected and used in recommenda-tion algorithms. While for most, this was expected and even desired,current practices made many participants feel that “[social media platforms] are trying to find out as much as they can about you” (P1)and that they are “gathering and sharing more information than they should” (P27).Participants expressed a desire to have a report that would state the reasons for data collection and that would have clearly defined data collection boundaries. The reason is that people had a hard time seeing “how [some data collected] would be helpful based on what the [platform] does as a service” (P25). This was especially true in situations where platforms were making inferences about users,as opposed to using data that was explicitly shared with a platform.What control users have over the data and what they see:Participants wanted to know if they “have any input or say on how [the algorithm] works, or if it is all just up to the company” (P15).They agreed that they should “be able to have a choice of what to see and what to not see” (P24), and specifically asked for “[platforms] today whether or not [the user] can turn [something] off” (P15).When presented with existing text from Privacy Policies, someparticipants argued that platforms are intentionally trying to dis-courage user decision-making; P23 noted that the vague language about what can or cannot be controlled is almost as if companies were saying: “Well, if we don’t explain it as much, they probably won't do it [change things].” Thus, user control mechanisms should be an inherent part of an ideal algorithmic transparency report—just sharing what is being done is not enough. Instead, platforms need to explain what options and choices users have (or do not have) in regards to different functions.Data gathered or shared outside the platform: Participantsagreed that algorithmic transparency reports should include whether information gathered for content recommendation may also beshared with third parties. They also wanted to know if platforms obtain information from other sources to support their own rec-ommendations. Participants were generally concerned that they don't know much about how data is being transferred and used between platforms.The agreement among participants was that “as long as [plat-forms] are not pulling information from outside sources, that’s ok”(P27). P27 elaborated: “What I’m doing [on a platform] I don’t mind them knowing...but I don’t want them tracking me when I’m not ac-tually logged in and using their platform”. To address this concern,platforms should include detailed, specific and understandable infor-mation about data sharing collaborations they have with partners and third parties.4.1.2 What information should algorithmic transparency report minimize? Prior work suggests that providing maximal informa-tion can be overwhelming, and that transparency requires more nuanced and intentional choices for information sharing [3]. Along With topics that should be included in recommendation algorithm transparency reports, participants were also in agreement on as-pects that they were not as interested in. Namely, (1) the goals of a recommendation system, (2) who benefits from it, and (3) technical details. These topics were primarily met with skepticism and were not perceived as valuable to end-users in the study. We elaborate below on the topics that platforms might say less about.Recommendation algorithm goals: Most participants were not interested in the goals of a particular algorithm. This is because they believed that they already “have a good understanding of what the goal is” (P14): “[It is to] feed people content to engage with the social media platform” (P15) and “show you things that you would be interested in buying” (P23). One participant argued that “the goal is to make money. Anything [users] get out of it is a happy accident.”When platforms explain the goals of an algorithm, they should take their audience’s skepticism into consideration, and thus (a)provide a concise explanation of the multiple goals of a system(including the platform’s incentives, such as helping advertisers target interested users), and (b) back up any general claims with concrete information about how the goal is expressed in practice.Given participants’ low motivation to learn more, a platform with a different or more nuanced goal for its recommender system will need to take great care to explain that clearly and in a way that users would be willing to interact with.Who benefits from the algorithm: Similar to statements about algorithm goals, participants expressed skepticism when asked about information on who benefits from an algorithm: “who benefits from it? I already know that. It’s the advertisers and [platform], soI don’t care” (P28). Like with the algorithm goal, we recommend minimizing information about who the platform thinks benefits from the system, especially if the answer is the user. Users view such claims as insincere, even if somewhat true.Technical details: Participants shied away from explanations about how algorithms work, whether because they “do not care about that” (P10), or because they assumed they “would know what anything meant” (P15). P7 mentioned that “organizations often include that kind of information, and I skip it”.We recommend that platforms share technical information more intentionally tailored to audiences. While people with technical backgrounds, along with researchers, auditors and other groups may be interested in technical details about how systems work,everyday users may be less concerned with that. Instead, they might want to know more about how choices made for an algorithm design impact them. We recommend making these aspects the focus, with additional technical details shared separately for relevant target audiences, to reduce information overload.4.1.3 What style and language should algorithmic transparency reports use? Straightforward, data-driven, easy to understand lan-guage is best for algorithm transparency reports. When we presented participants with excerpts from Privacy Policies, any “mar-keting” language (“we use your information to provide and improve your experience” ) or vague statements (“We may receive and process information about your location.”) were not well received. Specificity: Participants experienced most existing descriptions as fulfilling an obligation and lacking a sincere attempt to be trans-parent. Participants were bothered by the fact that many excerpts“do not give any specific information, [which] raises a few red flags”(P11), but rather general information about what they “might” do(e.g. the example above: “We may receive and process information about your location”.) Providing specific detail may result in longer descriptions of the conditions under which something will or will not be done, but it would still be key to sharing information that is understandable and meaningful to users.Directness: Participants were disappointed by descriptions that attempted to portray the platform positively instead of being direct about their practices. P17 detailed: “‘we’re committed to showing you content that’s relevant, interesting and personal to you’. Look How lucky you are that we are gathering your data. This sounds disingenuous to me.” Participants were more receptive to language that seemed to be honest about the platform’s incentives and needs,such as the involvement of advertisers, as opposed to excerpts that made it seem that only the users’ best interest was in mind.Demonstration: Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information: “‘We’re collecting this information to show you stuff about [something], because you searched [something]’. That makes sense, you can put two and two together” (P23). P11 noted that “the value of specific examples is just inestimable [...] ’If you search for mountain bikes, you may see an ad for sports equipment when you're browsing a site that shows ads served by us.’ I really like that.” ",1078-84,Examples about how data and algorithms work on a particular platform helped participants follow and comprehend information
