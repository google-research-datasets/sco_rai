,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{weerts2023, author = {Weerts, Hilde and Xenidis, Rapha\""{e}le and Tarissan, Fabien and Olsen, Henrik Palmer and Pechenizkiy, Mykola},
title = {Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is Not a Decision Tree},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,ResumeSelectionAlgorithm,Artifact,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,a resume selection algorithm ,
10,ResumeSelectionAlgorithm,Agent,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,a resume selection algorithm ,
11,GenderDiscrimination,Perceived_Problem,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,"penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college",
12,DiscriminatoryBaseRates,Perceived_Problem,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,"the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles",
13,MeasurementBias,Artifact,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,"When measurement bias is associated with a sensitive characteristic, in this case gender,",
14,GenderDisparitiesInHiring,Artifact,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,gender disparities in hiring rates,
15,InconsistentViewOnDiscrimination,Perceived_Problem,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,not always consistently distinguished between direct and indirect discrimination.,
16,InabilityToRecognizeGenderDiscrimination,Perceived_Problem,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion,
17,InconsistentUseOfStatistics,Perceived_Problem,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,no consistent use of statistics by the Court,
18,Unfairness,Perceived_Problem,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,no consistent use of statistics,
19,StructuralInequality,Causal_Theory,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,replicate the pattern which can result in an unfair allocation of jobs,
20,CourtOfJustice,Agent,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,The Court of Justice,
21,Amazon,Agent,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,Amazon,
22, , , , , ,
23,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
24,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
25,Amazon,hasProducedArtifact,ResumeSelectionAlgorithm,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,a resume selection algorithm that was under development at Amazon in 2017 [46].
26,GenderDiscrimination,constrainsAgent,ResumeSelectionAlgorithm,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,"As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college"
27,DiscriminatoryBaseRates,constrainsAgent,ResumeSelectionAlgorithm,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,"Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles"
28,MeasurementBias,influencesPrecept,Unfairness,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,"When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs"
29,GenderDisparitiesInHiring,reflectsPrecept,StructuralInequality,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,"gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates"
30,InconsistentViewOnDiscrimination,constrainsAgent,CourtOfJustice,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,The Court of Justice has not always consistently distinguished between direct and indirect discrimination.
31,InabilityToRecognizeGenderDiscrimination,constrainsAgent,CourtOfJustice,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion
32,InconsistentUseOfStatistics,constrainsAgent,CourtOfJustice,"A commonly cited example of algorithmic bias is a resume selection algorithm that was under development at Amazon in 2017 [46]. As It turned out, the algorithm penalized words that indicated the applicant’s gender, such as participation in the women’s chess team or attending an all-woman’s college. It is important to note that Amazon’s hiring algorithm was not necessarily less accurate for women compared to men. Instead, the main culprit for the disparity was unequal hiring rates: in the past, the company had primarily hired men for technical roles. An important question is why these hiring rates differed. We can identify at least two potential reasons: either the data is a biased measurement of reality or reality is biased.18First, we might be looking at a case of measurement bias: historical hiring decisions are incomplete measurements of actual employee quality. When measurement bias is associated with a sensitive characteristic, in this case gender, the model is likely to replicate the pattern which can result in an unfair allocation of jobs [59]. In other words, the sensitive characteristic is implicitly included as a factor in decision-making. This type of unfairness speaks to the exclusion-ary function of formal equality: protected characteristics should be excluded from decision-making. Second, gender disparities in hiring rates could in part be explained by disparities in behavior caused by factors related to structural inequality. For example, women may have been systematically discouraged from pursuing technical roles, resulting in fewer suitable candidates. From this perspective,the wrongness of Amazon’s hiring algorithm can best be considered through the lens of substantive equality. How would such a case of algorithmic unfairness be captured byEU discrimination law? According to Amazon, the algorithm was never actually used. For the sake of our argument, however, let's assume that the algorithm was deployed in the EU. Employment Discrimination on the basis of gender clearly falls within the ma-terial scope of non-discrimination law. While gender is not used directly as a factor by the algorithm, penalizing applicants on the basis of characteristics highly associated with the applicant’s gender can be seen as a form of proxy discrimination that would either fall under the indirect discrimination doctrine or, in line with theCourt of Justice’s jurisprudence in Dekker [16] under the direct discrimination doctrine if the decision criteria used are ""inextrica-bly linked"" with sex or gender. As argued by Adams-Prassl et al.[3], we may wonder to what extent attendance of an all women's college can be seen as an ""apparently neutral criterion"" that is not inseparably linked to gender. As mentioned above, the distinction between direct and indirect discrimination is key because it deter-mines whether observed disparities can be justified, and ultimately who is responsible for internalizing the costs of social inequality.From a conceptual perspective, predicting how the Court of Jus-tice would legally qualify the Amazon recruitment algorithm raises at least two issues. First, the Court of Justice has not always con-sistently distinguished between direct and indirect discrimination. For instance, in Dekker [16], the Court ruled that discrimination on grounds of pregnancy amounted to direct discrimination on grounds of sex because of the ""inextricable"" link that exists between pregnancy and sex. As a result, even where the protected characteristic itself was not used as a basis for a decision, using a proxy that is ""inseparably linked"" to it amounts to direct and not indirect discrimination. At the same time, it is unclear which proxies will be regarded as ""inseparably linked"" to protected char-acteristics. In Jyske Finans [29], the CJEU did not consider that the practice of a credit institution to subject an EU citizen to an addi-tional identity check when born outside the EU amounted to direct discrimination on grounds of racial or ethnic origin. The CJEU did not deem the link between someone’s country of birth and ethnic origin ""inseparable"".19 In sum, the boundary between direct and indirect discrimination is contested and the Court has not always been consistent in distinguishing both notions or in defining what""on grounds of"" a protected characteristic means.20 Second, part of the problem of distinguishing between direct and indirect discrimination is linked to the difficulty of defining what a protected characteristic is. The answer to this question di-rectly depends on the choice of comparator made by the Court.21 For instance, in the context of neutral dress codes imposed by employers on their employees, whether or not discrimination was deemed direct or indirect heavily depends on which comparator is chosen. If religious and non-religious employees are compared,it appears that not all religious employees are disadvantaged by the rule. This seems to exclude direct discrimination. However, if employees whose religion mandates wearing religious clothing and employees whose (absence of) religion does not are compared, this reveals that a well-defined group is exclusively disadvantaged by the rule [34, 78], because the rule is more compatible with some religious practices than others. In fact, the divide between direct and indirect discrimination has been extensively discussed by com-mentators in the context of the so-called headscarf cases. In itsAchbita [13] and Wabe [61] decisions, the Court has been criticized for failing to treat facially neutral dress codes as a form of direct discrimination on grounds of religion (and gender) [73].22 As for-mer Advocate General Sharpston stated, ""‘neutrality’ that in reality predictably denies employment opportunities to particular, very clearly identifiable, minority groups is false neutrality"" and should thus not fall within the scope of indirect discrimination [78].Given the Court’s problematic approach to the distinction between direct and indirect discrimination, there is a risk that theCourt could treat cases of algorithmic unfairness such as Amazon's Recruitment algorithm from the perspective of indirect discrimina-tion. This would raise two further issues. First of all, the notion of""particular disadvantage"" inherent in indirect discrimination is par-ticularly vague, which makes it difficult both to assess compliance and to provide evidence for prima facie discrimination. For example,in Kalliri [22, para. 31], the Court found evidence of prima facie discrimination because the height requirement of 1,70m ""work[ed]to the disadvantage of far more women than men"". The existence of a particular disadvantage is only assessed by the Court contextually.In Seymour-Smith [14] the Court considered that statistics showing that 77.4% of the men and 68.9% of the women in the workforce were able to meet the two-year employment requirement needed to obtain compensation for dismissal ""d[id] not appear, on the face of it, to show that a considerably smaller percentage of women than men is able to fulfill the requirement"" [81]. However, there is no consistent use of statistics by the Court. The normative princi-ples guiding this assessment and the thresholds operated by theCourt of Justice often remain implicit.23 We can see those elements emerge in a few cases such as YS v NK [18], which concerned acclaim of indirect discrimination on grounds of sex, age and property.The Advocate General dismissed the applicant’s argument that an austerity measure cutting a type of large pensions in use in the1990s amounted to a particular disadvantage against older men. If The comparison test showed that men were affected more by the measure than women in absolute terms, she reasoned that it would“at most [be] linked to an already existing state of inequality”. In Other terms, gender segregation on the labor market in the 1990s,the current gender pay gap and the gender pension gap would explain any apparent impact on older men: any “predominant im-pact on men would in all likelihood have to be solely attributed to the fact that men, on average, still earn more than women and are over-represented in management positions”. [18, para. 64 and76] This case reveals the normative principle underpinning theCourt’s assessment of a ""particular disadvantage"": the lens of indi-rect discrimination should capture the unjustified reinforcement of inequalities as opposed to mere punctual ""unbalances"". Hence,rather than targeting a precise threshold, probing legal compliance in situations of algorithmic unfairness requires reflecting on the implications of a given imbalance in terms of structural inequality.Second, the indirect discrimination doctrine allows for an ob-jective justification. If Amazon’s hiring algorithm is interpreted as indirect discrimination, the accuracy of the algorithm on a testset may be deemed an acceptable justification in court [3].24 With-out access to information regarding the data collection procedure and machine learning process, it is difficult for applicants to prove whether accuracy – as indicated by the alleged offender – is a good reflection of effectiveness in practice. However, in cases of out-comes tainted by measurement bias, accuracy on observed data is an inadequate measurement of the true effectiveness of the model.Moreover, accuracy in a test environment may not generalize to accuracy of the algorithm after deployment, particularly in cases of out-of-sample predictions (i.e. the model is used under circum-stances different from the one it was trained on) or concept drift (i.e.the data distribution evolves over time).25 Importantly for computer scientists thinking about how to translate legal norms to ensure compliance, the normative principle underpinning the AdvocateGeneral’s reasoning in YS v NK, i.e. substantive equality, can be used to shape the proportionality test. As confirmed by AG Kokott,""the existing economic inequality between the sexes is not exacer-bated further in the present case"" so ""the requirements regarding the justification of any indirect discrimination are correspondingly ower"". In other words, even though prima facie a particular disad-vantage arises punctually, it can be justified if it does not generate or reinforce structural inequalities.26 This is an important indicator for assessing legal compliance. ",809-11,there is no consistent use of statistics by the Court
