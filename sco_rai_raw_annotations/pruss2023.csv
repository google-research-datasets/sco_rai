,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{pruss2023, author = {Pruss, Dasha},
title = {Ghosting the Machine: Judicial Resistance to a Recidivism Risk Assessment Instrument},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,JusticeImpactedIndividual,Agent,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) ,
10,Judges,Agent,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",judges,
11,ConcernsAboutLackOfIndividualization,Perceived_Problem,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue.,
12,PerceivedLackOfUtility,Perceived_Problem,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",they simply did not find it useful.,
13,NoInterestInChange,Strategy,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",none of the judges I spoke with were looking to change ,
14,PreferenceForDifferentInformation,Perceived_Problem,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",would have preferred to see different information presented to them at sentencing time,
15,LackOfInformation,Perceived_Problem,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",a lack of information about what the tool did or what form its recommendation appeared on.,
16,Misconception,Causal_Theory,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",some degree of misconception about the tool ,
17,Skepticism,Causal_Theory,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",skepticism or concern about risk assessment instruments more broadly,
18,ConcernAboutRacialBias,Causal_Theory,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8","the tools bias, especially racial bias.",
19,PerceptionOfToolSuperiorityToHumans,Causal_Theory,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",the tool had biases but maintained that these were still better than human biases,
20,NoSubsituteForExperience,Causal_Theory,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8","for more experi-enced judges, such an instrument was unnecessary",
21,ProScienceStance,Causal_Theory,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",pro-science stances,
22, , , , , ,
23,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
24,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
25,ConcernsAboutLackOfIndividualization,constrainsAgent,JusticeImpactedIndividual,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) [...] One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue.
26,PerceivedLackOfUtility,constrainsAgent,Judges,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",The most common reason that judges did not use the tool was that they simply did not find it useful.
27,NoInterestInChange,constrainsAgent,Judges,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",none of the judges I spoke with were looking to change their PSI-ordering behavior.
28,PreferenceForDifferentInformation,constrainsAgent,Judges,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",half of the judges also stated that they would have preferred to see different information presented to them at sentencing time
29,LackOfInformation,constrainsAgent,Judges,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8","Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on."
30,Misconception,constrainsAgent,Judges,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",every judge I spoke with revealed some degree of misconception about the tool during the course of my interview
31,Skepticism,constrainsAgent,Judges,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",skepticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument
32,ConcernAboutRacialBias,constrainsAgent,Judges,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8","Another common concern judges raised was about the tools bias, especially racial bias."
33,PerceptionOfToolSuperiorityToHumans,constrainsAgent,Judges,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",other judges acknowledged that the tool had biases but maintained that these were still better than human biases
34,NoSubsituteForExperience,constrainsAgent,Judges,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8","A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary"
35,ProScienceStance,constrainsAgent,Judges,"Community Recommendations. Drawing on standpoint theory [26], I hired two justice-impacted individuals from the community organization Coalition to Abolish Death by Incarceration (CADBI) as consultants on the project in an effort to prioritize the affectedcommunity’s interests and knowledge in developing my interview questions. One of the consultants was formerly incarcerated and the other works supporting incarcerated people and their families.Prior to conducting interviews, I met with both consultants to deter-mine the scope of the project’s research questions and later solicited their written and verbal feedback on a draft of an interview guide Produced based on this initial meeting; I compensated consultants for their time at a rate of $40/hour. One individual expressed concern that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced and suggested gauging judges’ awareness of this issue. Consultants also wanted to include interview questions about the personal nature and impacts of their sentencing decisions. Based on this feedback, I added questions to the interview guide to probe judges’ concerns about the instrument and which personal factors judges consider in their sentencing decisions.Recruitment and Demographics. I conducted interviews with judges from Allegheny, Philadelphia, Delaware, Dauphin, and YorkCounties. In each county, I initially recruited judges through email and physically mailed study invitations and follow-up phone calls regarding these invitations until I received a response or the time-frame for my data collection passed. Other judges were recruited through snowball sampling from initial responders. I made an ef-fort to select a sample of judges with variation [48] across county,political orientation, favorability to risk assessment instruments,age, gender, race, and time served as a judge (see Table 1 for the results of a demographic survey given to interviewed judges; seeAppendix A for the survey). Nevertheless, it is likely that the sample over-represents judges with higher-than-average familiarity with risk assessment instruments, since these individuals are more likely to agree to an interview about such instruments and in turn likely to refer study participants similar to themselves [36]. I continued recruiting and interviewing judges until I achieved saturation, thatis, I no longer heard new information in my interviews [45]. Intotal, I attempted to recruit 86 judges, resulting in a response rate of 17%. [...] The most common reason that judges did not use the tool was that they simply did not find it useful. This is due in part to the work of activists, lawyers, and academics who, over years of pub-lic testimony hearings, successfully pressured the PennsylvaniaSentencing Commission to remove the most controversial parts of the instrument, including directly showing judges risk scores and detailed risk distributions. The implemented version of thetool recommends ordering additional information about low- and high-risk defendants, in keeping with the original goal of helping judges identify candidates for alternative sentences. However, none of the judges I spoke with were looking to change their PSI-ordering behavior. Judges reported either ordering PSIs for all trial cases,ordering PSIs for more serious trial cases, or almost never ordering PSIs; this behavior reflected how useful judges found the PSIs them-selves, whose contents vary by county. Nearly half of the judges Italked to also did not find the contents of PSIs helpful because in many counties, including the state’s most populous Philadelphia And Allegheny counties, the reports contain information judges can get simply by talking to the defendant. In other words, the tool intervenes on a factor – PSI-ordering behavior – that judges are uninterested in changing, and falsely assumes that successfully influencing PSI-ordering behavior will in turn influence sentencing decisions.Five judges explicitly used the words “useless” or “worthless”(sometimes with an expletive) to describe the Sentence Risk As-sessment Tool. Over half of the judges also stated that they would have preferred to see different information presented to them at sentencing time, including the causal impacts of different sentenc-ing practices on recidivism, a risk and needs responsivity risk as-sessment, information about how the risk assessment was derived(“Show me the math”), and information about risk categories (“It Would be better if they said high, moderate, or low, to be honest”;one judge said they would only want to see information about low-risk defendants, while another said they would only want to see information about high-risk defendants). 4.2 “I have no idea where it is on the form; I don’t recall looking at it at any point.” Another common reason that judges ignored the tool, which often overlapped with judges’ perceptions of the tool’s uselessness, was a lack of information about what the tool did or what form its recommendation appeared on. As one judge put it, “I never knew where that information was going to be provided for me. Was it going to come in an email? A news blog? A winter weather alert?I had no idea.” Several judges explicitly asked me to show them where on the sentence guideline form that judges routinely receive at sentencing time – “the world’s least user-friendly form” – the recommendation appears. Another judge called their supervising judge during my interview because they did not believe me that sentencing risk assessment instrument was in use in their county. With two exceptions, every judge I spoke with revealed some degree of misconception about the tool during the course of my interview,such as the claim that the tool shows judges risk scores (it does not),that the tool applies to DUI cases (it does not), and that the judge has to do something in order to generate the risk assessment (they don't; it is automatically generated and appears on the sentence guideline form). Several interviewed judges were ashamed about being on the record about their lack of awareness of the tool, while others used their lack of knowledge about the tool as a reason to decline participation in my study.Nearly all judges had low literacy of the tool, despite the Com-mission’s claim that, effective January 1, 2020, it would “conduct a six-month training and orientation for judges and practitioners related to the use of the Sentence Risk Assessment Instrument,the purpose of the recommendation, and the type of information recommended” [37]. Many judges and probation officers remarked that the tool – and how to use it – had been poorly publicized. Interpersonal conversations, Commission staff explained that their in-formation campaign had been derailed by the start of the pandemic coinciding with the roll-out of the tool.More broadly, however, my findings indicate systemic problems with how information is disseminated to judges in Pennsylvania. Inone particularly revealing moment, a judge told me that they were attending a virtual Continuing Judicial Education session over video call in the background of their computer – during our interview.The problem of judicial education was echoed to me by a chief probation officer, who lamented that even with respect to the risk assessment already included in PSIs in their county, the probation department had not done much in the way of educating judges about how to interpret risk assessment information, adding that many judges “didn’t really understand how it applies to the work that they do” and that this was likely the case statewide.14.3 “It’s unworkable. I don’t know how you're building that into numbers.”In addition to misinformation and perceptions of uselessness, skep-ticism or concern about risk assessment instruments more broadly was often a complementary reason that judges cited for ignoring the Sentence Risk Assessment Instrument, though it was typically a secondary issue. These concerns fell roughly into three categories.The most common concern, which roughly half of judges expressed, was that the tool ignored a defendant’s humanity. Notably,this was a central issue raised by CADBI members in their feedback on my study design; one formerly incarcerated individual worried that the new risk assessment tool would make judges more likely to ignore the humanity and personal circumstances of the people they sentenced. “Each individual has a history that brought them to this space,” this consultant told me. “There must be individual-ization.” Judges echoed this point, raising concern about “having a formula that takes away my ability to see the humanity of the peo-ple in front of me”; another judge argued that “cookie cutter justice doesn't work” and that risk assessment was “merely labeling andboxing”; a third said, “I don’t know how you can reduce all of the human factors that go into, you know, sentencing or making a bond decision and, and put it into a number, you know, I just, I just think that there are a world of human factors that need to be considered.” These judges emphasized the crucial role that individual narratives and personal context played in their sentencing decisions. Most judges also indicated that they did not assign central importance to aggregated recidivism risk in their sentencing decisions (with the exception of recidivism risk for sex crimes). Rather, they were interested in the personal trajectories of criminal defendants, particularly escalation toward violent behavior; whether a defendant was employed; and drug use. Another common concern judges raised was about the tools bias, especially racial bias. One judge, who identified as Black, was critical of the discriminatory potential of the tool: “Who’s making the determinations? Who’s interpreting the statistics? You can say anything with statistics.” Another judge noted the third-party au-dit’s finding that the tool’s high-risk category was less accurate than the low-risk category, commenting that this could be “prejudi-cial to certain minority groups because there was an historically higher arrest rate, possibly related to things like race rather than actual criminal activity.” Judges were concerned about other biases as well – a judge who was otherwise an advocate of risk assessment tools claimed that the tool was biased in favor of sex offenders (acclaim that is not factually accurate), while two others commented that age was an unfair indicator of recidivism because minorities are statistically more likely to be stopped by police at a younger age.This concern about bias was not unanimously shared, however; other judges acknowledged that the tool had biases but maintained that these were still better than human biases: “You can never take all biases out. You can never take out – there’s biases, people get arrested – what’s in it, but you can continue to work on the tools to try to make them as fair as possible. But it’s better than individuals.”One judge even claimed that “[risk assessment tools] have been deliberately distorted as being racist, as being not accurate, as been using wrong statistics and things like that.”The third most common concern was that the tool was worse than the discretion of experienced judges. A common refrain from judges was that younger, less experienced judges might get more benefit from the risk assessment tool, but that for more experi-enced judges, such an instrument was unnecessary. There was also a general sentiment from judges that personal discretion was a centrally-defining feature of what it means to be a judge; one judgewith over a decade of experience firmly announced in the first 10 seconds of our conversation that they were “elected to be a judge,not a robot.” Nine judges independently brought up that judges“don’t want to be told by anybody what to do;” however, those same judges did not view themselves as being in this category. Sevenjudges said their own sentencing practice was better than other judges, describing their sentencing using adjectives like “different,”“atypical,” or (pleasantly) “shocking” to defendants. Several judges were critical of any efforts to limit their discretion, including sen-tencing guidelines, which are supposed to standardize sentence lengths based on an individual’s prior record score and the gravity of their current offense. One judge aptly summarized this partic-ular concern: “[The legislature] is trying to give us more narrow options on what we can do. And, and I don’t like that, because I Think that there’s a reason that we’re up there – we’re up there because supposedly we’ve demonstrated some ability to think more broadly about the whole system and to make a better decision than just something that’s electronically generated. You know if you're going to do it all based on a computer program, then you don't need me out there.”Importantly, however, judges’ skepticism about risk assessment instruments should not be conflated with skepticism toward data-driven strategies in criminal justice more broadly. As already men-tioned, many judges reported wanting access to more data at sen-tencing time – just not the kind of information provided by this risk tool. Moreover, most judges did in fact acknowledge the im-portance of consistency in sentencing and, with few exceptions,reported complying with sentencing guidelines. With the exception of two judges, the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances at other points within the same interview. One judge, who had expressed concerns about the tool’s racial bias earlier in our interview, maintained that “I'm A believer in science. This [risk assessment] is science, so we need to use it.”4.4 “Anything that slows down processing will be met with resistance.”Several judges worried that the Sentence Risk Assessment Instru-ment could have unexpected downstream consequences, were it to be used. The Commission “expressly disavows the use of the sentence risk assessment instrument to increase punishment” [37].However, as one judge and public testimonies pointed out, judges can still infer risk levels from the ‘Additional Information’ label,and empirical evidence from other states suggests that judges are more likely to use risk information to detain individuals longer [27].“People know it’s called the risk tool. If it’s ‘Additional Information’,there may be some concern about how dangerous the defendants,” a judge noted. Moreover, if judges followed the tool’s recom-mendation to order PSIs for low-risk defendants – who often have minor sentences – then the tool could have the unintended effect of detaining these defendants longer pre-trial, since ordering a PSIcan take 60 days or longer, depending on the county. Another judge remarked, “I’m not letting them [the defendant] sit 8 more weeks in jail because some computer program said so.”Speaking about unintended impacts in other parts of the crimi-nal legal system, two probation officers also shared worries about the tool creating unnecessary – and invisible – labor for their departments, which are tasked with generating the risk and needs responsivity assessments that go into the PSIs in some counties.One of these officers said they feared they were going to get “a flood of cases” where judges were ordering PSIs, but that “thankfully that has not happened” because they did not have the resources to handle such a surge. They said they would like to see the system some-day permit having such an assessment done for every defendant,but that this would “require a lot of resources, a lot of resources.”The second officer raised the concern that the tool, if widely used,would “significantly slow down” the already-backlogged sentenc-ing process, which they said could cause individuals to spend evenmore time awaiting trial in jail. To this probation officer, the risk assessment instrument was just “another unfunded mandate, the burden of which was going to fall on county probation.”4.5 “We’re past that train stop and a little bit further down the tracks.”A minority of judges I spoke with were knowledgeable, vocal advo-cates of other risk assessment instruments and the PennsylvaniaSentencing Commission’s other projects. Even among these four judges, however, only one claimed to be regularly consulting theCommission’s tool, with the caveat that it had never changed theirPSI-ordering behavior. Judges in this group were either advocates of using risk assessment at other stages of the criminal legal pipeline,such as at preliminary arraignment, or were serving in counties where the Ohio Risk Assessment Instrument (ORAS), a significantly more detailed risk and needs responsivity risk assessment, is con-ducted by the probation department and is already a routine part of the PSIs that judges receive. One self-described “cheerleader” for risk assessment instruments explained: “I like the [Commission’s]tool, I just like our tool [the ORAS] better – it’s shinier and faster.”This was the position of two of the probation officers I spoke with well.In sum, although nearly all of the judges I interviewed reported ignoring the Sentence Risk Assessment Instrument, their reasons for this varied. This suggests a nuanced explanation for aversion to algorithmic systems in the criminal legal system that is neglected in existing discussions that are centered largely around lack of confidence in technology and fears of deskilling and surveillance.The rest of this paper discusses the implications of this finding for scholarship on algorithmic resistance and risk assessment instru-ments.","315, 316-8",the skeptical claims above were regularly expressed alongside pro-rata and pro-science stances
