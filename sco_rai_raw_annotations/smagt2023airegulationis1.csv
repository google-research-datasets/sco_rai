,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{smagt2023airegulationis,
    author = {Patrick van der Smagt and Laura Lucaj and Djalel Benbouzid},
    title = {AI Regulation Is (not) All You Need},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-08, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,MLModels,Agent,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,ML models,
10,Harm,Artifact,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,harm,
11,StabilityAI,Agent,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,Stability AI ,
12,DALLELikeModels,Artifact,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,models comparable to DALL·E,
13,ReverseEngineering,Strategy,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,reverse-engineering,
14,SafteyBypass,Strategy,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,safety filters be bypassed,
15,IgnoranceOfDangerSource,Perceived_Problem,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,over-looked a large part of the potential source of danger,
16,LackOfDocumentation,Perceived_Need,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,lack of documentation,
17,Developers,Agent,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,developers,
18, , , , , ,
19, , , , , ,
20, , , , , ,
21, , , , , ,
22, , , , , ,
23, , , , , ,
24,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
25,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
26,MLModels,hasProducedArtifact,Harm,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,ML models can produce harm
27,StabilityAI,hasProducedArtifact,DALLELikeModels,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267, Stability AI releases of models comparable to DALL·E
28,ReverseEngineering,constrainsAgent,StabilityAI,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,Stability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations
29,SafteyBypass,constrainsAgent,StabilityAI,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,"Stability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed"
30,IgnoranceOfDangerSource,constrainsAgent,StabilityAI,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,"Stability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger["
31,LackOfDocumentation,constrainsAgent,Developers,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,lack of documentation has prevented
32,DALLELikeModels,reflectsPrecept,ReverseEngineering,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,models comparable to DALL·E. Through reverse-engineering
33,DALLELikeModels,reflectsPrecept,SafteyBypass,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,"models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed"
34,DALLELikeModels,reflectsPrecept,IgnoranceOfDangerSource,"A recent example that highlights why some ML models can produce harm when deployed is provided by the analysis of theStability AI releases of models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger[65]. These findings display how the lack of documentation has prevented developers to access sufficient information for ex-ante mitigation measures, to properly understand and detect the safety risks [65].",1267,"models comparable to DALL·E. Through reverse-engineering these open-source implementations, not only could the safety filters be bypassed but it was shown that these safety mechanisms were limited to a handful of criteria and over-looked a large part of the potential source of danger"
