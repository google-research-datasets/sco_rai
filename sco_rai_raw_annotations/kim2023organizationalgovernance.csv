,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{rudnik2023careandcoordination,
    author = {John Rudnik and Robin Brewer},
    title = {Care and Coordination in Algorithmic Systems: An Economies of Worth Approach},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-10, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,Participants,Agent,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,Participants,
10,FrontlineWorkers,Agent,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,frontline workers,
11,Organizations,Agent,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,organizations,
12,IdentifiedProblems,Artifact,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,perceived the problems,
13,ImportanceOfUnderstanding,Perceived_Problem,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,the importance of understanding,
14,InvolvingUnderservedCommunities,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,members of underserved communities [...] aspirational and impactful,
15,LackOfBenefit,Perceived_Problem,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,might not always be the ones who would benefit directly from implementing the solution,
16,UnequalResources,Perceived_Problem,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,unequal distribution of resources within many organizations,
17,NeedForEfficiency,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,promote efficiency in clinical care.,
18,WellDefinedUsage,Goal,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"well defined, narrow intended uses of AI.",
19,SolutionAppropriateness,Goal,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,whether AI is indeed an appropriate solution to a problem.,
20,IdentificationOfResources,Goal,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software",
21,NeedForResources,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"resources [...] The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering.",
22,Inefficiency,Other_Precept,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,assessments were often inefficient,
23,QualityAssurance,Goal,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,ensured the quality of AI software under consideration,
24,LackOfConsensus,Perceived_Problem,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,a lack of agreed upon practices for model development and validation,
25,LiabilityRisks,Perceived_Problem,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,the burden of potential legal and liability risks.,
26,NeedForAlignment,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. ",
27,MetricIndentification,Goal,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,it was important to define which metrics the organizations should measure to determine the success of the solution,
28,NeedForNewWorkflow,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,a new workflow involving AI software should be designed before the software was integrated into clinical practice.,
29,NeedForShadowMode,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,the need for prospective validations of software in a ‘silent trial’ or ‘shadow mode.’ ,
30,HealthData,Artifact,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,healthcare data ,
31,NeedForRepresentation,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,alidating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.,
32,RiskAwarness,Goal,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,be aware of the potential risks and have plans to mitigate them from early on.,
33,RegaulatoryRequirements,Strategy,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,regulatory requirements,
34,LackOfCareKnowledge,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,do not have sufficient knowledge,
35,NeedForThresholds,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,to set clear eligibility criteria and model performance thresholds for a given model.,
36,NeedForGuideline,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.,
37,TailoredEducation,Strategy,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,the need to tailor education and training to the target audience,
38,LowAILiteracy,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,The low AI literacy ,
39,AutomationBias,Perceived_Problem,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,would show automation bias and over-rely on the model,
40,ClinicalValue,Other_Precept,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,how clinically valuable it would be to use the tool by showing them real-world evidence from the local context.,
41,Agency,Causal_Theory,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,agency,
42,StaffTurnover,Perceived_Problem,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,the biggest challenge in operational monitoring is staff turnover.,
43,ToolContinuity,Perceived_Need,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority,
44,DifficultyMeasuringImpact,Perceived_Problem,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,difficulty measuring the true impact of the component of a solution in isolation,
45,PeriodicAIReview,Strategy,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,periodic review of AI software monitoring and its use was important.,
46,Interviews,Artifact,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses,
47,PainPoints,Artifact,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,surfaced daily pain points and brought them to organizational leaders for consideration,
48,FeasibilityAssessment,Artifact,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,assess the feasibility and viability of adopting AI software to solve a prioritized problem.,
49,PerformanceMetrics,Artifact,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were [...] model performance metrics.",
50,DataQualityAssessment,Artifact,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,assessing the quality of data used to train and validate the model,
51,Bias,Causal_Theory,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,may contain bias,
52,ModelValidation,Artifact,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,model validation,
53,UnderservedPopulations,Agent,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,underserved populations ,
54,BusinessAdministrators,Agent,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,business adminsitrators,
55,AIAdoptionDecision,Artifact,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,decide to move forward with adopting the AI software or to stop the process.,
56,CommunicationAndSupport,Artifact,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"necessary communication, education, and support to frontline workers expected to adopt the AI software",
57,AIMontoring,Artifact,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,monitor the technical and operational aspects of the AI software,
58,Opportunities,Goal,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments.,
59,EndUser,Agent,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,end users of the potential solution,
60, , , , , ,
61, , , , , ,
62,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
63,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
64,Participants,hasProducedArtifact,Interviews,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses
65,FrontlineWorkers,hasProducedArtifact,PainPoints,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,frontline workers surfaced daily pain points and brought them to organizational leaders for consideration.
66,Organizations,hasProducedArtifact,IdentifiedProblems,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers."
67,IdentifiedProblems,reflectsPrecept,Opportunities,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments.
68,ImportanceOfUnderstanding,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"The participants highlighted the importance of understanding the context in which the problems were identified,"
69,InvolvingUnderservedCommunities,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice.
70,LackOfBenefit,constrainsAgent,EndUser,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,end users of the potential solution might not always be the ones who would benefit directly from implementing the solution
71,UnequalResources,constrainsAgent,Organizations,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,The participants flagged the unequal distribution of resources within many organizations
72,NeedForEfficiency,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care."
73,WellDefinedUsage,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"participants emphasized the importance of well defined, narrow intended uses of AI."
74,SolutionAppropriateness,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem.
75,Organizations,hasProducedArtifact,FeasibilityAssessment,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem.
76,IdentificationOfResources,constrainsAgent,Organizations,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software"
77,NeedForResources,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"The resources identified by participants as prerequisites to adopting AI software were extensive. The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering."
78,Inefficiency,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"Yet, the participants described how these assessments were often inefficient"
79,QualityAssurance,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"Participants reported ways that they ensured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible."
80,LackOfConsensus,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,They highlighted a lack of agreed upon practices for model development and validation
81,LiabilityRisks,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks."
82,NeedForAlignment,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. "
83,Participants,hasProducedArtifact,PerformanceMetrics,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics."
84,MetricIndentification,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,participants felt it was important to define which metrics the organizations should measure to determine the success of the solution
85,NeedForNewWorkflow,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice.
86,NeedForShadowMode,constrainsAgent,Organizations,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,Many organizations described the need for prospective validations of software in a ‘silent trial’ or ‘shadow mode.’
87,Participants,hasProducedArtifact,DataQualityAssessment,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,The participants reported that assessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model.
88,HealthData,reflectsPrecept,Bias,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,healthcare data is messy and may contain bias
89,Participants,hasProducedArtifact,ModelValidation,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"The participants noted that model validation could be a time-consuming process, which could take many months."
90,NeedForRepresentation,constrainsAgent,UnderservedPopulations,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.
91,RiskAwarness,constrainsAgent,Organizations,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on."
92,RegaulatoryRequirements,constrainsAgent,Organizations,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model.
93,LackOfCareKnowledge,constrainsAgent,BusinessAdministrators,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.
94,NeedForThresholds,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,participants proposed to set clear eligibility criteria and model performance thresholds for a given model.
95,Organizations,hasProducedArtifact,AIAdoptionDecision,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,organizations decide to move forward with adopting the AI software or to stop the process.
96,NeedForGuideline,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.
97,Organizations,hasProducedArtifact,CommunicationAndSupport,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"organizations provide necessary communication, education, and support to frontline workers expected to adopt the AI software"
98,TailoredEducation,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,Participants described the need to tailor education and training to the target audience
99,LowAILiteracy,constrainsAgent,FrontlineWorkers,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,The low AI literacy among front-line workers
100,AutomationBias,constrainsAgent,EndUser,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,the concern that end-users would show automation bias and over-rely on the model
101,ClinicalValue,constrainsAgent,EndUser,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context.
102,Agency,constrainsAgent,EndUser,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,give end users agency
103,Organizations,hasProducedArtifact,AIMontoring,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,organizations monitor the technical and operational aspects of the AI software
104,StaffTurnover,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,participants reported that the biggest challenge in operational monitoring is staff turnover.
105,ToolContinuity,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority
106,DifficultyMeasuringImpact,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,Participants described the difficulty measuring the true impact of the component of a solution in isolation
107,PeriodicAIReview,constrainsAgent,Participants,"Between April 2022 and January 2023, we conducted qualitative research using in-depth interviews, concurrent with design research.The goal of this work was to understand the current and aspira-tional state of AI adoption in healthcare organizations. We wished to identify requirements for the adoption of AI systems in health-care settings and map out adequate organizational governance structures. The design research informed how qualitative research results are framed around key decision points. We combined design research and qualitative research approaches to ensure that future product and program development addresses real needs of healthsystem leaders. This research was considered a quality improve-ment project without the involvement of human subjects. Thus,it was exempted from an IRB approval. All participants provided verbal consent to be interviewed and to have anonymized data used in qualitative analyses. 5.1 MethodFirst, we developed an interview guide and identified which type of internal and external roles to interview based on four relatively distinct stages of AI adoption: (1) problem identification and procurement, (2) development and adaptation, (3) clinical integration,and (4) lifecycle management [28] (Table 4 in Appendix C). Inter-view guides were designed to walk participants through these four stages. Questions in the interview guide covered processes and personnel involved in each stage (examples in Appendix C).Then, 89 professionals in healthcare and other relevant fields were recruited to participate in interviews. We used purposive and snowball sampling approaches to recruit participants. Participationwas completely voluntary. All participants whom we reached out to agreed to participate in the study. Of the 89 participants, 70 were from the partner sites. Among the 70 partner site participants, there was diverse representation across clinical (n=12), technical (n=34), operational(n=19), and regulatory (n=5) roles. Frontline clinicians who were users or champions of AI systems were considered clinical personnel. Technical personnel were those who develop technology andIT infrastructure, including data scientists, data engineers, and ITleaders. Operational personnel were those in charge of managing operations within a health system department or facility. Regula-tory personnel were those who ensure compliance with regulations.Two of the technical personnel and two of the operational personnel had joint appointments with clinical roles.Of the 89 participants, 19 were key informants with primary af-filiations outside of healthcare organizations. Key informants were identified based on expertise in areas critical to safe and responsible adoption of AI software, including bias (n=10), ethics (n=3), commu-nity engagement (n=3), organizational behavior (n=1), regulation(n=1), and credentialing (n=1). Key informants were recruited to capture perspectives that were poorly represented by health system participants.All interviews were conducted via Zoom by 1-4 project leaders. All interviews were conducted with a single interviewee, except for two sessions where 2 and 4 participants from the same orga-nization participated in each. Each interview was about an hourlong, ranging from 34 to 82 minutes, and a timer was used to spend at least 10 minutes on each stage. The participants were asked the questions described in the interview guide, with a semi-structured approach to capture participants’ varying scope of experience and emergent concepts. The interviews were recorded once the partici-pants gave a verbal consent and later were transcribed for analysis via Otter.ai, an automated transcription software. All participants gave informed consent for the interview and recording. We ana-lyzed data alongside the recruitment and decided to terminate data collection when no new themes or insights emerged (i.e. thematic saturation).To analyze the transcripts, we followed a modified grounded theory [7] approach and practiced a coding process to organize qualitative data. In our study, coding was conducted in 2 cycles. In the first cycle, we used descriptive open coding to develop initial codes and capture the general ideas that emerged from the raw data. In the second cycle, we used focused coding to examine the most frequent and significant codes and categorize the initials codes based on thematic similarity. After the second cycle of coding, we identified the most prevalent themes and subthemes, using the key decision point framework designed in Part 1. Data were analyzed using NVivo Qualitative Data Analysis Software.5.2 ResultsMajor themes emerged as important factors that shaped decisions related to adopting AI software (Table 1). The factors were grouped into 8 thematic areas that described key decision points across the entire process of adopting AI. Under each thematic area, sub-themes were identified with representative quotations that captured challenges and corresponding best practices. The results identified challenges and risks that organizations experience in the process of AI adoption as well as real-world processes and practices. This thematic area describes how the organizations identify a high-priority problem and consider adopting AI software to solve the problem. Representative quotations are presented in Appendix D.5.3.1 Problem identification. The participants reported two dominant ways problems were initially identified as meriting investment. In some organizations, frontline workers surfaced daily pain points and brought them to organizational leaders for consideration. In others, problem identification was primarily led by organizational leadership, even before a need was escalated from frontline work-ers. Organizational leaders perceived the problems as emerging opportunities for the organizations to grow and identified areas in which the organizations might benefit from investments. Some or-ganizations reported a mixed approach. The participants described the strengths and limitations of the two approaches.The participants highlighted the importance of understanding the context in which the problems were identified, especially when the organizational leaders raised the problems. To characterize the problems effectively, it was important to include frontline workers to provide an authentic sense of context. Involving members of underserved communities from the early stage of problem identifi-cation was raised as an aspirational and impactful practice. Once Problems were identified, interviewees described the importance of defining the end users for the potential solution. In this process,it was important to understand that end users of the potential solution might not always be the ones who would benefit directly from implementing the solution. Sometimes, other parts of the organizations or downstream users could receive direct benefits from the work of end users. Given that the needs of end users and beneficiaries of the solution might not align, interviewees emphasized the importance of engaging end users in the early stage of problem formulation to anticipate and mitigate these potential barriers to adoption. The participants also noted that the organizations were fre-quently approached by vendors, and there could be external pres-sures to procure AI software. For instance, a clinical leader with a prior connection to a vendor could initiate the procurement of software without a thorough, centralized evaluation. The partici-pants cautioned that such external forces might distract problem-led procurement which emerged as a best practice. In response to this possible risk, we observed that some organizations founded gov-ernance committees with enterprise-wide scope to prevent hasty procurement happening in silos.5.3.2 Problem prioritization. Once the problems were identified,in some organizations, they were prioritized through a centralized process based on organizational criteria. In others, business units within the organization were allowed to make their own decisions guided by organizational criteria applied to their specialty. Criteria Varied across organizations, including feasibility of solving the prob-lems, severity of clinical condition, number of patients impacted by it, sense of urgency, use of pre-existing digital infrastructure,organizational goals and strategic initiatives. The participants com-monly expressed a need to improve patient care and promote work efficiency by lessening the burden on clinicians.The participants flagged the unequal distribution of resources within many organizations. Some departments or organizational business units might benefit from greater resources than others and thus have greater capabilities to identify emerging opportu-nities and solve problems. In that case, problems experienced by groups with greater resources, such as departments of radiology or cardiology, could be prioritized over problems of the groups with fewer resources, such as departments of pediatrics or women's health. The participants suggested that organizations should allo-cate resources in an equitable fashion that does not further entrench existing disparities across business units.5.3.3 AI contribution. When participants were asked how AI could be optimally used to solve problems, responses and proposed use cases varied tremendously. Yet, a commonly described unique con-tribution of AI was the use of data to promote efficiency in clinical care. For example, interviewees described the potential for AI to help triage patients with different levels of risk, allocating physician time efficiently, and making informed decisions quickly. Despite the potential benefits of using AI, participants emphasized the importance of well defined, narrow intended uses of AI. It was felt important to remember that AI software is there to help clinicians make better use of their time, not to replace clinicians or their work. The participants also reported that it is important to verify whether AI is indeed an appropriate solution to a problem. Participants described many instances in which software with sim-ple decision logic can be a viable solution with greater transparency and lower resource requirements than AI or machine learning.5.4 Identify requirements for an AI product as a viable component of the solution. This thematic area describes how organizations assess the feasibility and viability of adopting AI software to solve a prioritized problem. During this initial evaluation process, the organizations identify required resources, assess the feasibility of adoption, con-duct preliminary quality assurance of the AI software, seek align-ment among affected parties, and determine whether to build or buy the AI software. Representative quotations are presented inAppendix E.5.4.1 Requirement and feasibility. The resources identified by participants as prerequisites to adopting AI software were extensive.The resources included funding, time, data environment, IT infras-tructure, regulatory approval, and employees with skills and exper-tise in data science, clinical practice, data engineering, and software engineering. The participants noted that complying with regulations would require additional resources. Assessing requirements and feasibility was an important step for determining whether I would be a viable component of a solution. Yet, the participants described how these assessments were often inefficient, with resources and capabilities decentralized and duplicated across the organization.To enhance efficiencies, participants recommended centralizing roles and capabilities to think broadly about the optimal use of AIr within the organization.5.4.2 Quality assurance. Participants reported ways that they en-sured the quality of AI software under consideration, especially when they decided to buy AI software from external vendors. They Conducted literature reviews. They examined whether the AI software had an accurate and equal performance across different sub-groups of patients, whether it had been validated for their patient population, and whether it was built on valid data sets that are easily accessible. They also inspected whether the AI software could be easily integrated in the existing clinical workflow and IT interface, whether it complied with regulation, and whether it could be sustainably used and expanded for other use cases. In this process,the participants reported difficulty with accessing the necessary material, unless the AI software was built in house. Vendors often did not have or were not willing to share the material. Given the lack of documentation and the amount of due diligence required,the participants raised a need for a standardized structure of quality assurance and active communication across organizations.5.4.3 Build vs. buy decision. Participants felt motivated to build A-house when it did not exist in the market or they were skeptical about an existing model. They had multiple concerns about an external AI, including that it could be inappropriate for their target problem or patient population or that it could impose greater risk tolerance for meeting regulatory compliance. They highlighted a lack of agreed upon practices for model development and validation. A primary benefit of building AI internally was having full transparency & visibility into the model development process. On The other hand, when the valid AI software already existed in the market or the organizations lacked the resources to build, includ-ing data for validation and internal expertise, the participants felt motivated to buy AI from external vendors. The benefit of buying was that the organizations could focus resources saved by not developing the AI on its implementation, which was seen as the area where organizations had the most to learn. Yet, it did not mean that buying AI software was necessarily cheaper than building one. Infact, buying AI software was sometimes seen as more expensive,as adapting the model to a setting where it would be integrated was costly. Also, buying AI software did not exempt the organizations from complying with regulations. Regardless of whether the organizations decided to build or buy AI software, they all felt the burden of potential legal and liability risks. 5.4.4 Aligning affected parties. The participants reported that in making decisions to develop or adapt AI software, all affected parties from various disciplines, including AI specific committees and specialists in clinical, operational, and technical areas, must be aligned. Challenges in aligning affected parties included identifying key affected parties who should be involved in making the decision and aligning different priorities across all affected parties.5.5 Develop measures of outcomes and success of the AI productThis thematic area describes how the organizations set their goals and defined measurable outcomes. Representative quotations are presented in Appendix F.5.5.1 Model performance measure. Sensitivity, specificity, area under the receiver operating characteristic curve, positive predictive value number, and false positive and negative rate were commonly referenced by interviewees as model performance metrics. Some of these were felt to map more intuitively to a given use case or end user experience than others. In defining the model metrics, it was seen to be important to ensure that the model performs similarly on subgroups stratified by important demographic factors so that patients receive equitable care.5.5.2 Success measure. In addition to the model performance metrics, participants felt it was important to define which metrics the organizations should measure to determine the success of the solu-tion. Outcomes commonly captured in the success metrics included improvements in patient outcomes, reductions in burden on health-care providers, and reductions in cost. A challenge to developing success measures was reconciling the different definitions of success across different affected parties. 5.6 Design a new optimal workflow to facilitateintegrationThis thematic area describes how organizations develop workflow and user experiences surrounding AI software to facilitate its adop-tion into clinical care. Representative quotations are presented inAppendix G.5.6.1 Operational optimization. The participants reported that a new workflow involving AI software should be designed before the software was integrated into clinical practice. As algorithm developers tend to have limited insight into an existing workflow,it was critical that end users of the AI software were involved in the process of developing the new workflow and provided context into the current state. The participants proposed to design the new workflow in line with the existing one to minimize the changes made in implementation. Given that implementation of the AI software would create inevitable changes in the existing workflow,they also highlighted the importance of recognizing the changes and providing necessary support to front-line workers reconciling these changes.5.6.2 Technical optimization. For the AI software to cause minimal friction, designing an intuitive user interface (UI) was also an important component for optimizing technical aspects of workflow. One of the most common practices of building a user-friendly UI ways to make it simple and customize it for different groups of users and different sites of users. Simplicity in the user interface could be achieved by standardizing the visualization of the model, con-ducting usability tests with users, and using technology that was already familiar to users.5.7 Evaluate safety, effectiveness, and equity concerns in the intended setting prior to clinical useThis thematic area describes how the organizations conduct a com-prehensive review of data & evaluate the AI software before they routinely use the software in clinical practice. After the evalua-tion, an organizations decides to clinically integrate or abandon theAI software. Many organizations described the need for prospec-tive validations of software in a ‘silent trial’ or ‘shadow mode.’ While prospective model validation takes place once the model is integrated into IT infrastructure, it is important to note that all other safety, effectiveness, and equity concerns should be taken into consideration throughout the entire process of AI adoption.Representative quotations are presented in Appendix H.5.7.1 Data quality assurance. The participants reported that as-sessing the quality of data used to train and validate the model was an important step for ensuring the safety and the efficacy of the model. Given that healthcare data is messy and may contain bias, it was important to perform thorough data quality assurance,even though it required significant resources. Some of the common practices of data quality assurance were ensuring that there was minimal bias present in data, creating a diverse data set for training the model, and cleaning data before building the model.5.7.2 Validation. Validating the model on real-time data prior to clinical integration was seen as a key component of assessing the safety, efficacy, and equity concerns of the model. Common methods of prospective validation included conducting a silent trial,comparing the model outcomes to patient charts in the medical record, and conducting a pilot on a subset of patients. The participants noted that model validation could be a time-consuming process, which could take many months. Yet, it was an essential process to go through, especially when testing model accuracy on retrospective labels might not be sufficient for model validation due to inaccuracies in ground truth labels. The participants sug-gested that for successful model validation, algorithm developers should collaborate closely with clinical partners. Many also recommended validating the model on underserved populations as well as a large representative sample of populations to ensure equal model performance across different groups of patients.5.7.3 Risk mitigation. To ensure the safe, effective, and equitable use of AI software, the participants indicated that organizations should be aware of the potential risks and have plans to mitigate them from early on. They suggested that being aware of the potential risks would be the first step. Cybersecurity risk, bias and disparities in care, and unexpected harms to patients were the most prevalent risks that the participants identified.To mitigate these risks, participants suggested that organizations should deeply understand regulatory requirements and work with a group of people with expertise in the full breadth of relevant domains when evaluating the model. For instance, the governance body should work closely with frontline workers who are knowl-edgeable and experienced in clinical care and community members to promote equity in healthcare. The participants pointed out that business administrators who do not have sufficient knowledge inpatient care were typically the ones who evaluated AI algorithms.To ensure the safe, effective, and equitable use of AI software, par-ticipants emphasized that clinical personnel must be part of the evaluation process. From a technical standpoint, the participants proposed to set clear eligibility criteria and model performance thresholds for a given model. Eligibility criteria defined which patient populations would be eligible for model use. If there were any disparities detected in the model performance among different patient popula-tions, participants suggested addressing these issues by adjusting thresholds based on the disparities in the model performance. Then,once the model was ready for clinical integration, participants pro-posed to implement it incrementally in clinical settings to minimize the potential harm in patient care that could be caused by unfore-seen safety issues.5.7.4 Clinically integrate vs. abandon decision. After evaluating the safety, efficacy, and equity concerns of the model, the organizations decide to move forward with adopting the AI software or to stop the process. When all interested parties were satisfied with the model,they decided to integrate the model into clinical practice. However,when they were not satisfied with the model and did not perceive the benefit of adopting the model, they decided to abandon it. The Participants reported that organizations often decide not to move forward with the adoption process when the model was no longer valuable, not implementable, not interoperable, too expensive, or unsuccessful in a pilot launch. The participants suggested that a decision to clinically integrate or abandon the model should be made based on evidence supported by data and that the organizations should be open to making an abandonment decision at any given point. One of the most prevalent challenges in making this decision was a lack of standardization in the decision-making process. The participants shared that having a standardized structure or guide-line for risk assessment and clinical integration decisions would be helpful for making informed decisions.5.8 Execute AI product rollout, workflow integration, communication, education, andscalingThis thematic area describes how the organizations provide neces-sary communication, education, and support to frontline workers expected to adopt the AI software, once they decide to integrate themodel into clinical care. Representative quotations are presented in Appendix I.5.8.1 Communication. Communicating the plan for rollout of the tool was important for successful clinical integration. The plan for rollout explained when the model would be implemented in the workflow, who the end users would be, how to use the model, how to receive support and help in using the tool, and how to provide feedback on the tool. Bi-directional communication between devel-opers and users was seen as key, both to allow adaptations needed to accommodate end users’ needs on the ground and to build a sense of accountability that AI developers were listening to end users and respecting their agency. In communications leading up to clinical integration, it was also critical to explain the short-term and long-term benefits of using the tool and to answer potential questions from end users.The participants described that the launch of the tool was disseminated via various communication platforms, including email and website, depending on the target audience. Participants also reported that the communication was found to be more effective when it was made by clinical champions or trustworthy leaders in the organizations. The target audience of the communication includes not only end users of the tool but also those who would oversee change management and those who would be impacted by the change.5.8.2 Education and training. Participants described the need to tailor education and training to the target audience, their role, and the scale of change management. Education and training materials were disseminated in various ways, frequently including online learning management systems and webinars. One participant suggested that simulation-based training would be the most effective method of education and training. Participants agreed that many operational and clinical parties have heard of AI but do not have much knowledge in it. Given The low AI literacy among front-line workers, participants won-dered whether education and training should also offer foundational knowledge of AI.5.8.3 Trust. Building an appropriate scope of trust between the AIsoftware and end users was known to be a big challenge. Even if end users trusted the technology, there was the additional challenge of ensuring this trust did not extend beyond the model’s competence. Participants were aware of the concern that end-users would show automation bias and over-rely on the model. Participants hoped to build trust in AI software to a degree where clinical end users trusted the technology and found it useful, but not to a degree where they would fully rely on it in making decisions.The participants suggested a few approaches to build trust with end users. The first approach was to make algorithm developers have conversations with end users to provide transparent expla-nations about the model with trustworthy data and make them become familiar with AI and its use. If communicating directly with the end users was not feasible, the developers should at least speak with frontline clinicians or clinical leaders who were trusted by the end users. Gaining trust from them was found to be an efficient way to slowly build trust with the end user community. The second approach was to show end users how clinically valuable it would be to use the tool by showing them real-world evidence from the local context. One participant, for example, described an incident where they showed end users how the tool would be able to ef-ficiently triage their patients. Another approach was to give end users agency and engage them from early stages of the AI software adoption process. That way, they could have transparency over the process, raise questions, provide feedback, and ultimately feel more confident about adopting the tool.5.9 After operationalization, monitor and maintain the AI product and impactedecosystemThis thematic area describes how the organizations monitor the technical and operational aspects of the AI software to ensure that it continues to function appropriately, if they decide to sustain the use of it. In this stage, we use the term impacted ecosystem to describe the environment in which front-line workers and patients are affected by use of the AI in practice. Representative quotations are presented in Appendix J.5.9.1 Technical monitoring. Technical monitoring involves mon-itoring the technical aspects of the model, such as model perfor-mance metrics, to ensure that the model continues to perform as intended. The participants cautioned that model evaluation may have to change over time, as the model performance may drift due to changes made in clinical care. They suggested that if the model drifts, the entire team of technical, clinical, and operational parties should get together to potentially retune the model and continue monitoring and revalidating the model over time.5.9.2 Operational monitoring. Another type of monitoring that the participants described was operational monitoring. The participants reported that the biggest challenge in operational monitoring is staff turnover. Staff turnover, which happens frequently in clinical care, made the fidelity of model adoption particularly chal-lenging,  because it required the need for ongoing education and training. Additionally, when new tools became available, old tools tended to be abandoned and not used. The participants described how difficult it was to make clinicians continue to use existing tools when newly launched tools could take priority. To mitigate these challenges, the participants suggested creating robust documen-tation standards and governance rubrics for every tool clinically integrated. Participants also emphasized the importance of creating new roles dedicated to overseeing operational monitoring. 5.9.3 Outcome monitoring. Outcomes were measured across various domains, including patient health outcomes, equity in patient care, financial outcomes, and staff satisfaction with the tool. Monitoring outcomes could be challenging in general, especially when there was a lack of infrastructure for quality monitoring. Partici-pants described the difficulty measuring the true impact of the component of a solution in isolation, because algorithms are often combined with treatment protocols, follow-up care, and changes in care-team responsibilities. This was particularly problematic when a model had been implemented for quite some time, as outcome measures defined prior to the model’s launch became less relevant benchmarks for the evaluation of current outcomes.Despite the challenges, outcome monitoring was still perceived to be important. For example, monitoring patient outcomes could address a limitation of technical monitoring when model perfor-mance might not be a reliable measure for patients receiving ap-propriate care. Monitoring staff satisfaction with the tool would inform why end users might use or not use the tool and identify areas of improvement.5.9.4 Cadence of monitoring reviews. The participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software mon-itoring and its use was important. The participants emphasized that additional monitoring might be required when any updates or unexpected events (e.g. implementation of new medical equipment within the relevant workflow) took place in clinical settings. They Noted that frequent monitoring would require significant resources,as more models were put into practice. To make the monitoring process more feasible, the participants suggested the creation of channels for users where they could report any concerns related to the models and potential adverse events that happen in clinical care. The frequency of AI system monitoring varied widely across organizations and use cases, ranging from 24/7 to once every 3 years.5.9.5 Accountability and ownership. The participants reported that there was no clear ownership of lifecycle management activities. Participants often recommended a centralized group with dedicated resources be accountable for monitoring the AI software. They proposed that joint accountability between clinical, statistical, and technical providers would be ideal for continued use of the model. One challenge that they raised was how to engage clinicians, particularly end users, as product owners, as they were often stretched between too many responsibilities already. 5.10 Update or decommission the AI products and impacted ecosystemThis thematic area describes how organizations make updates to or decommission the AI software. Many participants shared that they had not yet updated or decommissioned a model. These participants reported their aspirations for a future state of organizational governance. Representative quotations are presented in AppendixK.5.10.1 Update. The participants reported that they would consider updating the model when changes in clinical care, intervention outcome, and IT environment were detected. They appreciated that often a model cannot be simply retrained, because the prior clinical integration changed the environment. Participants also recommended conducting user studies and quantitative analyses to understand potential problems to address before deciding on model updates.5.10.2 Decommission. Participants reported that they would consider decommissioning a model if the model performed poorly, when safety concerns emerged, when the model would be replaced by a newly available better model, when there was no value in the clinical use case, and when the model was no longer finan-cially supported. A rare example of model decommissioning was observed with software used for capacity management during the early stages of COVID-19. Some participants reported that they had to decommission software that was developed during the peak of the COVID-19 pandemic because they no longer found it to be useful.The participants indicated that decisions to decommission a model are very challenging, especially when there is no clear own-ership or delegation of responsibility defined for monitoring the model. However, they noted that when a serious safety issue is identified, even with little notice, a decision to decommission the model should be made urgently.",1398-1404,"participants reported that while it would not be practical to constantly have clinicians monitor model performance, periodic review of AI software monitoring and its use was important."
