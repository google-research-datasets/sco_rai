,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{benbouzid2023, author = {Benbouzid, Bilel}, title = {Fairness in Machine Learning from the Perspective of Sociology of Statistics: How Machine Learning is Becoming Scientific by Turning Its Back on Metrological Realism}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-12, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,AirthmeticEqualityOfOutcome,Goal,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,pure arithmetic equality of outcomes across group,
10,USLaw,Agent,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,The legal concepts corresponding to this objective in the United States,
11,EuropeanLaw,Agent,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,The legal concepts corresponding to this objective [...] in Europe,
12,StatisticalParityMetrics,Artifact,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,"statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) ",
13,CDD,Artifact,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,what they term “conditional demographic disparity” (CDD).,
14,ComputingSystems,Artifact,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,Computing systems,
15,ClassifactionAlgorithms,Agent,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,classification algorithms,
16,DisparateImpact,Artifact,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,disparate impact,
17,IndirectDiscrimination,Artifact,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,indirect discrimination,
18,LegalConceptionOfFairness,Causal_Theory,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,correspond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress.,
19,NoAutomaticDiscriminationDetection,Causal_Theory,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,"cannot and should not be designed to automatically detect, evaluate, and correct discriminatory decisions independent of local judicial guidance and interpretation.",
20, , , , , ,
21, , , , , ,
22, , , , , ,
23,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
24,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
25,AirthmeticEqualityOfOutcome,constrainsAgent,ClassifactionAlgorithms,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,"according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across group"
26,USLaw,hasProducedArtifact,DisparateImpact,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,"The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. "
27,EuropeanLaw,hasProducedArtifact,IndirectDiscrimination,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,"The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. "
28,StatisticalParityMetrics,reflectsPrecept,LegalConceptionOfFairness,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,"statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) correspond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress."
29,CDD,reflectsPrecept,LegalConceptionOfFairness,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “conditional demographic disparity” (CDD).
30,ComputingSystems,reflectsPrecept,NoAutomaticDiscriminationDetection,"Legal scholars nonetheless still consider the family of statistical parity metrics to be the only ones that are consistent with anti-discrimination law. They look at the problem of algorithmic fairness from a different perspective. On their view, according to the legal regime that classification algorithms must adhere to, their aim should be pure arithmetic equality of outcomes across groups – or that, at the least, they should allow for the identification of greater or lesser inequalities in predictions in order to limit disparities between groups. The legal concepts corresponding to this objective in the United States and in Europe are known as “disparate impact”and “indirect discrimination”, respectively. Both imply the use of statistics as a source of evidence.A full comparison of the differences between these two notions and their implications for the conception of fairness in machine learning is beyond the scope of this article (for an in-depth analysis,see [46]. Here we will look only at the European case, through the example of the debate sparked by the work of [47]. Their le-gal analyses show that statistical parity metrics (which have been criticized by machine learning researchers, as we saw above) corre-spond more closely to the legal conception of fairness because they shape algorithmic decisions in the direction of compensation and redress. They have considerable potential to drive the critical re-evaluation of behaviors and social conventions that are embedded in the learning data. In other words, anti-discrimination law demands that decision algorithms narrow the gap between vulnerable and privileged groups. The challenge in this context is to combine an algorithmic decision support system and a politico-legal program for acting on (and indeed, changing) the state of the world. It is on the terms of this challenge that this particular family of metrics is compatible with the law. According to Wachter and her collabora-tors, only metrics that are compatible with a corrective conception of social justice are justifiable in terms of anti-discrimination law.More specifically, based on an analysis of case law from the Euro-pean Court of Justice (ECJ) , they show that the relevant principle inEuropean law and jurisprudence is “contextual equality.” They find that the technical measure of fairness that is closest to the ECJ’s“gold standard” for assessing discrimination is what they term “con-ditional demographic disparity” (CDD). This is the statistical parametric with an added conditional constraint that is represented by one or more variables. The metric’s conditional character enables its adaptation to the “contextual equality” criterion set out in caselaw. In the debate on fairness in machine learning, these legal schol-ars do not just propose that a given metric be chosen over another.They propose to change the very terms in which the problem of fair-ness is posed. Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct dis-criminatory decisions independent of local judicial guidance and interpretation. Instead what is needed is an automated “early warn-ing system” for discrimination. This requires designing systems that can automatically produce the kinds of statistical evidence that the judiciary needs to make informed normative decisions, and for the system's auditors to systematically detect potential discrimination before it occurs. In other words, what is needed are coherent tech-nical standards that align with the reference criteria of the judiciary to assess algorithmic discrimination. These two types of metrics,equality of opportunity and statistical parity, reflect two different processes: the algorithmic struggle for a social justice whose nature is always contested, and the algorithmic determination of what is fair according to case law, which is constantly changing. The situation is made all the murkier by the fact that AI systems must pursue these two programs at the same time.",38-40,"Computing systems, they argue, cannot and should not be designed to automatically detect, evaluate, and correct discriminatory decisions independent of local judicial guidance and interpretation."
