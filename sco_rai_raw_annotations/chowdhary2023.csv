,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{chowdhary2023, author = {Chowdhary, Shreya and Kawakami, Anna and Gray, Mary L and Suh, Jina and Olteanu, Alexandra and Saha, Koustuv},
title = {Can Workers Meaningfully Consent to Workplace Wellbeing Technologies?},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,Participants,Agent,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5","participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates).",
10,People,Agent,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",people,
11,LackOfOngoingConsent,Perceived_Need,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",do not typically support evolving or ongoing consent,
12,PerspectivesOnPassiveSensing,Perceived_Problem,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5","varying perspectives about consent, the workplace, and passive sensing",
13,ChallengesToMeaningfulConsent,Perceived_Problem,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace.,
14,PerceptionOfEmployerMotive,Causal_Theory,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit.,
15,DifficultyOfConsent,Perceived_Problem,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",the high level control employers have over workers' needs makes consent to workplace technologies especially difficult.,
16,LackOfKnowledgeOfLaborRights,Perceived_Need,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",workers may not know what their rights are in the workplace.,
17,LackOfHIPAAUnderstanding,Perceived_Need,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5","are aware that HIPAA exists, that it protects them, but they don't understand anything about the context",
18,FearOfDisclosure,Perceived_Problem,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5","love to tell my manager, but I don’t really want [Amellio] to out me as autistic.",
19,LackOfAccountability,Perceived_Need,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5","accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests.",
20,PowerImbalance,Causal_Theory,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5","power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers",
21,ConcernsAboutSensitiveData,Perceived_Problem,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",concerned that the nature and amount of data collected are sensitive and could lead to negative consequences.,
22,ConcernsAboutTrustworthiness,Perceived_Problem,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",concerns about the technology being fundamentally un-trustworthy,
23,LackOfTransparency,Perceived_Need,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent.,
24,SensingTechnologies,Artifact,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",sensing technologies,
25,InabilityToPartiallyConsent,Perceived_Problem,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",unable to decline consent to only parts of the technology without losing all access,
26,TradeOffs,Perceived_Problem,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm.,
27, , , , , ,
28,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
29,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
30,PerspectivesOnPassiveSensing,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5","We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates)."
31,ChallengesToMeaningfulConsent,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace.
32,PerceptionOfEmployerMotive,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit.
33,DifficultyOfConsent,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult.
34,LackOfKnowledgeOfLaborRights,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",Participants remarked that workers may not know what their rights are in the workplace.
35,LackOfHIPAAUnderstanding,constrainsAgent,People,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5","People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context."
36,FearOfDisclosure,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5","there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic."
37,LackOfAccountability,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5","Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests."
38,PowerImbalance,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5","P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers"
39,ConcernsAboutSensitiveData,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences.
40,ConcernsAboutTrustworthiness,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",expressed concerns about the technology being fundamentally un-trustworthy
41,LackOfTransparency,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",Participants believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent.
42,SensingTechnologies,influencesPrecept,LackOfOngoingConsent,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",sensing technologies do not typically support evolving or ongoing consent
43,InabilityToPartiallyConsent,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",Participants felt unable to decline consent to only parts of the technology without losing all access
44,TradeOffs,constrainsAgent,Participants,"We aimed to recruit participants with varying perspectives about consent, the workplace, and passive sensing, including managers, legal professionals and policymakers, AI builders (developers, design-ers, researchers), and people with experience negotiating privacy in the context of a power asymmetry (i.e., union organizers, patient ad-vocates). We recruited participants in August 2022 using a snowball and convenience sampling strategy by posting on organizational email lists and directly contacting people in different roles through emails and direct messaging. Interested respondents were provided with a consent form and responded to an in-take demographic survey (Table 1). We had 15 participants from 10 organizations of varying sizes, types (technology, consulting, research-based, non-profit), and four global locations (U.S., U.K., Canada, and India).Participants were compensated with $50 USD Visa gift cards. [...] 4.1 RQ1: Challenges to Meaningful ConsentWe found four broad themes related to challenges: 1) facets of inherent power differential in the workplace, 2) consequences of the power differential, 3) inherent risks posed by the technology, and4) technological barriers to supporting affirmative consent. Fig. 1 overviews these themes of challenges, which we discuss below.4.1.1 Facets of inherent power differential in the workplace. Participants described challenges to meaningful consent that were facets of an inherent power asymmetry in the workplace. Little guarantee to (data) privacy in the workplace. Participants noted workers are currently not guaranteed data privacy in the workplace. They pointed out employers are allowed to collect vast amounts of data on workers and are deemed owners of anydata produced by employees, compromising consent and privacy.P14 expressed distrust about employers’ ownership of workplace devices: “There’s just an inherent distrust I have about the security of the data collected. The fact that it’s collected at the workplace makes me think, who needs to know, and why? And if it is collected on devices installed by my employers, they are necessarily connected to the data storage. So these questions need to be addressed before someone can meaningfully consent to be part of this.”Participants also thought the legal landscape that guarantees employers' ownership of work devices limits the effectiveness of strong data security practices, and the black box nature of these technologies could lead to surveillance workers may not be aware of; with P13 noting: “I am concerned that I don’t have a good reason what my company is doing otherwise to monitor my activity. I Have no way of knowing if they have a keylogger on my machine, or if they have any sort of remote desktop watching stuff. Just because they have consent doesn’t mean that it doesn’t exist, and so, even if this is completely encrypted and secreted away and management has no insight into the data or anything after, they can still see if Iclick in [to Amellio], hypothetically. I cannot know that.” The workplace bottom line takes precedence over workers interests. Although Amellio is framed as supporting worker well-being, participants voiced their distrust e.g., “If my manager is encouraging me to use this, my first question would be, why? What's The benefit for them and what’s the visibility that they have over the system?” [P15]. Participants with a labor rights background called attention to what they thought was employers’ ulterior motive—reducing healthcare costs and boosting productivity to increase profit. P12 described how employers often make decisions that ultimately benefit them, even when they claim to support workers: “My employer once required me to install something that reminded me to look away from the screen and stop typing for a bit to avoid some eye disease or carpal tunnel syndrome. These protect the interests of the employer. I could never imagine a world where an employer says, ‘it looks like you’re really stressed out, you should take advantage of the unlimited vacation;’ there are certain things that are good for your health but against the employer’s interests. “Employers have a high level of control over workers’ needs. Participants noted how the high level control employers have over workers' needs makes consent to workplace technologies especially difficult. P14 expressed concern about how workers are at risk because “the power differential is related to your ability to make a living” and expressed skepticism about whether this power differen-tial can ultimately be mitigated at all. In addition, US workers rely on employers for health insurance, which makes sharing health data with employers especially risky. To navigate through this risk,P8 reframed consent as a relational process dependent on how trust-worthy their employer was and how reliant they were on them,“The chance that my employer will use [my data] to model eventu-ally my [health] benefit, is a possibility I don’t want to contribute to. That’s where it is a relational thing. If I do trust my employer to do the ethical thing compared to some other company where I Wouldn't feel that way, compared to working at a nonprofit where they don’t actually give me much benefits anyways, so it’s OK.”4.1.2 Consequences of the power differential. Participants described other challenges to consent that could be characterized as conse-quences of the inherent power differential in the workplace.Information asymmetry about workplace rights and use of collected data. Participants remarked that workers may not know what their rights are in the workplace. This is not only caused by the legal landscape that may not properly protect workers’ rights,but also because employers may not transparently communicate those rights. Drawing on their experience as a union organizer,P13 pointed out HIPAA, emphasizing that the lack of knowledge might prevent people from being able to make informed decisions: “People are aware that HIPAA exists, that it protects them, but they don't understand anything about the context. They think it applies to their employer. Of course it doesn’t.”Participants thought that information asymmetry is also caused by the technology’s black box nature. They were concerned about the risks related to the data and the company’s stewardship and use of the collected data. P5 asked “[t]his data is going to fit which tasks? Who is responsible for that? What is the risk associated with this? What is the actual privacy action plan? Where can I find documentation to read more? How do you ensure the system is responsible maintained over time?” P5 also expressed a desire for greater transparency about how the technology is used.Workers may not be able to negotiate. Participants raised con-cerns about workers’ inability to negotiate due to existing power dynamics and critiqued the idea of a boundary-setting conversations the power difference between workers and managers makes true negotiation difficult. P11 noted the difficulty of being honest with managers: “the people who are maybe uncomfortable to address these concerns to the manager or have no intention of actually using[Amellio], they would certainly not show up for such a conversa-tion.” For P13 the boundary-setting conversation with the manager is like “outing yourself as someone with concerns.” Participants Also worried about penalties for voicing concerns. P12 brought up the context of being neurodivergent: “there are certain concerns I Would love to tell my manager, but I don’t really want [Amellio] to out me as autistic. I’m not gonna say that to my manager. If Ihave concerns with the system that is against the interest of the employer, I’m not going to say that to my manager.” Lack of accountability. Participants brought up accountability as a missing criterion for consent, arising from how employers own the technology and prioritize their interests. P13 expressed skepticism about the efficacy of the boundary-setting conversation because employers are not beholden to any legitimate responsibility to workers: “I have no reason to believe that my manager has the answers to these, or is going to tell me the actual answer to this—he’s not beholden to any sort of oath of truth here and can just say whatever wants.” Participants also expressed apprehension about the potential misuse of their data and data-driven inferences. In fact, not only the original terms, but also the redesigned consent interfaces, could not help clarify P5’s questions about whether the company owned the data and what their accountability processes looked like. Another element here is the lack of adequate legal means for accountability: “The main way in which I’ve seen the company breaking the law is through unfair labor practices, a wide range of things where they’re impinging on worker’s right. This Goes to the National Labor Relations Board, which is very slow and bureaucratic. There are very low stakes for the company and they can pay lawyers to beat down whoever else. [..] There would need of a rapid turn around and a very severe consequence” [P13].Workers may feel coerced into consenting or may not be given an actual choice to consent given the employers’ control over workers. Several participants described consent in the work-place as “tacit” or “expected”. P2 attributed this to the “overarching agreement between yourself and your employer” which thus made consent less “staggered or point-to-point”. P1 similarly expressed that employment carries an expectation of agreeing and their con-sent could even be implicitly assumed. Others pointed out how this implicit expectation could manifest in different ways, even when there is increased specificity or space for negotiation. For example,after reviewing the re-designed consent interface, P14 noted valuable improvements but still felt “there’s an assumption from the beginning when I’m reading this, I’m going to say yes, so it already feels odd.” Participants also felt that even though the boundary-setting conversation intended to create space for negotiation and reflection for the worker, it could feel coercive, as another effort to make the worker agree. P5, P8, P9, and P10 named the power imbalance between workers and managers as a source of potential coercion, expressing that workers may feel coerced into consenting out of a desire to not disobey or disappoint their managers (P10), or fear of damaging their relationship with the manager (P5). In Contrast, other participants found it coercive only if workers were individually targeted but “if this was just given out as a blanket thing to all employees, then I think that would be fine, compared to specifically targeting an individual” [P3]. Additionally, participants described how workers may also feel coerced out of fear of e.g., losing their job: “I think workplace have nuances like how much power the employee has. The Amazon warehouse workers probably wrote somewhere that they consent to wear that weird band. But the recourse is, well, then you won’t work here” [P12]. Another participant, P13 described experiences with technically being given a choice but they felt coerced to consent, because of penalties or that “there might be a lot of consequences.”4.1.3 Inherent risks posed by the technology. Another set of challenges that emerged from our focus groups related to the inherent nature of passive sensing technologies. Passively collected data and data-driven inferences are sensitive and risky. Participants were concerned that the nature and amount of data collected are sensitive and could lead to negative consequences. P4 was concerned about how Amellio’s use of a cam-era might also threaten the privacy of others in the workplace, whileP2 noted that through passive sensing, previously “non-sensitive”data collected in aggregate could lead to sensitive inferences and require stricter regulation to protect privacy. Other participants also noted that even the collection of seemingly non-sensitive and harmless data in large quantities could lead to other risks e.g., “the metadata surrounding [Amellio], even if I am getting the feedback just for me, I don’t have any faith about how the rest of it is being stored, how secured the system is, and I think it could lead to penalty down the road, even if it is not the intent” [P13].The technology itself may not be trustworthy (fair, unbiased,accurate & beneficial). Some participants, especially AI builders,expressed concerns about the technology being fundamentally un-trustworthy. P15 questioned the validity, “I say this as someone who has some kind of understanding of AI. Maybe someone from a different background wouldn’t have the same reaction. I simply think it is not possible to infer emotions from facial expressions.[..] there’s a lot of gray areas with this type of technology andI don’t really see the value of it either.” Likewise, P5 question the construct validity and potential inaccuracies in Amellio’s infer-ences: “I want to know the assumptions and proxies used, because happiness, excitement, and distress are social constructs, and have many interpretations related to culture, for example. It’s better to include some sort of clarification of how [Amellio] identifies them.”Participants were concerned about potential biases and related consequences e.g., if the app mistakenly assesses them as alwaysunhappy or unsatisfied (P11). P13 noted that being neurodivergent,they were concerned about the technology’s reliability and benefits:“as someone who’s neurodivergent, things like this [technology]frequently doesn’t work for me. [..] Is this likely gonna do more harm than good? My distractibility or other things might be com-pletely different from other people.” Participants sought evidence about the technology’s efficacy: “I would like to see studies or some evidence backing up that tracking these kinds of data can actually improve employee health or wellbeing” [P3].4.1.4 Technological challenges to supporting meaningful consent.These challenges relate to the design and mechanics of wellbeing sensing technologies, as well as to how people interact with them.Information asymmetry about the technology. Participants Believe the lack of transparency about how the technology works and its limitations make it difficult to provide meaningful consent. After reviewing the re-designed consent interfaces, P14 critique the design for not providing adequate information: “You’re asking to check boxes to agree before the complete information is offered.We haven’t described data analysis yet, so how can you consent? It's Not true consent if we don’t understand how the information could be used, and the risks also need to be described before they can truly consent.” Similar concerns were raised about the boundary-setting conversations, with P8 emphasizing the lack of background knowl-edge as “setting up a conversation between a developer assumes that you will be able to have a conversation,” and P14 emphasizing that the “uninformed don’t even know what they don’t know.”Technology may not support ongoing consent. Participants noted that sensing technologies do not typically support evolving or ongoing consent, i.e., consent should be an ongoing process [80].They stressed how technologies often collect massive amounts of data in the background, but rarely support the “reversible” dimen-sion of FRIES. P7 described how most services, after securing the first consent agreement, do not present an easy way to review,revise, or back out of the agreement: “After I’ve given consent to something, it’s always presented to you as the front door to enter[..] And then, after that point, it gets forgotten and there isn’t a clear way to revise that consent or go back and be like, hey, I don't remember what the exact terms are.” P2 was similarly concerned with sensing technologies being ill-equipped to handle ongoing consent over a long period of time as the technology itself evolves.Cannot say no without losing access to the technology. Participants felt unable to decline consent to only parts of the technology without losing all access. P6 pointed out how the initial terms of Amelia asked for consent as all-or-nothing and violated the “specific” dimension of FRIES: “You are either agreeing to all of these data being collected or none of it, so it is definitely not specific consent.” Multiple participants thought the re-designed consent interface better-supported specificity, especially the option to select which specific data they wanted to share. Those with AI building experience, however, were hesitant if this level of specificity could be technically supported. P10 contemplated the possibility of dis-entangling a specific data stream from the technology’s accuracy,but found it somewhat impractical “now that everything is basically a deep neural network where we don’t understand the internals. “Drawing on their expertise in developing passive sensing tech-nologies, P1 also noted that decoupling the data sharing from the insights might not be possible as users “can’t be like, hey, I wanna see how my sleep looks, but I’m not enthusiastic about giving my data [to FitBit]–whose sleep data would you be looking at?” Participants also thought this violates other FRIES dimensions,such as “freely given” and “enthusiastic,” as workers might feel they had no choice but to consent. Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm. P6 noted how, under standard consent paradigms, they often find that enthusiasm was irrelevant and not supported at all: “I really don’t think [enthusiasm] is the norm for privacy consents [..] I know I don’t really trust Facebook But I really value what I’m getting out of using Facebook. So it’s like a trade-off.” Similarly, P3 expressed, “I feel sometimes you don’t have to be enthusiastic. You could be more neutral about it, especially when wanting to use a certain service or product. Sometimes I Just wanna use a product, and maybe I’m not enthusiastic about giving them my data and some personal information, but I do it asa trade-off for having that kind of access. ","571, 572-5",Participants reflected on how the inextricability of “good stuff” and “bad stuff” creates a situation where the decision to consent becomes ultimately about trade-offs and not so much about enthusiasm.
