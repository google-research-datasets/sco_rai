,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{candela2023, author = {Qui\~{n}onero Candela, Joaquin and Wu, Yuwen and Hsu, Brian and Jain, Sakshi and Ramos, Jennifer and Adams, Jon and Hallman, Robert and Basu, Kinjal}, title = {Disentangling and Operationalizing AI Fairness at LinkedIn}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,LinkedIn,Agent,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,LinkedIn,
10,PeopleAlsoViewed,Artifact,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,"a people recommendation product [...] ""People Also Viewed""",
11,NoReductionInFairnessGap,Artifact,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,"no reduction in the fairness gap,",
12,ClosedFairnessGap,Artifact,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,reduced the gap in offline experiments,
13,NoUnintendedConsequences,Artifact,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,We therefore did not find unintended consequences,
14,GenderBias,Perceived_Problem,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,an observed gap in predictive parity between male and female members,
15,NoLargeGenderSkew,Goal,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,we did not see a large gender skew for the source or destination members,
16,NoUnjustifiableFeatures,Goal,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,did not reveal any unjustifiable features.,
17,AddingGenericFeatures,Strategy,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,we observe that adding generic LinkedIn features ,
18,GenderAwareApproaches,Strategy,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,Gender-aware approaches,
19,BiasMitigationTraining,Strategy,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,bias mitigation training (BMT) algorithm ,
20, , , , , ,
21, , , , , ,
22, , , , , ,
23,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
24,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
25,LinkedIn,hasProducedArtifact,PeopleAlsoViewed,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,a people recommendation product at LinkedIn called “People Also Viewed”
26,PeopleAlsoViewed,reflectsPrecept,GenderBias,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members.
27,PeopleAlsoViewed,reflectsPrecept,NoLargeGenderSkew,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,"In the case of PAV, we did not see a large gender skew for the source or destination members"
28,PeopleAlsoViewed,reflectsPrecept,NoUnjustifiableFeatures,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,An audit of the PAV model features did not reveal any unjustifiable features.
29,NoReductionInFairnessGap,reflectsPrecept,AddingGenericFeatures,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,"we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy."
30,ClosedFairnessGap,reflectsPrecept,GenderAwareApproaches,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,Gender-aware approaches consistently outperformed gender-blind options to close the gap.
31,ClosedFairnessGap,reflectsPrecept,BiasMitigationTraining,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,"BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online."
32,NoUnintendedConsequences,reflectsPrecept,BiasMitigationTraining,"This section illustrates how the methods described in Section 3 are applied in a real-world example. We focus on a people recommendation product at LinkedIn called “People Also Viewed” (PAV, see figure 4a and the appendix). This product recommends addi-tional profiles you might be interested to connect with or learn from based on the current profile you are viewing and is an example of the scarce resource allocation paradigm (limited number of recommended profile slots). The product is the second largest source of traffic for profile views, therefore any negative bias (failing to surface qualified recommendations) could adversely affect members' abilities to grow their network. On the other hand, ‘over-recommending’ members can lead to harm such as unwanted connection requests and spam. In this case study we focus on measuring and mitigating bias for binary gender. Equal AI treatment in this case means that the model that recommends related profiles for PAV produces ranking scores that satisfy predictive parity across binary gender. The motivation for this use case is an observed gap in predictive parity between male and female members. Figure 4b illustrates the real-world impact of lack of predictive parity. Consider a group of female and male members that share a similar AI relevance score: the females in that group will see higher real-world outcomes of actual network formation. This means that the score should have been higher for the females in that group. The model is therefore under-predicting for females. Meeting equal AI treatment would require the two curves in 4b to overlap.The remainder of this Section details our approach to mitigating this predictive parity gap. Although we focus on binary gender, the framework can be extended to other demographic groups, assuming there is sufficient data. We follow the steps in Section 3 to mitigate the observed gap in predictive parity.4.2.1 Root-Cause Analysis. Our framework provides a standard set of investigations that can provide insights into the mechanisms leading to a (gender) gap 5.(1) Distribution of binary gender: The obvious first step is breaking down the data to see the ratio of binary gender;in the most trivial case, if one gender is missing from the data, it would explain why the model is miscalibrated for this group. In the less extreme case, binary gender skew could still contribute to bias if the relevant signal for one gender is not large enough for the model to learn predictions for each gender. In the case of PAV, we did not see a large genderskew for the source or destination members (see Table 1).6.(2) Feature justifiability: Building AI models responsibly requires that we only include features if they are justifiable. Although we do not give a formal definition, the use of feature is justified, for example, if the feature is relevant to the modeling task. Domain knowledge and common sense are required to judge that relevance in context.The justifiability bar is highest for demographic information or other highly confidential data. In addition to privacy and security concerns, including these data in a model could exacerbate biases or lead to other unintended consequences. This is why we ask that the unjustifiable features be removed even if model performance drops. Removing a feature that isn't justified doesn’t mean all other features correlated with it also need to be removed. On the contrary, as we discuss in Section 4.2.2, one way to close the predictive parity gap is to add relevant correlated features on a causal chain. An audit of the PAV model features did not reveal any unjustifiable features. (3) Cohort-level Error Analysis: We have observed that PAV is miscalibrated when segmenting on binary gender. How-ever, the metric is a population-level average, meaning we do not have insight into the level of miscalibration for dif-ferent subgroups. To get a more granular understanding of what is causing this binary gender-based gap in predictive performance, we perform a cohort-level analysis.We build a tree model to split the data using a user-defined er-ror metric. The splits are determined such that the partition of the data maximize the differences in the error metric. Theresulting cohorts and the features used in the splits can givemore granular insight into model performance compared to standard feature importance methods [42], as the data is automatically segmented into high and low-error cohorts.Furthermore, we can include non-model features as candi-dates in the cohort splits, and we can also use a non-model error metric as the splitting criterion [76].For the fairness use case, we set the metric as the residual between the label and the predicted score. This is a proxy for miscalibration error, which is essentially what predictive parity tries to measure. As the goal is human-interpretable cohorts, we limit the depth of the tree to be three, so that only eight cohorts are generated. For candidate features, we include the gender of any member in the model (for PAV, these would be the viewer, current profile member, and rec-ommended member) in addition to the actual model features.Figure 7 (see appendix) shows the result of running the er-ror cohort analyzer on PAV data. The error cohort model used the binary gender of both the recommended member and the current profile member as top feature splits, with MALE cohorts generally having lower residuals compared to FEMALE cohorts. This suggests binary gender could be a driving factor for the measured bias, increasing the likelihood that gender will need to be directly used for mitigation. 4.2.2 Mitigation experiments without demographic information. Mitigation of fairness violations is typically studied in contexts that allow access to demographic information during model training and inference. However, a key component of our justifiability framework is our proposition that demographic data should only be used when other methods are demonstrably inadequate, and when we were unable to find negative unintended consequences from mitigating using demographic data. We therefore evaluated a wide range of methods for mitigation without demographic data at infer-ence time, from feature-selection methods to fairness-constraints in-processing methods. This section provides an overview of these techniques, and presents our rationale for ultimately choosing to mitigate with demographic data.The problem of closing the gender calibration gap without us-ing gender as a feature can be cast as one of identifying missing features that are relevant and correlate with gender. We motivate this approach by visualizing gender and model features in a causal graph [61], where the terminal node is model prediction. Our goal is then to determine if there are additional, non-gender features that could ‘block’ the effects of gender on the gap such that the gap is reduced. Let’s consider a toy model with only two features; we show the causal relationships between the features, model predictions, and binary gender (which is not included as a feature) in a directed acyclic graph (DAG) (see Figure 5a). The original model is biased because binary gender is causally related to the predictions through a direct path. Imagine we identify a new feature, click probability, that is a descendant of binary gender and fully captures its effect. When click probability is included in an updated model version,binary gender no longer has a direct effect on the model predictions(see Figure 5b). We can therefore theoretically close any gender-based bias by including missing features in the direct causal path of binary gender. A caveat of this approach is that our toy example only shows the ideal case where the new feature closes the gap and does not have additional interaction effects. In reality, any newnode in a large graph can drastically change the DAG by adding new edges and therefore changing causal relationships [36]. New Features in the direct path of binary gender may even widen the gap, therefore we cannot indiscriminately add features we think are correlated with binary gender and outcome.One approach for a more intentional mitigation strategy is to leverage domain expertise to identify relevant missing features;however, it is difficult to scale these ad-hoc solutions. We therefore first created a superset of common LinkedIn member features and tested automated feature selection algorithms to pick the subset that could close the gap. The two main strategies we tested were Quantile Prediction Drift (QPD) and a method based on causal effect decomposition [18]. We also tested more ‘standard’ techniques such as brute force search of the feature space and imputation of missing feature values (identified as another possible issue from root cause analysis). Finally, we ran experiments including binary gender in the model to get a direct comparison between gender-aware andgender-blind techniques. The results are outlined in Table 2 and we observe that adding generic LinkedIn features showed no reduction in the fairness gap, regardless of the feature selection strategy. Gender-aware approaches consistently outperformed gender-blind options to close the gap. While we have focused on data-based mitigation methods, there are also other avenues of mitigation. Some examples include tun-ing model parameters to optimize for calibration, training with calibration-motivated loss functions, or post-processing with group-agnostic calibrators. As part of our efforts to identify effective demographic data-restricted methods for mitigation, we conducted a wide survey of methods that varied in both the point of intervention (pre-/in-/post-processing) as well as the volume of demographic data required (e.g., only in training vs. in training and inference). We present this survey in a separate paper, see Hsu et al. [43], but note here that the overwhelming conclusion is that using gender in post-processes was by far the most effective strategy for achieving predictive parity fairness. Given that our thorough experimentation with gender-blind and gender-limited techniques did not lead to successful mitigation,we felt justified in using the post-processing gender-based calibration to mitigate the algorithmic bias in PAV.4.2.3 Mitigation experiments with Demographic Information. We Use the post-processing technique detailed in DiCiccio et al. [23] a sour bias mitigation training (BMT) algorithm which requires the use of demographic information. Intuitively, BMT fulfills predictive parity by setting 𝐸 [𝑌 | ˆ𝑌 = 𝑠, 𝐴 = 𝑎] = 𝑠 for all groups 𝐴 (here 𝑠 is the predicted score from the model). As shown in Table 2, BMT(row 1) successfully reduced the gap in offline experiments. Based on our root cause analysis and offline mitigation experiments, we concluded that BMT was effective (offline) and should be experimented online. We launched an online A/B test comparing the baseline PAV model with a BMT-mitigated version (Figure 6a and 6b) that also showed a reduction in the predictive parity gap. Furthermore, the BMT model showed significant 2% lift in total profile actions (e.g., clicking the recommended profile), which is the main product metric, confirming our hypothesis that equal treatment need not sacrifice general model performance. 4.2.4 Measuring unintended consequences. Ensuring equal AI treat-ment in our case study requires that the gap between AI scores attributed to males and females be closed. Relative to male scores,female scores will now be higher. The product consequence of this is that a higher proportion of female profiles will be shown on the PAV product. Before we can ship this equal AI treatment in-tervention, aimed at eliminating predictive bias, we need to make sure there are no negative real-world consequences. One potential risk is that females might now receive more unwanted messages,invites, or even harassment. To quantify this risk, we built edge-level (member → female) metrics that measure the rate at which females accept connection request, respond to messages received Table 3 summarizes the experimental results. The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. ",1219-1222,"The treatment group accepts connection invitations at a higher rate and reports problematic invitations at a lower rate. This means higher networkformation and fewer unwanted connection requests. We observe slight a decrease in messaging response rates, tolerable given the other two positive outcomes. We therefore did not find unintended consequences that arise from ensuring equal treatment for PAV. "
