,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{venkatagiri2023diverseperspectives,
    author = {Sukrit Venkatagiri and Jacob Thebault-Spieker and Naomi Mine and Kurt Luther},
    title = {Diverse Perspectives Can Mitigate Political Bias in Crowdsourced Content Moderation},
    year = 2023
}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-11, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,FairnessReversal,Perceived_Problem,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically.",
10,WorkingProfessional,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,working professionals,
11,Communities,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,communities,
12,LawStudents,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,law students,
13,Students,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,students,
14,PeopleApplyingForCredit,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,people applying for credit,
15,AdultDataset,Artifact,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Adult: Dataset,
16,CommunityCrimeDataset,Artifact,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Community Crime: Dataset,
17,LawSchoolDataset,Artifact,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Law School: Dataset,
18,FairClassifier,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,a fair classifier,
19,StudentDataset,Artifact,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Student: Datset,
20,CreditDataset,Artifact,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Credit: Dataste,
21,Selectivity,Strategy,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"fairness is achieved predominantly through selectivity,",
22,Inclusiveness,Strategy,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,fairness is achieved predominantly through inclusiveness,
23, , , , , ,
24, , , , , ,
25, , , , , ,
26, , , , , ,
27, , , , , ,
28, , , , , ,
29, , , , , ,
30,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
31,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
32,FairnessReversal,constrainsAgent,FairClassifier,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically."
33,WorkingProfessional,hasProducedArtifact,AdultDataset,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Adult: Dataset of working professionals
34,Communities,hasProducedArtifact,CommunityCrimeDataset,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Community Crime: Dataset of communities
35,LawStudents,hasProducedArtifact,LawSchoolDataset,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Law School: Dataset of law students
36,Students,hasProducedArtifact,StudentDataset,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Student: Dataset of students
37,PeopleApplyingForCredit,hasProducedArtifact,CreditDataset,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Credit: Dataset of people applying for credit
38,AdultDataset,reflectsPrecept,Selectivity,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity,"
39,CommunityCrimeDataset,reflectsPrecept,Selectivity,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity,"
40,LawSchoolDataset,reflectsPrecept,Inclusiveness,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifier𝑓𝐹 becomes less fair than its conventional counterpart 𝑓𝐶 if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets 𝐵 for which the conventional clas-sifier 𝑓𝐶 exhibits greater fairness than the group-fair model 𝑓𝐹 . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by 𝑦 = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by 𝑦 = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the “advantaged” group(e.g. the group with higher PR for PR based fairness) as group 1,or 𝐺1, while the disadvantaged group is referred to as 0 or 𝐺0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for 𝑓𝐶 , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain 𝑓𝐹 , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to 𝑓𝐶 , and the rest are group-fairclassifiers 𝑓𝐹 for different values of 𝛼 (recall that higher 𝛼 entails greater importance of group fairness). What we observe is that in many cases, particularly when 𝛼 is not very high, there is a range of budget values 𝐵 for which 𝑓𝐹 becomes less fair than 𝑓𝐶 . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., 𝛽 = 0 is used in all experiments instead of 𝛼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at 𝐵 = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets 𝐵 keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both 𝑓𝐶 and 𝑓𝐹 , it appears amplified for some of the group fair classifiers 𝑓𝐹 .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier 𝑓𝐹 compared to 𝑓𝐶 .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under 𝑓𝐶 , but negatively under 𝑓𝐹 , is larger than the number of agents negatively classified by 𝑓𝐶 , but positively under 𝑓𝐹 , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of 𝑓𝐹 and 𝑓𝐶 (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (𝑓𝐶 ) includes few additional greenpoints (disadvantaged group) compared to the blue region (𝑓𝐶 ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for 𝑓𝐶 and 𝑓𝐹 ; in the first two examples the positive rates on both groups drops when switching from 𝑓𝐶 to 𝑓𝐹 , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and 𝑓𝐹 remains more fair than𝑓𝐶 over a broad range of strategic manipulation budgets 𝐵. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness"
