,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{venkatagiri2023diverseperspectives,
    author = {Sukrit Venkatagiri and Jacob Thebault-Spieker and Naomi Mine and Kurt Luther},
    title = {Diverse Perspectives Can Mitigate Political Bias in Crowdsourced Content Moderation},
    year = 2023
}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-11, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,FairnessReversal,Perceived_Problem,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically.",
10,WorkingProfessional,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,working professionals,
11,Communities,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,communities,
12,LawStudents,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,law students,
13,Students,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,students,
14,PeopleApplyingForCredit,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,people applying for credit,
15,AdultDataset,Artifact,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Adult: Dataset,
16,CommunityCrimeDataset,Artifact,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Community Crime: Dataset,
17,LawSchoolDataset,Artifact,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Law School: Dataset,
18,FairClassifier,Agent,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,a fair classifier,
19,StudentDataset,Artifact,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Student: Datset,
20,CreditDataset,Artifact,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Credit: Dataste,
21,Selectivity,Strategy,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"fairness is achieved predominantly through selectivity,",
22,Inclusiveness,Strategy,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,fairness is achieved predominantly through inclusiveness,
23, , , , , ,
24, , , , , ,
25, , , , , ,
26, , , , , ,
27, , , , , ,
28, , , , , ,
29, , , , , ,
30,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
31,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
32,FairnessReversal,constrainsAgent,FairClassifier,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically."
33,WorkingProfessional,hasProducedArtifact,AdultDataset,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Adult: Dataset of working professionals
34,Communities,hasProducedArtifact,CommunityCrimeDataset,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Community Crime: Dataset of communities
35,LawStudents,hasProducedArtifact,LawSchoolDataset,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Law School: Dataset of law students
36,Students,hasProducedArtifact,StudentDataset,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Student: Dataset of students
37,PeopleApplyingForCredit,hasProducedArtifact,CreditDataset,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,Credit: Dataset of people applying for credit
38,AdultDataset,reflectsPrecept,Selectivity,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity,"
39,CommunityCrimeDataset,reflectsPrecept,Selectivity,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity,"
40,LawSchoolDataset,reflectsPrecept,Inclusiveness,"Our central goal is to understand the conditions under which fairness reversal occurs in strategic settings, that is, when a fair classifierğ‘“ğ¹ becomes less fair than its conventional counterpart ğ‘“ğ¶ if agents act strategically. Fairness reversal occurs when there is a range of strategic manipulation budgets ğµ for which the conventional clas-sifier ğ‘“ğ¶ exhibits greater fairness than the group-fair model ğ‘“ğ¹ . In This section, we study this phenomenon empirically, demonstrating that it is commonly observed for several benchmark datasets. Datasets and Algorithms. For our empirical study, we use five datasets commonly used as benchmarks for group-fair classifica-tion: Adult: Dataset of working professionals where the goal is to predict high or low income (protected feature: gender) [13, 26]. Community Crime: Dataset of communities where the objective is to predict if the community has high crime (protected feature:race) [13, 36]. Law School: Dataset of law students where the ob-jective is to predict bar-exam passage (protected feature: race) [42]. Student: Dataset of students where the objective is to predict a student receiving high math grades (protected feature: race) [11, 13]. Credit: Dataset of people applying for credit where the objective is to predict creditworthiness (protected feature: age) [13].All five datasets have binary outcomes, and we label the more desirable outcome for the individuals by ğ‘¦ = 1 (e.g., having a high income in the Adult dataset), with the less desirable outcome la-beled by ğ‘¦ = 0. Consequently, higher positive rate (PR), true positive rate (TPR), or false positive rate (FPR) is more desirable for indi-viduals. Group membership in each dataset is determined by race,gender, or age which in these datasets corresponds to a binary feature (as in [23] the age feature is made binary by considering those older than 25 as Old, and those 25 or younger as Young). Detailed breakdown of the datasets can be found in Section E.7 of the Supplement. In all cases, we refer to the â€œadvantagedâ€ group(e.g. the group with higher PR for PR based fairness) as group 1,or ğº1, while the disadvantaged group is referred to as 0 or ğº0. In Our experiments, we only consider features that can potentially be manipulated (see Section E.7 of the Supplement for further details).W e use four types of conventional classifiers for ğ‘“ğ¶ , namely logis-tic regression (LGR), support vector machines with an RBF kernel(SVM), neural networks (NN), and gradient boosting trees (GB),and three group-fair approaches to obtain ğ‘“ğ¹ , Reductions [1], Gerry-Fair [24], and EqOdds [35]. The first two are in processing methods which learn a fair model direction on a given dataset, while the third remedies unfairness through postprocessing the predictions of a conventional classifier. To study strategic manipulation, we use a mix of local search for categorical features [28, 40] and projected gradient descent (PGD) for continuous features [31]; further details are provided in Section E.6 of the Supplement.Fairness reversals under strategic agent behavior. In Figure 1 we in-vestigate fairness reversals on three datasets with both Reductions And EqOdds fairness methods; additional experiments in Section Of the Supplement show that this illustration is representative in the sense that although fairness reversals do not occur in all cases,they are quite common. Consider first Figure 1 (top), which exam-ines settings where predictions do not take the sensitive features as an input (we call these group-agnostic classifiers). In these three plots, the dashed line corresponds to ğ‘“ğ¶ , and the rest are group-fairclassifiers ğ‘“ğ¹ for different values of ğ›¼ (recall that higher ğ›¼ entails greater importance of group fairness). What we observe is that in many cases, particularly when ğ›¼ is not very high, there is a range of budget values ğµ for which ğ‘“ğ¹ becomes less fair than ğ‘“ğ¶ . Moreover,in many cases, this range is considerable. In Figure 1 (bottom plots),where group-fair classifiers are group-aware, including the sensi-tive feature as an input, the fairness reversal phenomenon is even more dramatic (not that EqOdds attempts to achieve 0 unfairness between groups, i.e., ğ›½ = 0 is used in all experiments instead of ğ›¼)  In this experiment, when best responding agents are capable of misreporting their group as if it where a feature in x (fairness is still computed with true group membership). Due to the particular nature of EqOdds, specifically its handling of agents from different groups, we observe a sharp change in fairness at ğµ = 1, the precise budget for which misreporting group membership is feasible. Figure 1 exhibits several additional phenomena. Note, in particu-lar, that in many cases the unfairness (i.e., FPR difference between the groups) initially increases as the budget increases, but in all cases as budgets ğµ keep increasing, eventually unfairness vanishes as a result of strategic behavior by agents. Furthermore, much as we observe this initial unfairness increase for both ğ‘“ğ¶ and ğ‘“ğ¹ , it appears amplified for some of the group fair classifiers ğ‘“ğ¹ .What causes fairness reversal? As we formally prove below, the essential condition is selectivity of fair classifier ğ‘“ğ¹ compared to ğ‘“ğ¶ .Specifically, in binary classification, there are, roughly, two ways one can improve fairness on a given dataset (that is, without any consideration of strategic behavior); either through inclusiveness (positively classifying additional agents from the disadvantaged group by changing their predicted class to 1), or through selectivity (negative classifying some of the members of the advantaged group by changing their predicted class 1 to 0).Our key observation is that selectivity leads to fairness reversals, while inclusiveness does not. Specifically, we observe that as the number of agents positively classified under ğ‘“ğ¶ , but negatively under ğ‘“ğ¹ , is larger than the number of agents negatively classified by ğ‘“ğ¶ , but positively under ğ‘“ğ¹ , fairness reversals are morecommons.We illustrate this in Figure 2, which shows the decision bound-aries of ğ‘“ğ¹ and ğ‘“ğ¶ (top row), as well as associated fairness as a func-tion of budget (bottom row) for several combinations of dataset,classifier, and fairness definition. On the Adult and Crime datasets(first two columns), fairness is achieved predominantly through selectivity, as the orange region (ğ‘“ğ¶ ) includes few additional greenpoints (disadvantaged group) compared to the blue region (ğ‘“ğ¶ ),but excludes many blue points (advantaged group). This is given more precisely in terms of the respective group-wise positive rates for ğ‘“ğ¶ and ğ‘“ğ¹ ; in the first two examples the positive rates on both groups drops when switching from ğ‘“ğ¶ to ğ‘“ğ¹ , while in the third case the positive rate for both groups increases. This, in turn, leads to instances of fairness reversal (bottom row first column). In the Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness, and ğ‘“ğ¹ remains more fair thanğ‘“ğ¶ over a broad range of strategic manipulation budgets ğµ. The reason that selectivity leads to fairness reversal is that those from the advantaged group who are excluded tend as a result to be close to the decision boundary than those from the disadvantaged group.",391-3,"Law School dataset (third column), in contrast, fairness is achieved primarily through inclusiveness"
