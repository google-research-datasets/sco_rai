,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{benbouzid2023, author = {Benbouzid, Bilel}, title = {Fairness in Machine Learning from the Perspective of Sociology of Statistics: How Machine Learning is Becoming Scientific by Turning Its Back on Metrological Realism}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-12, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,PredPol,Agent,"One of the effects of this coexistence is to have made the question of “biases” more of a political than a methodological problem.Traditionally, from a realist perspective, bias is defined as a sys-tematic error (of measurement, reasoning, procedure, or judgment)that produces a deviation from the “truth.” In machine learning,researchers call the reality outside their models “ground truth,” and often define bias as a deviation from that truth [34]. Thus, while controlling biases is in general an essentially methodological chal-lenge, in machine learning it is a crucial scientific task, because it promises to offer a way of removing all traces of subjectivity,producing “mechanically” objective scientific statements [15].But when it is the ground truth itself that is biased because society is structurally unjust and unequal; when the technologies themselves are an integral part of unequal social structure (structur-ing the data); and when this ground truth can defined in multiple ways depending on the perceiver’s objectives and interests, then the question of bias can no longer be posed in realist terms. Biasesthen become social norms in themselves, which must be taken into account either to change society or, on the contrary, to choose todo nothing. Actors in FairML have given the concept of bias a new status and meaning – at least for engineers. This analysis of the close connection between quantification techniques and the social construction of reality has long since been widely taken up in the social sciences [35].In a context where data for connectionist AI systems are seldom based on a protocol specifically designed for the purpose, data sci-entists more readily take up a constructivist stance. For example,the developers of PredPol, a company that specializes in predicting crime in order to guide police patrols, recognize that the major ethical challenges of predictive policing revolve around locating the biases in the training data: these originate in the social interactions between the police, criminals and the public at large that produce reports of crimes. They admit that these statistical coding operations have feedback on reality, by way of the algorithmic systems that they are used to drive. The results of actions taken based on biased predictions themselves become training data, which can then reinforce and intensify existing inequality in the distribution of arrests or of protection across the population [36]. A good description of this algorithmic construction of reality through machine learning represents biases as features of a cyclical process consisting of three steps [37]: first, from data generation algorithm (for example, where historical or structural biases that reflect asymmetrical power relations in the social world are fed into an algorithm); next, from algorithm to user interaction(for example, the omission of variables, often related to developers prejudices and interests); and then from user interaction to data (for example, behavioral biases among users that are “undesired”by the designers).The many accusations leveled at algorithmic systems reveal a“sociology of biases” that shows that bias will never be eliminated by increasing methodological rigor. The substance of the critique of algorithmic decisions has to do not with the rigor of data col-lection, but with the system’s perspective on the world (in the sense of standpoint epistemology ) and its feedback effects. It's Simply impossible to produce a neutral decision system, and, for the users of these systems, bias is a problem in moral philosophy that arises in relationship to those who are “computed” (we will return to this point below). This why the main objective for the designer of algorithmic systems must be to avoid an unconsciously partial decision, while seeking to understand bias as an aspect of the machine’s interactions with the world, in the terms both of moral philosophy and of the sociology of inequalities. Hence the idea, shared by most FairML researchers, that algorithms may cre-ate the potential for new forms of transparency, and thus for novel ways of detecting discrimination [38]. To prevent discrimination,we must have the means to detect it, which can be very difficult with human decision-makers. While algorithms may increase the risk of discrimination, they also have the potential to facilitate its detection - and thus its prevention. With the field of fairness in machine learning, algorithms have become major political actors[39].",37-8,"the developers of PredPol, a company that specializes in predicting crime in order to guide police patrols",
10,FeedbackOnReality,Artifact,"One of the effects of this coexistence is to have made the question of “biases” more of a political than a methodological problem.Traditionally, from a realist perspective, bias is defined as a sys-tematic error (of measurement, reasoning, procedure, or judgment)that produces a deviation from the “truth.” In machine learning,researchers call the reality outside their models “ground truth,” and often define bias as a deviation from that truth [34]. Thus, while controlling biases is in general an essentially methodological chal-lenge, in machine learning it is a crucial scientific task, because it promises to offer a way of removing all traces of subjectivity,producing “mechanically” objective scientific statements [15].But when it is the ground truth itself that is biased because society is structurally unjust and unequal; when the technologies themselves are an integral part of unequal social structure (structur-ing the data); and when this ground truth can defined in multiple ways depending on the perceiver’s objectives and interests, then the question of bias can no longer be posed in realist terms. Biasesthen become social norms in themselves, which must be taken into account either to change society or, on the contrary, to choose todo nothing. Actors in FairML have given the concept of bias a new status and meaning – at least for engineers. This analysis of the close connection between quantification techniques and the social construction of reality has long since been widely taken up in the social sciences [35].In a context where data for connectionist AI systems are seldom based on a protocol specifically designed for the purpose, data sci-entists more readily take up a constructivist stance. For example,the developers of PredPol, a company that specializes in predicting crime in order to guide police patrols, recognize that the major ethical challenges of predictive policing revolve around locating the biases in the training data: these originate in the social interactions between the police, criminals and the public at large that produce reports of crimes. They admit that these statistical coding operations have feedback on reality, by way of the algorithmic systems that they are used to drive. The results of actions taken based on biased predictions themselves become training data, which can then reinforce and intensify existing inequality in the distribution of arrests or of protection across the population [36]. A good description of this algorithmic construction of reality through machine learning represents biases as features of a cyclical process consisting of three steps [37]: first, from data generation algorithm (for example, where historical or structural biases that reflect asymmetrical power relations in the social world are fed into an algorithm); next, from algorithm to user interaction(for example, the omission of variables, often related to developers prejudices and interests); and then from user interaction to data (for example, behavioral biases among users that are “undesired”by the designers).The many accusations leveled at algorithmic systems reveal a“sociology of biases” that shows that bias will never be eliminated by increasing methodological rigor. The substance of the critique of algorithmic decisions has to do not with the rigor of data col-lection, but with the system’s perspective on the world (in the sense of standpoint epistemology ) and its feedback effects. It's Simply impossible to produce a neutral decision system, and, for the users of these systems, bias is a problem in moral philosophy that arises in relationship to those who are “computed” (we will return to this point below). This why the main objective for the designer of algorithmic systems must be to avoid an unconsciously partial decision, while seeking to understand bias as an aspect of the machine’s interactions with the world, in the terms both of moral philosophy and of the sociology of inequalities. Hence the idea, shared by most FairML researchers, that algorithms may cre-ate the potential for new forms of transparency, and thus for novel ways of detecting discrimination [38]. To prevent discrimination,we must have the means to detect it, which can be very difficult with human decision-makers. While algorithms may increase the risk of discrimination, they also have the potential to facilitate its detection - and thus its prevention. With the field of fairness in machine learning, algorithms have become major political actors[39].",37-8,"have feedback on reality, by way of the algorithmic systems that they are used to drive. The results of actions taken based on biased predictions themselves become training data, which can then reinforce and intensify existing inequality in the distribution of arrests or of protection across the population ",
11,EthicalChallenges,Perceived_Problem,"One of the effects of this coexistence is to have made the question of “biases” more of a political than a methodological problem.Traditionally, from a realist perspective, bias is defined as a sys-tematic error (of measurement, reasoning, procedure, or judgment)that produces a deviation from the “truth.” In machine learning,researchers call the reality outside their models “ground truth,” and often define bias as a deviation from that truth [34]. Thus, while controlling biases is in general an essentially methodological chal-lenge, in machine learning it is a crucial scientific task, because it promises to offer a way of removing all traces of subjectivity,producing “mechanically” objective scientific statements [15].But when it is the ground truth itself that is biased because society is structurally unjust and unequal; when the technologies themselves are an integral part of unequal social structure (structur-ing the data); and when this ground truth can defined in multiple ways depending on the perceiver’s objectives and interests, then the question of bias can no longer be posed in realist terms. Biasesthen become social norms in themselves, which must be taken into account either to change society or, on the contrary, to choose todo nothing. Actors in FairML have given the concept of bias a new status and meaning – at least for engineers. This analysis of the close connection between quantification techniques and the social construction of reality has long since been widely taken up in the social sciences [35].In a context where data for connectionist AI systems are seldom based on a protocol specifically designed for the purpose, data sci-entists more readily take up a constructivist stance. For example,the developers of PredPol, a company that specializes in predicting crime in order to guide police patrols, recognize that the major ethical challenges of predictive policing revolve around locating the biases in the training data: these originate in the social interactions between the police, criminals and the public at large that produce reports of crimes. They admit that these statistical coding operations have feedback on reality, by way of the algorithmic systems that they are used to drive. The results of actions taken based on biased predictions themselves become training data, which can then reinforce and intensify existing inequality in the distribution of arrests or of protection across the population [36]. A good description of this algorithmic construction of reality through machine learning represents biases as features of a cyclical process consisting of three steps [37]: first, from data generation algorithm (for example, where historical or structural biases that reflect asymmetrical power relations in the social world are fed into an algorithm); next, from algorithm to user interaction(for example, the omission of variables, often related to developers prejudices and interests); and then from user interaction to data (for example, behavioral biases among users that are “undesired”by the designers).The many accusations leveled at algorithmic systems reveal a“sociology of biases” that shows that bias will never be eliminated by increasing methodological rigor. The substance of the critique of algorithmic decisions has to do not with the rigor of data col-lection, but with the system’s perspective on the world (in the sense of standpoint epistemology ) and its feedback effects. It's Simply impossible to produce a neutral decision system, and, for the users of these systems, bias is a problem in moral philosophy that arises in relationship to those who are “computed” (we will return to this point below). This why the main objective for the designer of algorithmic systems must be to avoid an unconsciously partial decision, while seeking to understand bias as an aspect of the machine’s interactions with the world, in the terms both of moral philosophy and of the sociology of inequalities. Hence the idea, shared by most FairML researchers, that algorithms may cre-ate the potential for new forms of transparency, and thus for novel ways of detecting discrimination [38]. To prevent discrimination,we must have the means to detect it, which can be very difficult with human decision-makers. While algorithms may increase the risk of discrimination, they also have the potential to facilitate its detection - and thus its prevention. With the field of fairness in machine learning, algorithms have become major political actors[39].",37-8,the major ethical challenges of predictive policing,
12,BiasesInTrainingData,Perceived_Problem,"One of the effects of this coexistence is to have made the question of “biases” more of a political than a methodological problem.Traditionally, from a realist perspective, bias is defined as a sys-tematic error (of measurement, reasoning, procedure, or judgment)that produces a deviation from the “truth.” In machine learning,researchers call the reality outside their models “ground truth,” and often define bias as a deviation from that truth [34]. Thus, while controlling biases is in general an essentially methodological chal-lenge, in machine learning it is a crucial scientific task, because it promises to offer a way of removing all traces of subjectivity,producing “mechanically” objective scientific statements [15].But when it is the ground truth itself that is biased because society is structurally unjust and unequal; when the technologies themselves are an integral part of unequal social structure (structur-ing the data); and when this ground truth can defined in multiple ways depending on the perceiver’s objectives and interests, then the question of bias can no longer be posed in realist terms. Biasesthen become social norms in themselves, which must be taken into account either to change society or, on the contrary, to choose todo nothing. Actors in FairML have given the concept of bias a new status and meaning – at least for engineers. This analysis of the close connection between quantification techniques and the social construction of reality has long since been widely taken up in the social sciences [35].In a context where data for connectionist AI systems are seldom based on a protocol specifically designed for the purpose, data sci-entists more readily take up a constructivist stance. For example,the developers of PredPol, a company that specializes in predicting crime in order to guide police patrols, recognize that the major ethical challenges of predictive policing revolve around locating the biases in the training data: these originate in the social interactions between the police, criminals and the public at large that produce reports of crimes. They admit that these statistical coding operations have feedback on reality, by way of the algorithmic systems that they are used to drive. The results of actions taken based on biased predictions themselves become training data, which can then reinforce and intensify existing inequality in the distribution of arrests or of protection across the population [36]. A good description of this algorithmic construction of reality through machine learning represents biases as features of a cyclical process consisting of three steps [37]: first, from data generation algorithm (for example, where historical or structural biases that reflect asymmetrical power relations in the social world are fed into an algorithm); next, from algorithm to user interaction(for example, the omission of variables, often related to developers prejudices and interests); and then from user interaction to data (for example, behavioral biases among users that are “undesired”by the designers).The many accusations leveled at algorithmic systems reveal a“sociology of biases” that shows that bias will never be eliminated by increasing methodological rigor. The substance of the critique of algorithmic decisions has to do not with the rigor of data col-lection, but with the system’s perspective on the world (in the sense of standpoint epistemology ) and its feedback effects. It's Simply impossible to produce a neutral decision system, and, for the users of these systems, bias is a problem in moral philosophy that arises in relationship to those who are “computed” (we will return to this point below). This why the main objective for the designer of algorithmic systems must be to avoid an unconsciously partial decision, while seeking to understand bias as an aspect of the machine’s interactions with the world, in the terms both of moral philosophy and of the sociology of inequalities. Hence the idea, shared by most FairML researchers, that algorithms may cre-ate the potential for new forms of transparency, and thus for novel ways of detecting discrimination [38]. To prevent discrimination,we must have the means to detect it, which can be very difficult with human decision-makers. While algorithms may increase the risk of discrimination, they also have the potential to facilitate its detection - and thus its prevention. With the field of fairness in machine learning, algorithms have become major political actors[39].",37-8,locating the biases in the training data,
13, , , , , ,
14, , , , , ,
15, , , , , ,
16, , , , , ,
17,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
18,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
19,EthicalChallenges,constrainsAgent,PredPol,"One of the effects of this coexistence is to have made the question of “biases” more of a political than a methodological problem.Traditionally, from a realist perspective, bias is defined as a sys-tematic error (of measurement, reasoning, procedure, or judgment)that produces a deviation from the “truth.” In machine learning,researchers call the reality outside their models “ground truth,” and often define bias as a deviation from that truth [34]. Thus, while controlling biases is in general an essentially methodological chal-lenge, in machine learning it is a crucial scientific task, because it promises to offer a way of removing all traces of subjectivity,producing “mechanically” objective scientific statements [15].But when it is the ground truth itself that is biased because society is structurally unjust and unequal; when the technologies themselves are an integral part of unequal social structure (structur-ing the data); and when this ground truth can defined in multiple ways depending on the perceiver’s objectives and interests, then the question of bias can no longer be posed in realist terms. Biasesthen become social norms in themselves, which must be taken into account either to change society or, on the contrary, to choose todo nothing. Actors in FairML have given the concept of bias a new status and meaning – at least for engineers. This analysis of the close connection between quantification techniques and the social construction of reality has long since been widely taken up in the social sciences [35].In a context where data for connectionist AI systems are seldom based on a protocol specifically designed for the purpose, data sci-entists more readily take up a constructivist stance. For example,the developers of PredPol, a company that specializes in predicting crime in order to guide police patrols, recognize that the major ethical challenges of predictive policing revolve around locating the biases in the training data: these originate in the social interactions between the police, criminals and the public at large that produce reports of crimes. They admit that these statistical coding operations have feedback on reality, by way of the algorithmic systems that they are used to drive. The results of actions taken based on biased predictions themselves become training data, which can then reinforce and intensify existing inequality in the distribution of arrests or of protection across the population [36]. A good description of this algorithmic construction of reality through machine learning represents biases as features of a cyclical process consisting of three steps [37]: first, from data generation algorithm (for example, where historical or structural biases that reflect asymmetrical power relations in the social world are fed into an algorithm); next, from algorithm to user interaction(for example, the omission of variables, often related to developers prejudices and interests); and then from user interaction to data (for example, behavioral biases among users that are “undesired”by the designers).The many accusations leveled at algorithmic systems reveal a“sociology of biases” that shows that bias will never be eliminated by increasing methodological rigor. The substance of the critique of algorithmic decisions has to do not with the rigor of data col-lection, but with the system’s perspective on the world (in the sense of standpoint epistemology ) and its feedback effects. It's Simply impossible to produce a neutral decision system, and, for the users of these systems, bias is a problem in moral philosophy that arises in relationship to those who are “computed” (we will return to this point below). This why the main objective for the designer of algorithmic systems must be to avoid an unconsciously partial decision, while seeking to understand bias as an aspect of the machine’s interactions with the world, in the terms both of moral philosophy and of the sociology of inequalities. Hence the idea, shared by most FairML researchers, that algorithms may cre-ate the potential for new forms of transparency, and thus for novel ways of detecting discrimination [38]. To prevent discrimination,we must have the means to detect it, which can be very difficult with human decision-makers. While algorithms may increase the risk of discrimination, they also have the potential to facilitate its detection - and thus its prevention. With the field of fairness in machine learning, algorithms have become major political actors[39].",37-8,"the developers of PredPol, a company that specializes in predicting crime in order to guide police patrols, recognize that the major ethical challenges of predictive policing revolve around locating the biases in the training data"
20,BiasesInTrainingData,constrainsAgent,PredPol,"One of the effects of this coexistence is to have made the question of “biases” more of a political than a methodological problem.Traditionally, from a realist perspective, bias is defined as a sys-tematic error (of measurement, reasoning, procedure, or judgment)that produces a deviation from the “truth.” In machine learning,researchers call the reality outside their models “ground truth,” and often define bias as a deviation from that truth [34]. Thus, while controlling biases is in general an essentially methodological chal-lenge, in machine learning it is a crucial scientific task, because it promises to offer a way of removing all traces of subjectivity,producing “mechanically” objective scientific statements [15].But when it is the ground truth itself that is biased because society is structurally unjust and unequal; when the technologies themselves are an integral part of unequal social structure (structur-ing the data); and when this ground truth can defined in multiple ways depending on the perceiver’s objectives and interests, then the question of bias can no longer be posed in realist terms. Biasesthen become social norms in themselves, which must be taken into account either to change society or, on the contrary, to choose todo nothing. Actors in FairML have given the concept of bias a new status and meaning – at least for engineers. This analysis of the close connection between quantification techniques and the social construction of reality has long since been widely taken up in the social sciences [35].In a context where data for connectionist AI systems are seldom based on a protocol specifically designed for the purpose, data sci-entists more readily take up a constructivist stance. For example,the developers of PredPol, a company that specializes in predicting crime in order to guide police patrols, recognize that the major ethical challenges of predictive policing revolve around locating the biases in the training data: these originate in the social interactions between the police, criminals and the public at large that produce reports of crimes. They admit that these statistical coding operations have feedback on reality, by way of the algorithmic systems that they are used to drive. The results of actions taken based on biased predictions themselves become training data, which can then reinforce and intensify existing inequality in the distribution of arrests or of protection across the population [36]. A good description of this algorithmic construction of reality through machine learning represents biases as features of a cyclical process consisting of three steps [37]: first, from data generation algorithm (for example, where historical or structural biases that reflect asymmetrical power relations in the social world are fed into an algorithm); next, from algorithm to user interaction(for example, the omission of variables, often related to developers prejudices and interests); and then from user interaction to data (for example, behavioral biases among users that are “undesired”by the designers).The many accusations leveled at algorithmic systems reveal a“sociology of biases” that shows that bias will never be eliminated by increasing methodological rigor. The substance of the critique of algorithmic decisions has to do not with the rigor of data col-lection, but with the system’s perspective on the world (in the sense of standpoint epistemology ) and its feedback effects. It's Simply impossible to produce a neutral decision system, and, for the users of these systems, bias is a problem in moral philosophy that arises in relationship to those who are “computed” (we will return to this point below). This why the main objective for the designer of algorithmic systems must be to avoid an unconsciously partial decision, while seeking to understand bias as an aspect of the machine’s interactions with the world, in the terms both of moral philosophy and of the sociology of inequalities. Hence the idea, shared by most FairML researchers, that algorithms may cre-ate the potential for new forms of transparency, and thus for novel ways of detecting discrimination [38]. To prevent discrimination,we must have the means to detect it, which can be very difficult with human decision-makers. While algorithms may increase the risk of discrimination, they also have the potential to facilitate its detection - and thus its prevention. With the field of fairness in machine learning, algorithms have become major political actors[39].",37-8,"the developers of PredPol, a company that specializes in predicting crime in order to guide police patrols, recognize that the major ethical challenges of predictive policing revolve around locating the biases in the training data"
21,PredPol,hasProducedArtifact,FeedbackOnReality,"One of the effects of this coexistence is to have made the question of “biases” more of a political than a methodological problem.Traditionally, from a realist perspective, bias is defined as a sys-tematic error (of measurement, reasoning, procedure, or judgment)that produces a deviation from the “truth.” In machine learning,researchers call the reality outside their models “ground truth,” and often define bias as a deviation from that truth [34]. Thus, while controlling biases is in general an essentially methodological chal-lenge, in machine learning it is a crucial scientific task, because it promises to offer a way of removing all traces of subjectivity,producing “mechanically” objective scientific statements [15].But when it is the ground truth itself that is biased because society is structurally unjust and unequal; when the technologies themselves are an integral part of unequal social structure (structur-ing the data); and when this ground truth can defined in multiple ways depending on the perceiver’s objectives and interests, then the question of bias can no longer be posed in realist terms. Biasesthen become social norms in themselves, which must be taken into account either to change society or, on the contrary, to choose todo nothing. Actors in FairML have given the concept of bias a new status and meaning – at least for engineers. This analysis of the close connection between quantification techniques and the social construction of reality has long since been widely taken up in the social sciences [35].In a context where data for connectionist AI systems are seldom based on a protocol specifically designed for the purpose, data sci-entists more readily take up a constructivist stance. For example,the developers of PredPol, a company that specializes in predicting crime in order to guide police patrols, recognize that the major ethical challenges of predictive policing revolve around locating the biases in the training data: these originate in the social interactions between the police, criminals and the public at large that produce reports of crimes. They admit that these statistical coding operations have feedback on reality, by way of the algorithmic systems that they are used to drive. The results of actions taken based on biased predictions themselves become training data, which can then reinforce and intensify existing inequality in the distribution of arrests or of protection across the population [36]. A good description of this algorithmic construction of reality through machine learning represents biases as features of a cyclical process consisting of three steps [37]: first, from data generation algorithm (for example, where historical or structural biases that reflect asymmetrical power relations in the social world are fed into an algorithm); next, from algorithm to user interaction(for example, the omission of variables, often related to developers prejudices and interests); and then from user interaction to data (for example, behavioral biases among users that are “undesired”by the designers).The many accusations leveled at algorithmic systems reveal a“sociology of biases” that shows that bias will never be eliminated by increasing methodological rigor. The substance of the critique of algorithmic decisions has to do not with the rigor of data col-lection, but with the system’s perspective on the world (in the sense of standpoint epistemology ) and its feedback effects. It's Simply impossible to produce a neutral decision system, and, for the users of these systems, bias is a problem in moral philosophy that arises in relationship to those who are “computed” (we will return to this point below). This why the main objective for the designer of algorithmic systems must be to avoid an unconsciously partial decision, while seeking to understand bias as an aspect of the machine’s interactions with the world, in the terms both of moral philosophy and of the sociology of inequalities. Hence the idea, shared by most FairML researchers, that algorithms may cre-ate the potential for new forms of transparency, and thus for novel ways of detecting discrimination [38]. To prevent discrimination,we must have the means to detect it, which can be very difficult with human decision-makers. While algorithms may increase the risk of discrimination, they also have the potential to facilitate its detection - and thus its prevention. With the field of fairness in machine learning, algorithms have become major political actors[39].",37-8,"They admit that these statistical coding operations have feedback on reality, by way of the algorithmic systems that they are used to drive. The results of actions taken based on biased predictions themselves become training data, which can then reinforce and intensify existing inequality in the distribution of arrests or of protection across the population "
