,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{grill2023, author = {Grill, Gabriel and Fischer, Fabian and Cech, Florian}, title = {Bias as Boundary Object: Unpacking The Politics Of An Austerity Algorithm Using Bias Frameworks}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,AustrianPublicEmployment,Agent,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,Public Employment Service Austria (AMS) ,
10,IncreaseObjectivityEfficiencyAndEffectiveness,Goal,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures)",
11,JobSeekerClassification,Causal_Theory,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,classifies job seekers into one of three categories based on their predicted “integration chance”,
12,UseOfPersonalAttributes,Strategy,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers.",
13,JobSeekers,Agent,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,job seekers,
14,AustrianDataProtectionAgency,Agent,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,the Austrian Data Protection Agency,
15,InsufficiencyOfHumanOversight,Perceived_Need,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,the proposed human oversight is insufficient,
16,LackOfTransparency,Perceived_Need,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,not transparent about the AMS algorithm,
17,PublishedDocuments,Artifact,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,initially published documents by the AMS ,
18,AutomatedDecisionMaker,Agent,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,the ADM,
19,AutomatedDecisionMaker,Artifact,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,the ADM,
20,RelianceOnFewVariables,Perceived_Problem,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"a few, often dichotomous, categories. ",
21,LackOfData,Perceived_Need,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations.",
22,LimitedPerspective,Perceived_Problem,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS.,
23,AppearanceBias,Causal_Theory,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,issues around appearance are sidelined [...] it has been shown that they can be important factors in a job search and furthermore may result in discrimination,
24,ChangingNorms,Causal_Theory,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"how norms, laws, and representations are not stable over time but constantly change",
25,LaborMarketChanges,Causal_Theory,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"Recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly.",
26,LackOfThirdGender,Perceived_Need,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"don’t account for the third gender option, which was recently legally recognized in Austria",
27,PersonalAttributes,Artifact,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,personal attributes,
28,ForbidingUse,Artifact,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,which forbade its use in 2020,
29,ConfusingFormulation,Perceived_Problem,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"confusingly formulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description",
30,Simplicity,Perceived_Need,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,is comparatively simple,
31,DifferentialAccuracy,Perceived_Problem,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations",
32,LackOfExpertEvaluation,Perceived_Need,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,no evaluation by external experts with access to the data,
33,QuestionableClassifications,Perceived_Problem,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,may center questionable classifications instead of job seekers’ articulated needs.,
34,HistoricalDiscrimination,Perceived_Problem,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,accounts for historical disadvantage and discrimination in the labor market and by the AMS,
35, , , , , ,
36, , , , , ,
37, , , , , ,
38,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
39,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
40,AustrianPublicEmployment,hasProducedArtifact,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM"
41,IncreaseObjectivityEfficiencyAndEffectiveness,constrainsAgent,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures)"
42,JobSeekerClassification,constrainsAgent,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,the system classifies job seekers into one of three categories based on their predicted “integration chance”
43,UseOfPersonalAttributes,constrainsAgent,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers."
44,JobSeekers,hasProducedArtifact,PersonalAttributes,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers"
45,AustrianDataProtectionAgency,hasProducedArtifact,ForbidingUse,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"the system gained public attention and was reviewed by the Austrian Data Protection Agency, which forbade its use in 2020"
46,InsufficiencyOfHumanOversight,constrainsAgent,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,The Data Protection Agency also argued that the proposed human oversight is insufficient
47,LackOfTransparency,constrainsAgent,AustrianPublicEmployment,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,The public employment service was not transparent about the AMS algorithm
48,PublishedDocuments,reflectsPrecept,ConfusingFormulation,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"initially published documents by the AMS turned out to be confusingly formulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description"
49,AutomatedDecisionMaker,reflectsPrecept,Simplicity,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,the algorithmic system is comparatively simple
50,AutomatedDecisionMaker,reflectsPrecept,DifferentialAccuracy,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algorithm"
51,AutomatedDecisionMaker,reflectsPrecept,LackOfExpertEvaluation,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,no evaluation by external experts with access to the data used for constructing and evaluating the models
52,RelianceOnFewVariables,constrainsAgent,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market."
53,LackOfData,constrainsAgent,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method."
54,LimitedPerspective,constrainsAgent,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,The data used to construct the model only represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS.
55,AppearanceBias,constrainsAgent,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination"
56,ChangingNorms,constrainsAgent,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time."
57,LaborMarketChanges,constrainsAgent,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly."
58,LackOfThirdGender,constrainsAgent,AutomatedDecisionMaker,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female"
59,AutomatedDecisionMaker,influencesPrecept,QuestionableClassifications,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs. The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances."
60,AutomatedDecisionMaker,influencesPrecept,HistoricalDiscrimination,"Introduced in October 2018 by the Public Employment Service Austria (AMS) under a right-wing government,2 the stated goal of the ADM were to decrease the influence of the subjectivity of caseworkers, increase the overall efficiency and speed of counseling,and improve the effectiveness of supportive measures (including,for instance, specialized job training, re-education, and other labor market reintegration measures). To achieve these goals, the system classifies job seekers into one of three categories based on their predicted “integration chance” (IC) into the job market:• Group A: High short-term prospects3• Group C: Low long-term prospects4• Group B: Medium prospects (neither part of groups A or C)This classification determines the levels of support available to the job seeker. Job seekers from group A are deemed most likely to find gainful employment soon without additional support mea-sures. Subsequently, the frequency of mandatory visits with theAMS would also be reduced for these job seekers and they wouldn't be offered certain support measures. Based on the assumption that expensive active labor market programs and other supportive measures would not substantially improve the (already presumably low) reintegration chances of group C, these job seekers would be referred to an external institution offering supervision and other stabilization' measures on a voluntary basis. Finally, group B, the residual category, would be offered all traditional supportive mea-sures.As such, the system was introduced as a semi-automated decision-support tool that promises to help distributing scarce resources efficiently. To make the necessary predictions in the form of the values, the system utilizes data reaching back four years in the form of both personal attributes of job seekers, as well as their history on the labor market and the performance of regional job centers. Nationwide implementation was planned for 2020. Due to a number of factors, including the heated public debate, the system gained public attention and was reviewed by the Austrian Data ProtectionAgency, which forbade its use in 2020. The agency argued that the processing of sensitive information requires a dedicated legal foun-dation according to the GDPR and the use for individual prediction is not covered by existing laws. A court case challenging this ruling is ongoing, and the system is currently not in use.The planned operationalization of the so-called ‘AMS algorithm"" presented a number of tensions calling into doubt its capability to reach the stated goals of efficiency, effectiveness and reduction of human subjectivity and prejudice; for instance, the stated goal to streamline the job seeker / caseworker interactions stands in stark contrast to the assumption that caseworkers would be capable of acting as a ‘human corrective’ for any erroneous classifications,which puts additional strain on the already limited time with jobseekers. The Data Protection Agency also argued that the proposed human oversight is insufficient and that the system should be classified as an automated decision making system according to GDPR Art. 22. This would results in stricter accountability regulations being applied that would have to be legally binding (as opposed to merely' internal guidelines) Our study of the AMS algorithm is based mostly on internal documents provided by the public employment agency and the con-tracted company as well as documents in direct response to a catalogue of questions we posed to these parties. We also engaged with civil society groups while conducting this work, such as jobseeker interest groups. Many of these documents are not public, but provided to the authors for a commissioned case study [3] with the permission to use them for further scientific research. The section 3.4 presents an in-depth classification of different of problematic biases of the AMS algorithm by reworking and extending results of the commissioned study.To analyze the documents, we used qualitative document analy-sis as a method [9]. This involved close reading of the documents and annotating relevant sections. Our analysis draws on sensibili-ties from a feminist and constructivist tradition [14, 66] and seeks to follow a line of research concerned with the social study of al-gorithms [59, 61]. This means we understand these documents not as purely descriptive or objective, but instead as constructed texts emerging out of practices, situated in organizational cultures and responding to specific contextual demands.Three kinds of information were annotated: First, information about the broader context out of which the AMS algorithm was created, such as what requirements guided development. Second,information that allowed us to understand the precise technical workings of the algorithm, which includes descriptions of variables and databases, and internal evaluations of the system. Third, how the AMS algorithm was supposed to be embedded into the actual processes of the public employment agency’s counseling processes.Based on identified technical details, we tried to reconstruct the workings of the AMS algorithm to assess its potential impacts on jobseekers. We use the term reconstructing over reverse-engineering[39] because we were not able to recreate an implementation of system because we didn’t have access to the data used for constructing and evaluating the models. We then identified possible biases of the ADM, and categorized and interpreted them using a chosen framework [26].The public employment service was not transparent about the AMS algorithm, and this proved to be controversial. The initially published documents by the AMS turned out to be confusingly for-mulated, contradictory and selectively disclosed flattering statistics of error rates and precision in lieu of a detailed and balanced description [31]. One document described a logistic regression model that was not used for the classification. Against this backdrop, our analysis contributes to a better public and academic understanding because it can build on internal documents detailing the AMS algo-rithm’s development and evaluation, providing crucial information that was previously unknown to the public. We found that the algorithmic system is comparatively simple and not a complex, modern statistical system, despite initially being portrayed as such. Before the categorization of job seekers into one of the three categories as outlined above, each individual’s IC value is calculated based on a simple ratio between prior observations of job-seekers with the same or similar attributes: those that fulfilled either the long or short integration criterion versus those that didn't.A matrix cross-relating a total of 13 variables models job seekers,including gender, age, citizenship, education, health impairments,duties of care for others, job sector, assignment to a specific job center and prior employment history. An individual is then compared to a historical group of job seekers with the same variable values.To explicate this procedure with a simplified example, a 35-year old woman with non-EU citizenship and no health impairments would be compared to all other 30 to 49 year-old women with non-EU citizenship and no health impairments within the prior four years. If, for instance, 83 out of a total of 100 persons with these same attributes did manage to find gainful employment for at least 3 months within the first 7 months of unemployment, the individual's short-term IC value would be given at 83%.Given the many possible combinations of personal attributes and variable values, it is not surprising that–for a significant subset of job seekers–the number of comparative observations could be quite low, in some cases even less than 10. The system would then merge adjacent groups by joining certain variable values. The specific process of how these merges would occur was not disclosed. The variables were not only chosen due to the advertised goals of increasing accuracy, efficiency, and effectiveness. For example, they were meant to be convincing and explainable to different stake-holders and certain variables were omitted due to ethical concerns while others remained. Similarly, the thresholds applied to IC scores to produce the three risk categories were calculated using other constraints beyond the stated goals. Further complicating the process is the fact that the total population of available observations was split further into those with complete employment history within four years prior and those where this data was incomplete. The latter were again separated into individuals with a migration background, individuals under25 years of age and the remaining individuals with incomplete em-ployment history. This step was likely a measure to improve the accuracy / error rates of the predictions, and to make up for a lack of observations for certain subpopulations. Overall, the system was advertised as 80% accurate; however, this purported accuracy varies greatly depending on which subset of the populations individuals are assigned to, sinking as low as 69% for some combinations as indicated by the internal evaluations by creators of the AMS algo-rithm. There was no evaluation by external experts with access to the data used for constructing and evaluating the models, which calls the reported numbers into question. Prior work has also high-lighted, how reductive performance indicators can hide problematic disparities disadvantaging marginalized groups and arbitrariness in classification [2, 29]. We employ a long-established bias framework for computer systems by Friedman and Nissenbaum [26] to analyze the AMS algorithm and illustrate how its three basic categories for bias (technical, pre-existing, and emergent) can be traced. We adapted the categories slightly to make them fit the context and added subcategories that we deemed useful for illustrating specific structural issues of the al-gorithm that could lead to harms, such as not receiving appropriate support measures when needed.3.4.1 Technical Bias. This bias category is concerned with repre-sentational accuracy in light of ""technical constraints or technical considerations"" [26, p. 334] such as the abstraction, reduction, and decontextualization necessary in statistical modeling at scale [54].This category provides a lens on flexibilities and tradeoffs decisionmakers have to consider, which highlights the non-innocence and political character of their work [10]. It attributes accountability to human actors as design decisions are problematized. This can point to ways the system could be potentially improved [60] or how it has inherent fundamental issues that no design can remedy. In the context of the AMS algorithm, this flexibility refers to the decisions involved actors such as on the data used and how categories are constructed, which influence which job seekers receive social sup-port measures from the state. The following paragraphs categorize various biases and illustrate how long-standing problematic soci-etal assumptions and established practices are reproduced through design decisions and thereby reify marginalization and enacting representational harms [8].Rigidity and Coarseness of Variables: The variables assumed to influence job seeker chances are modeled based on a few, often dichotomous, categories. Consequently, these categories have to be coarse and encompassing to be convincing and allow for a classification scheme that can maintain the promise of capturing the complexity and variety of the Austrian labor market. For example, three age groups were used to categorize job seekers, which poses that people part of these groups have sufficiently similar integration chances to allow prediction. The variables fix fluid, continuous reali-ties into supposedly stable and discrete categories. For example, the dichotomous health impairment variable absurdly oversimplifies disability and how it impacts possibilities for work in the labor mar-ket, disregarding difference, and representational justice in favor of supposed bureaucratic efficiency. Rigid Categories: The categorization into risk groups is based on strict thresholds applied to calculated integration chance (IC) values. Thus, job seekers with IC values that only differ minimally may be assigned to completely different groups, which entail completely different treatment and support measures. Uncertainty of Job Seeker Groups: As noted above, the AMS algorithm supposes that job seekers that share variable values have the same integration chance. Increasing the number of potential values also increases the number of groups, which is assumed to increase homogeneity within groups and differentiation between them. Yet Since the number of job seekers is constant, when the number of groups increases so does the number of groups based on very few observations. According to the documentation, about 1900 groups are based on 50 or more observations and are thereby considered“statistically satisfactory,"" a term made up by the creators of theAMS algorithm. About 39% of job seekers are classified based on aggregated integration chances of groups that are considered “sta-tistically not satisfactory."" About 12% of job seekers are classified based on groups with less than 10 observations. Making predictions based on so few observations is concerning and a consequence of the chosen method. As a remedy, smaller ones are aggregated into bigger ones, but it was not made clear what criteria were used for merging. The system requires stable, historical data about jobseeker groups in order to calculate integration chances. Since it only considers data collected over a period of 4 years, it cannot assess new job seekers that are part of groups that represent sets of values not encountered in that timeframe. For these groups no data would be available at all and, in turn, the integration chances for such jobseekers would either have to be guessed or would need to be based on another group with available observations. Counselors don't know on how many observations a classification is based upon and thereby cannot consider this uncertainty in their assessment.Privileged Perspectives: The data used to construct the modelonly represents a limited perspective on the labor market. It is only able to uncover correlations between essentialized characteristics of individual job seekers as recorded in governmental databases and periods for which job seekers use the services of the AMS. This Partial perspective ignores, e.g., people that look for a job with-out being registered with the AMS. Furthermore, the correlations that can be uncovered in this setup only attribute unemployment characteristics of job seekers. The data does not reveal, for in-stance, if certain industries or companies have problematic hiring practices, e.g., excluding women or other minoritized communities from open positions. The data does not contain any information on whether job seekers found a position they are happy with, either. Similarly, it is not able to uncover correlations that may point to problems with caseworkers at the AMS and the AMS management.The tool ultimately provides a perspective on the labor market that,based on calculated reasons for unemployment building on the available correlations, always insinuates that job seekers are the ones responsible for being unemployed.Qualitative Factors: The AMS algorithm does not consider quali-tative factors which potentially impact job seeker integration. For Example, the intrinsic motivation of job seekers is ignored although it may impact reintegration chances significantly. Similarly, issues around appearance are sidelined, although it has been shown that they can be important factors in a job search and furthermore may result in discrimination [64].3.4.2 Emergent Bias. This category seeks to capture bias that ""arises in context of use [. . . ] some time after a design is completed"" [26,p. 335]. Put differently, this bias category foregrounds changing contexts, for example, due to the passage of time or use in space it was not intended for. This section identifies several bias cate-gories we deemed relevant to the AMS algorithm. For example, how norms, laws, and representations are not stable over time but constantly change, which affects the reliability of the system and cannot be easily solved through updates as it may take time until these changes result in stable, measurable trends (if they ever do so). Consequently, job seekers are increasingly misrepresented over time. A desired ""unbiased"" state can never be reached, and the envisioned yearly updates cannot be considered a sufficient fix for these problems. Disruptive events: The algorithm is updated yearly, and thereby,it assumes that the labor market does not change much in that year. Yet, recent examples, like COVID-19 or 2008 financial crash,highlight how disruptions happen regularly and often invalidate recent empirical labor market data as structures change quickly. Changing Laws and Norms: Both laws and norms change, and can invalidate aspects of the system. For example, the historical databases don’t account for the third gender option, which was recently legally recognized in Austria. This is also affects for the AMS Algorithm, as it only recognises a gender binary of male and female. With that, it either forces people not conforming to this binary into one of these categories or they simply cannot be processed. It is unclear how new versions of the system will handle this change because there are no historical records for people identifying as neither male nor female to calculate their integration chance.Interpretation and Interface: : The AMS algorithm’s group determination likely deteriorates the quality of counseling, as case-workers have very little time (sometimes only 10 minutes) [2] for engaging with job seekers and resolving issues with the calculated scores. The envisioned interface presents counselors a risk category and for job seekers with fully documented histories (about 70%)also the calculated integration chance can be accessed. These bits of information do not provide much room for interpretation and contestation for job seekers. In fact, internal instructional materials reveal that counselors are trained to convince job seekers of the objectivity of the algorithm when they question their own classifica-tion. After the introduction of the algorithm, counseling may center questionable classifications instead of job seekers’ articulated needs.The interface is also meant to aid counselors in this convincing by providing a set of a few rudimentary explanations for certain scores that can be presented to worried job seekers. Most of these are not very descriptive and some of them even reproduce racist and sexist stereotypes, e.g., only women (but not men) with chil-dren are warned about their childcare responsibilities negatively impacting their integration chances. Previous work highlights how the discretion caseworkers have for interpreting the interface and possibly changing group assignments could also further reinforced disparities if caseworkers hold discriminatory beliefs [28].3.4.3 Preexisting Bias. This form of bias has its ""roots in social in-stitutions, practices, and attitudes"" and ""exist[s] independently, andusually prior to the creation of the system"" [26, p. 334]. In practice, item bodies prejudices, held values, established practices and current social orders that seep into algorithmic systems through actions of institutions or individuals. In our case, this analytical bias category desensitizes us to the ways that contemporary injustices and historical inequalities on the labor market and the AMS as a public institution are inscribed in the algorithm and reinforced when it is used in decision making. Thus, this analysis problematizes these inequalities,as the desired ""unbiased"" norm or center in this category would bean algorithm that does not allocate fewer resources to people based on historical injustices. Instead, a ""less"" biased algorithm according to this category would seek to remedy and repair these injustices,for example, by allocating more resources to marginalized people,so they don’t negatively affect people’s opportunities on the labor market.Variables as direct/indirect Proxy for Inequality: The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality. The effects of these injustices are accounted for in the model through vari-ables that directly model marginalized groups, e.g., women are a category of the gender variable, and also indirectly, e.gThe area an unemployed person lives is also a variable and correlates with socio-economic opportunity. Multiple Models as Proxy for Inequality: For some job seekers,the data for four prior years is not complete. For these individuals,the designers of the system created three separate models based on particular variable values. The category for ""people with migration background,"" for example, contains people of foreign nationality,naturalized citizens, and people who have at least one parent of foreign nationality, three very different circumstances. It is not completely clear why these lines of separation were chosen. The Separation concentrates many job seekers with calculated high risk of long-term unemployment in one model, which means that new job seekers classified as ""people with migration background"" are likely to be also classified as high risk. The effects of the segmenta-tion also partially align with stated policy goals of the government under which the AMS algorithm was introduced, to focus less on the support and integration of refugees and more on other groups[3]. ",1841-4,"The data used to construct risk scores also accounts for historical disadvantage and discrimination in the labor market and by the AMS. Thus,the calculated risk of long-term unemployment of marginalized groups affected by these mechanisms is higher, which means fewer resources for these groups with the danger of a self-reinforcing loop algorithmically optimized to exacerbate inequality."
