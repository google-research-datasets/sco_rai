,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{field2023examiningrisksof,
    author = {Anjalie Field and Amanda Coston and Nupoor Gandhi and Alexandra Chouldechova and David Steier and Emily Putnam-Hornstein and Yulia Tsvetkov},
    title = {Examining risks of racial biases in NLP tools for child protective services},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-08, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,DHS,Agent,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",Public agencies [...] DHS,
10,TurningToAlgorithmic,Strategy,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",turning to algorithmic models,
11,ImprovingAccuracy,Perceived_Problem,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",goal of improving the consistency and accuracy of time-sensitive high-stakes decisions,
12,RiskTools,Artifact,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",Risk assessments tools,
13,NoContext,Perceived_Need,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",failing to account for relevant individual context,
14,AlgorithmSurvey,Artifact,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",survey of CPS algorithms,
15,Saxena,Agent,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",Saxena et al.,
16,AugmentingDataWithNL,Strategy,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",augmenting the data features with natural language,
17,NoteUtility,Causal_Theory,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",contact notes may appear useful data for risk prediction,
18,Subjectivity,Causal_Theory,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","reflects their perceptions of events, which may or may not accurately reflect reality",
19,SensedRisk,Causal_Theory,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations",
20,AlgorithmicBias,Causal_Theory,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",algorithmic bias,
21,Researchers,Agent,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",us,
22,People,Agent,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",persons,
23,Data,Artifact,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion",
24,DiscoveredBias,Artifact,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparit",
25, , , , , ,
26,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
27,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
28,DHS,hasProducedArtifact,RiskTools,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",Public agencies are increasingly turning to algorithmic models [...] Risk assessments tools
29,Saxena,hasProducedArtifact,AlgorithmSurvey,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","In a survey of CPSalgorithms, Saxena et al. [52] find"
30,Researchers,hasProducedArtifact,DiscoveredBias,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity"
31,People,hasProducedArtifact,Data,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","text is written by people [...] 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral"
32,TurningToAlgorithmic,constrainsAgent,DHS,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",Public agencies are increasingly turning to algorithmic models [...] Risk assessments tools
33,ImprovingAccuracy,constrainsAgent,DHS,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions
34,NoContext,constrainsAgent,DHS,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",Public agencies are increasingly turning to algorithmic models  [...] Risk assessments tools in general have been criticized for failing to account for relevant individual context
35,AlgorithmicBias,constrainsAgent,Researchers,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485",we focus on algorithmic bias
36,RiskTools,reflectsPrecept,TurningToAlgorithmic,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","By turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools"
37,RiskTools,reflectsPrecept,ImprovingAccuracy,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools "
38,AlgorithmSurvey,influencesPrecept,AugmentingDataWithNL,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias."
39,AlgorithmSurvey,influencesPrecept,Subjectivity,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","survey of CPSalgorithms, Saxena et al. [...] text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality "
40,AlgorithmSurvey,influencesPrecept,SensedRisk,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","In a survey of CPSalgorithms, Saxena et al. [...] here are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations."
41,DiscoveredBias,reflectsPrecept,AlgorithmicBias,"Public agencies are increasingly turning to algorithmic models with the goal of improving the consistency and accuracy of time-sensitive high-stakes decisions [7, 9, 16, 46, 52, 60]. These models reflect evolution from operator-driven checklists derived from regressions to machine-learning methods that draw on hundreds of pieces of information [35]. Risk assessments tools in general have been criticized for failing to account for relevant individual context and for automating biases in the data [18, 48, 63]. In a survey of CPSalgorithms, Saxena et al. [52] find that current tools rely on structured tabular data and suggest that augmenting the data features with natural language could be one way to incorporate context,improve model performance, and reduce bias. Though they dispute this suggestion in follow up work [53], the initial suggestion evidences how contact notes may appear useful data for risk prediction: they generally contain numerous details not captured in structured data. However, their incorporation could exacerbate many of the concerns around risk assessment: text is written by people and reflects their perceptions of events, which may or may not accurately reflect reality [17, 53]. There are numerous risks associated with incorporating text notes into risk predictive models, including reducing transparency, overfitting, increasing surveillance and privacy violations. In this section, we focus on algorithmic bias as one specific risk, and we provide additional discussion in Section 5.In the anonymous county, DHS uses a predictive risk assessment tool to aid in call-screening. For an incoming referral, the tool presents call-screening staff with a score from 1 to 20 that aims to reflect the likelihood that the child will be placed (removed from home) within 2 years conditional on the referral being screened in. While the model has undergone changes, the original version was a logistic LASSO that selected 71 features from > 800 variables providing demographics, past welfare interaction, public welfare,county prison, juvenile probation, and behavioral health informa-tion on all persons associated with each referral. Some features are derived from previous interactions with the child welfare system,(e.g. the number of previous referrals associated with people on the new referral). [...] Figure 4 displays the percent of children of each race that are flagged as high-risk as well as the racial composition of those that are flagged for both models,allowing us to examine if the hybrid model is liable to increase the number of black families involved in CPS. Under both models, a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity. This reduction is also visible in the histograms of raw model scores:under the structured model, scores for black children are right-shifted compared to white children, while under the hybrid model, the score distributions are nearly identical.","1483, 1485","we focus on algorithmic bias [...] a higher percentage of black children are flagged as high-risk relative to white children, but the hybrid model reduces this disparity"
