,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{ali2023, author = {Ali, Sanna J. and Christin, Ang\`{e}le and Smart, Andrew and Katila, Riitta},
title = {Walking the Walk of AI Ethics: Organizational Challenges and the Individualization of Risk among Ethics Entrepreneurs},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,NeedForBuyIn,Perceived_Problem,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"a challenge at big organizations was getting “buy in,” ",
10,LackOfClarityInLaw,Perceived_Need,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,There’s not clarity in the law,
11,ResistenceToFairnessWork,Perceived_Problem,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.”",
12,EthicsEntrepreneurs,Agent,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,ethics entrepreneurs.,
13,Defensiveness,Perceived_Problem,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,reacting defensively when concerns were raised,
14,NeedForGoodRelationships,Perceived_Need,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership.",
15,ProductInnovationAsCoreTask,Strategy,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,assumes product innovation as the core task of the organization,
16,EthicsInterventions,Artifact,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,ethics interventions,
17,ResistanceDueToDelays,Perceived_Problem,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,encounter resistance from them if and when their recommendations were likely to delay timelines for product launches.,
18,BiasForAction,Strategy,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,the impetus to “move fast” and have a “bias for action” ,
19,NorthStarMetrics,Strategy,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"“North Star” metrics and team sub-metrics,",
20,FrequentReorganization,Perceived_Problem,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with.",
21,PersonalRisk,Perceived_Problem,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,acute personal risks and pressures they felt as they went about their day-to-day jobs. ,
22,MaleDominatedEnvironment,Perceived_Problem,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"The male-dominated, engineering-centric world of Silicon Valley ",
23,TechnologyIndustry,Agent,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,large technology companies,
24,OtherTechWorkers,Agent,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,a Director-level employee at a Big Tech company [...] not working in a role directly related to AI ethics or responsibleAI;,
25,PragmaticLegitimacy,Perceived_Need,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,functionally superior or “pragmatically legitimate”,
26,EvidenceOfConnections,Artifact,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,concrete evidence of connections between the platform and bad things happening in real life.,
27,MarginalizedEthicsEntrepreneurs,Agent,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,fairness and ethics related issues as someone from a marginalized background particularly difficult.,
28, , , , , ,
29, , , , , ,
30, , , , , ,
31, , , , , ,
32,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
33,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
34,NeedForBuyIn,constrainsAgent,EthicsEntrepreneurs,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” "
35,LackOfClarityInLaw,constrainsAgent,EthicsEntrepreneurs,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"There’s not clarity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ"
36,ResistenceToFairnessWork,constrainsAgent,OtherTechWorkers,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.”"
37,EthicsEntrepreneurs,hasProducedArtifact,EthicsInterventions,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs."
38,Defensiveness,constrainsAgent,EthicsEntrepreneurs,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,ethics entrepreneurs often described product team members reacting defensively when concerns were raised
39,NeedForGoodRelationships,constrainsAgent,EthicsEntrepreneurs,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership."
40,ProductInnovationAsCoreTask,constrainsAgent,TechnologyIndustry,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,the technology industry assumes product innovation as the core task of the organization
41,EthicsInterventions,reflectsPrecept,PragmaticLegitimacy,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality"
42,ResistanceDueToDelays,constrainsAgent,EthicsEntrepreneurs,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches.
43,BiasForAction,constrainsAgent,TechnologyIndustry,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else.
44,NorthStarMetrics,constrainsAgent,EthicsEntrepreneurs,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics,"
45,EthicsEntrepreneurs,hasProducedArtifact,EvidenceOfConnections,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"Our job was to find such concrete evidence of connections between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ "
46,FrequentReorganization,constrainsAgent,EthicsEntrepreneurs,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with."
47,PersonalRisk,constrainsAgent,EthicsEntrepreneurs,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” "
48,MaleDominatedEnvironment,constrainsAgent,MarginalizedEthicsEntrepreneurs,"Overall, ethics workers described the product teams that they worked with, including engineers and product managers as, by and large “agreeable.” Kate, a PhD student who had worked at multiple companies in AI ethics-related roles, described people's reactions: “Everyone’s like ‘Oh, I want to do this. . . Like, I’m happy to help you. I’m happy to participate.’” Others were pleasantly surprised by the positive reception to their efforts, with one partici-pant highlighting the increased public awareness of the negative impacts of AI as compared to five years ago. Despite these positive reactions, ethics workers still faced challenges when it came to implementation of their recommendations. A lack of buy-in from leadership meant that ethics workers lacked organizational support,such as authority and resources, to fulfill their roles, perhaps a reflection of the intended symbolic nature of the role. To accomplish their goals, ethics workers became ethics entrepreneurs, relying on interpersonal skills to build active support.Many participants mentioned the main problem they faced,which came with several downstream effects: a lack of support from leadership. Sienna, a social scientist who had worked at two large technology companies, pointed out that a challenge at big organizations was getting “buy in,” which was especially difficult given the uncertainty surrounding legal frameworks and the internal specifications of what “fairness” might even mean in machine learning. As she explained: The last challenge at big orgs is getting company-wide buy-in—that means leadership support and bottoms up engineering support for this work. Sometimes it's hard to do that because of company priorities, and there's always resource constraints. There’s not clar-ity in the law, and there’s not clarity in research yet,so it’s difficult to make the case that we need to do XYZ. . . It’s hard to get buy-in when you don’t have that clarity.Morgan, a former engineer and ethics worker at a Big Technology Company, echoed these feelings: “Sometimes, no VP is willing to advocate for our team getting in the way of somebody’s product launch because there’s some ethical issue. . . There’s no incentive for people at the top to like help the people who do have ethics experience and background make waves in the company and change things.”We observed this lack of buy-in firsthand through an interview with Siva, a Director-level employee at a Big Tech company. Sivawas not working in a role directly related to AI ethics or responsibleAI; we spoke to him to gain a better contextual understanding of how ethics entrepreneurs might be perceived. He emphasized that almost everything he oversees requires automation. When asked if he had ever consulted the fairness team at his company, he bristled at the suggestion: “I haven’t had the need to consult the fairness team. No, it’s just not how [the company] works. It’s not possible for small teams to keep on auditing the entirety of products. Each Team has its own standards and procedures, launches are reviewed,there are launch metrics.” When asked if he had ever noticed a potential issue related to ethics or fairness close to a launch or after an AI product had already launched, he flatly denied any such occurrence. In response, he said the company was “well-managed”and so “issues like that rarely surface.” Notably, the company and vertical Siva was employed in, which we do not disclose to preserve anonymity, has been publicly critiqued multiple times by academics for racial bias in its algorithms.Given the difficulties around securing buy-in from leadership, it followed that there was often insufficient organizational support to ensure that AI products were developed responsibly. Ayesha, aBlack woman PhD student who had worked at multiple technology companies, gave the caveat, “To a certain extent, it’s [leadership buy-in] there; otherwise, the team wouldn’t have formed.” However,for the team to be truly effective, they needed more leadership buy-in in order to ensure that resources (such as time and personnel for data labeling) were dedicated to these efforts. They also needed leadership to formalize practices as requirements (such as reviews or audits early in the development cycle) and to grant ethics workers authority to mandate changes to products before launches. Without authority granted to ethics employees, formalization of an ethics review process at an early stage in the development cycle, or resources specifically dedicated to responsible AI, theonus for change falls onto individuals who must use various strate-gies to influence others. Following the framework of institutional entrepreneurship, we thus analyze AI ethics workers as ethics en-trepreneurs. This framework helps us reconcile the role that ethics workers play in effecting change within the organization, the dom-inant institutional logics that they must fight against, and the in-consistencies in whether ethics interventions are actually applied. While critics argue that ethics is subordinated to capitalist interests[25, 32], we examine how ethics interventions are nevertheless implemented, albeit inconsistently, as a result of the efforts of ethics entrepreneurs. The ethics entrepreneurs we spoke to relied on their individual relationships across the company, advocates within middle management, and skills involving persuasion and diplomacy to be effective at their jobs. The institutional entrepreneurship frame-work demonstrates the dependence of ethics work on individuals,in contrast to the corporate façade of structural reform but also in opposition to the overly simplistic interpretation of ethics washing.Within this institutional environment, ethics entrepreneurs could intervene successfully using interpersonal strategies such as iden-tifying and motivating allies or reframing problems in terms of mutual interest. Yet we find that issues could just as easily go unad-dressed depending on personal interests, interpersonal dynamics,internal politics, and more. For instance, ethics entrepreneurs often described product team members reacting defensively when concerns were raised, others passive aggressively acknowledging critiques but stating that they would not be acting on them, and some perceiving the work as “somebody else’s problem” and trying to offload all work to ethics entrepreneurs (which was problematic when the product team held the expertise on the machine learning model). One ethics entrepreneur working in Trust & Safety, Dave,explained how he explicitly networked within teams to find out, “Is There a certain personality on the team that just tends to have a lot of influence?” Dave would work with the manager of the team and an ML engineer who knew the team to figure out who needed to bein the room when he presented results. Ayesha similarly described the delicacy with which she had to approach others in order to do this work: It’s hard to ask them [to do things] because of politics. You want to have good relationships with engineers. You’re supposed to be in a role where you’re not taking too much of their time, and it’s supposed to be in partnership. You’re supposed to ping them, but you can't ping them too much. There’s a lot of diplomacy that has to happen. Overall, with the responsibility to make change dependent on individuals and small-scale interactions, participants described a wide variation in their experiences. Some described being met with skepticism on the one hand but also “deeply curious and empathic people” on the other. Given these differences between interlocu-tors, ethics entrepreneurs were more likely to engage deeply with people on product teams who cared about the work and willingly participated while sometimes disengaging entirely with teams who are indifferent to or frustrated by these efforts. Overall, the decoupling of policy, practice, and outcomes as well as the lack of organizational infrastructure meant that interpersonal dynamics could take precedent in how a matter unfolded. This resulted in inconsistency in how and whether specific issues were addressed. 4.2 Power Lies with ProductAs we saw, means-ends decoupling can emerge when there is a fragmented rationalized environment: the firm operates according to one rationalized logic but is also responding to environmental pressures that operate according to a different rationalized logic.As such, any reforms responding to environmental pressures are unrelated to the core tasks of the organization and thus siloed [3]. In the case of AI ethics, the technology industry assumes product innovation as the core task of the organization, while external pres-sures emphasize values like fairness and justice. In this section, we present the ways in which power lies with product and, regardless of whether or not ethics is siloed per se, the goal of product inno-vation supersedes goals related to responsible AI. The authority to make product decisions thus comes from product managers. In Response to the supremacy of “product” within the organization, ethics entrepreneurs attempted to negotiate with product managers for resources, theorize ethics-oriented interventions as functionally superior or “pragmatically legitimate” [18] by linking them to product quality, and advocate for early integration into the product development cycle.Participants noted that power was often located with product managers because, as one participant said, “they often bring all of the right resources together to drive product development.” John, who interacted with compliance-related employees, described the kinds of resource constraints ethics entrepreneurs would face: “Are There funds available for teams to collect a new dataset if the one that they used was problematic in some ways? Or like fundingfor like re-labeling, or re-annotating an existing dataset? Or even just like having the time and personnel to conduct a fairness eval-uation?” If resources were not allocated for ethics work, ethics entrepreneurs would sometimes attempt to convince the product team to allocate funds out of their project budgets for these kinds of activities.As another participant noted, product managers “are ultimately responsible for making sure there’s time available.” Accordingly, many ethics entrepreneurs highlighted that they would encounter resistance from them if and when their recommendations were likely to delay timelines for product launches. Sam, who previously worked at a Big Tech company but transitioned to a non-profit,said while she perceived teams as amenable to her suggestions,“They might be like, ‘We don’t have time for this, we have to get to launch.’. . . No one was antagonistic to it, there was more just this pressure to get this thing out.” Specifically, leadership would set timelines for product launches in order to beat “investor expecta-tions” through frequent product innovation. The focus on launch deadlines as well as the impetus to “move fast” and have a “bias for action” (corporate values at Meta and Amazon respectively) set expectations within the company that rapid product innovation is the priority above all else. Multiple participants posited that efforts would be more success-ful if leadership were to mandate integration of ethics initiatives in early stages of the product development cycle. Respondentsnoted that they had positive experiences when engaging in a consul-tation or review of a product early in its design, such as during the ideation or conception phase. In those cases, suggestions could be incorporated early on and could shape the direction of the product.However, teams often consulted responsible AI team members at the end of a product development cycle, for example after a model was already trained but before it was deployed. This would result in scrambles to course correct just before a launch deadline if it was even feasible to do so. Gabriel, an ethics entrepreneur at a large technology company with a background in philosophy, noted, The thing that a team might be annoyed about is the fact that the work is delayed, or like major suggestions are being made very late on in the process. But I think like most of that anger could be taken away through a very early engagement. So if. . . these points has come up, and the direction of the project had changed,there probably wouldn’t be such a tension. . . . But the question to ask there is like, why is it the case that the team ended up going to ethics review so late in the game?Perhaps Gabriel’s question could be answered by our application of neo-institutional theory: that ethics and product follow different and separate rationalized logics that silo and buffer activities for each other.How then do ethics entrepreneurs overcome challenges associ-ated with the supremacy of product? Some ethics entrepreneurs take a pragmatic route, focusing on making automated ethics tools as easy as possible to use in order to reduce “friction” and achieve higher adoption rates amongst product teams. Others were able to influence others by reframing any problem as an issue of product quality. For example, Kate said,I think the vernacular we use around these topics is really important, especially when working with machine learning practitioners. . . I’ll try to pick up as much of the internal culture and internal jargon as Ican. I very rarely say ’fairness’ or ’ethics’ or anything like that because like, that’s a loaded word. . . I really try to go to product teams with the idea that, ‘I'm Here to help you make a better product.’ And in my experience, the way organizations work, everyone can get behind making better products.Using product quality as a justification demonstrates one way forethics entrepreneurs to integrate ethics activities more seamlessly into the core activities of the organization. It also confers legitimacy to a new practice that developers may not see the value of. Finally, while ethics entrepreneurs attempt to integrate ethics into the organization’s core activities, others expressed a more critical view of the structural conditions surrounding frequent product innovation. One informant who had done AI ethics-related consult-ing for different technology corporations critiqued this focus on constant innovation as part of a systemic issue with technology and venture capital:We see all of these products that are based on some sort of speculative reward. And these products have all of this runway based on the amount of investment in them. And so, they’re able to go out there into the real world and produce harm without necessarily having a real market need. Like nobody needs better ads. . . . But if they’re gonna get $500 million ofVC funding to give better ads, they’re going to build something that promises to give better ads, whether it does or not.Consistent with this critique of needless and constant product innovation, multiple participants raised an example of an ethical issue where the solution was not to correct the model, but rather simply not to use AI for the feature at all.4.3 The Problem with Performance MetricsWe have seen how product innovation was perceived as the top priority for many technology organizations, superseding ethics concerns; one way in which the prioritization of “product” is oper-ationalized is through a company’s metrics. Ethics entrepreneurs often referred to “North Star” metrics and team sub-metrics, typically issued by analytics software compa-nies such as Mixpanel and others, as central to the negotiations around their work. Some product teams held the assumption that any intervention to improve fairness, for example, was going to“degrade” their models, make them less accurate, or affect their ability to meet revenue targets. In those cases, ethics entrepreneurs had to spend time “pre-emptively assuaging their fears” and make sure their suggestions did not “make things harder for that team.” Dave, who worked in Trust and Safety, said he would frequently encounter the concern over less accurate models but did not find that AI ethics suggestions actually resulted in degraded models in practice Many ethics entrepreneurs further described a pressure to provide quantitative evidence for their own ethics or responsible AI-related goals. Several said that they were currently focused on defining metrics for fairness or ethics, because that was the existing structure through which work was incentivized. Dave explained: No matter how much people want to do a good job,at the end of the day, quarterly goals will get in the way. Whether it’s revenue or monthly users that your job depends on. Early advice I got was, if you could ever get the CEO to say, ’Ok one of our North Star Metrics is related to ethics/fairness,’ that would have just changed everything. At the same time, multiple ethics entrepreneurs described their focus as working to determine “ethics” or “fairness” metrics in response to the question, “What is ethics and how well are you doing it?” However, many characterized this as an inherently difficult problem. While pursuing his PhD, James interned at a Big Tech Company whose business model was centered around engagement. He depicted his own role as measuring the merits of introducing friction to make their AI products more responsible: Our job was to find such concrete evidence of connec-ions between the platform and bad things happening in real life. . . that we could convincingly say, ‘You will lose more money. . . if you don’t throttle, than you will lose by throttling.’ And it’s really hard to have evi-dence that concrete, like predicting the future. Andso in my time, I don’t think I witnessed any while I Was working there.In this example, the North Star metrics around engagement were so highly prioritized that any recommendations to reduce those metrics required irrefutable quantitative evidence. At the same time, uncovering evidence of this kind necessitated data work that was not well supported by the existing data infrastructures at the company. As such, without this evidence, the default was to con-tinue to prioritize product innovation that increased engagement.Furthermore, much as Bromley and Powell had warned [3], ethics entrepreneurs were directed to spend time and resources measur-ing outcomes for a goal that might not be measurable, as opposed to spending that time implementing practices that were likely to further responsible AI goals.Relatedly, when ethics entrepreneurs introduced practitioners to fairness toolkits that would give a better sense of how the model performed for different demographic groups, they would react by asking for a specific target they needed to meet. One participant recounted his experience recommending the use of fairness toolk-its to engineers who would respond by asking him, “What’s the threshold? Is it 80%? 85%? You know, what should I do?” These Questions often did not have clear answers or legal standards and would require more philosophical inquiry at the foundation of what it meant for the model to be fair and ethical.4.4 The Problem with ""Reorgs""Multiple participants mentioned the frequency of reorganizations,or “reorgs,” which rearranged the structure of their teams and/or the teams they worked with. Some ethics entrepreneurs described it as a source of stress for product teams as well. Theoretically, reor-ganizations would support “Agile” product development, a popular management style for software engineering. As Posner explains,Agile was “designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit.The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary” [34]. Frequent reorganizations in this environment would indicate that the organizational structure is responsive to the shifting needs of the company and projects. James, the PhD student intern at a Big Tech company,sourced this philosophy of constant reorganizations as part of en-gineering and startup culture:The reshuffling and the reordering is supposedly, you know, company wide policy for continuous optimiza-tion and continuous iteration. . . . That top-down tech culture leads people to think that you could do con-stant A/B tests on your personnel and your corporate structure, just like you do constant A/B tests on your platform for user engagement checks, and just see what fits, what works better. And like every time you see an adjustment that you think might improve met-rics just a little bit, then you change just a little bit.James explained that his team had three different bosses over the course of his three-month internship: he was unable to access institutional knowledge that might have helped him do his job better and avoid repeating previously attempted projects. Impor-tantly, in the context of our other findings, it could also mean that relationships that ethics entrepreneurs had taken time to build and depended on for their work were disrupted, creating additional friction and inefficiency to implement ethics.4.5 The Individualization of Risk in AI Ethics Our study identifies several characteristics of technology companies that lead to ethics work being inconsistently taken into considera-tion by individuals. We find that ethical questions are the-prioritized product development cycles, disincentivized by metrics, and dis-rupted by constant reorganizations. These challenges come with important ramifications in terms of inequality and individual risk for workers invested in fairness, equity, and ethics.First, given that North Star metrics aligned teams within the company toward a common goal, individuals were likely to be evaluated on their contributions to those metrics rather than ethics-related objectives. As such, individuals’ promotions and careers are at stake as they try to meet their launch deadlines and improve metrics, as opposed to doing due diligence on ethics concerns that might stall a product. While evaluation and promotion based on metrics was one area of concern and could result in AI Ethics work being disincentivized, many ethics entrepreneurs described risks they felt more broadly as well. Some of these perceived risks were described above, but even ethics entrepreneurs who did agree to participate in our study voiced various hesitations about partici-pating, being audio recorded, being directly quoted, and otherwise being identified by their interviews in this publication, especially if they were still employed by technology companies in this field.Furthermore, participants described the acute personal risks and pressures they felt as they went about their day-to-day jobs. Sam commented, “Being very loud about putting more brakes on it was a risky thing to do. It was not built into the process” and noted that it required appealing to leadership. Likewise, Dave, described why he left his previous company, where he had started their AI ethics initiatives:I was going to be in a position where if something did[an issue surfaced close to or after launch], I would bethe one to see it and basically need to be the whistle-blower. And I realized there were really no protec-tions in place for me. And the stress level that was causing was not really being acknowledged. . . [Ethicsworkers] can be in a position of being put under unreasonable pressure to basically need to decide or at least be the one to deliver the bad news to teams. Like,‘Yeah, you should pull the plug on this before you launch it.’ . . . It’s just not acknowledging how much pressure that puts on people who are often not senior enough to be making those decisions.While Dave felt this pressure as a white man, many participants highlighted that marginalized people were often more likely to experience these adverse kinds of pressure. Ayesha, a woman of color, observed that “the people who are actually heavily invested in it as like—as a call to action, as a vocation, and as a means for. . . social change, are the ones who inhabit marginalized identities,”noting that experiencing discrimination themselves means that the work “hits differently.” She noted that she often felt she had to speak up “whenever like I thought that something terrible is happening, not just with the team, but the company at large.” Manyparticipants also referenced Timnit Gebru, a Black woman computer scientist who was employed at Google and whose departure from the company made headlines in the media, as evidence of the risk that people from marginalized backgrounds take on in this field[43]. The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult. Ayesha described some antagonism as “part of an engineering norm that says it’s okay to tear people down instead of provid-ing thoughtful criticism.” Some of the pushback she had received in professional spaces reminded her of “reddit threads that are really. . .white supremacist and incel-y.” She felt that because of the position she was in, she “couldn’t just shut it down immediately” and instead had to be “very thoughtful and diplomatic and graceful about it.”To summarize, the way ethics and responsibility principles are incorporated at many technology corporations puts the onus on individuals to take on work as opposed to relying on organizational processes which may produce more consistent outcomes. As a result, it puts stress and pressure on individuals who advocate at great personal risk to their own careers. This can be particularly damaging to people from marginalized backgrounds, who bring an underrepresented perspective and whose lived experiences with discrimination may compel them to speak up. Not only does this further harm already marginalized people by adding additional bur-dens and barriers to their success at work, it also has the potential to push them out of the organization entirely from sheer exhaustion or as the result of retaliation.",220-4,"The male-dominated, engineering-centric world of Silicon Valley May have made speaking on fairness and ethics related issues as someone from a marginalized background particularly difficult."
