,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{brewer2023envisioningequitable,
    author = {Robin N. Brewer and Christina N. Harrington and Courtney Heldreth},
    title = {Envisioning Equitable Speech Technologies for Black Older Adults},
    year = 2023
}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-09, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,Unfairness,Perceived_Problem,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",Unfairness,
10,LackOfTrainingDataDiversity,Artifact,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",a lack of diverse representation in speech training data,
11,RacialBiases,Causal_Theory,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",racial biases exist,
12,FearOfExclusion,Causal_Theory,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",expected that using AI technologies could lead to overuse or exclude communities,
13,LackOfCulutralKnowledge,Perceived_Need,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",certain cultural knowledge or ethnic knowledge tha we wouldn't expect,
14,VoiceTechnologies,Agent,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",voice technologies ,
15,CodeSwitching,Strategy,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",“code switches” or alternates between English that sounds more “white” than Black.,
16,PoorFeaturePerformance,Perceived_Problem,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",Current features that are intended to improve natural language processing did not work well ,
17,ExpectationOfDifferentialPerformance,Perceived_Problem,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",participants expect voice technologies to favor certain speech patterns over others.,
18,ConcernsOfOverreliance,Perceived_Problem,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",Concerns about reliance and dependence,
19,ConcernsAboutHumanOsbolescence,Perceived_Problem,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16).,
20,ConfusionAboutTechnology,Perceived_Problem,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",participants’ confusion about AI technology boundaries or how to describe A,
21,ConernAboutExclusion,Perceived_Problem,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",concerned that researchers would exclude Black people from the future of AI and voice technology design,
22,FemaleSpeakers,Agent,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",female speakers with accented speech.,
23,Participants,Agent,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",participants,
24,Misunderstanding,Artifact,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",misunderstand community-specific cultural and regional knowledge,
25, , , , , ,
26, , , , , ,
27,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
28,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
29,Unfairness,constrainsAgent,VoiceTechnologies,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",Most ASR and voice technologies have been deemed to be “unfair”
30,LackOfTrainingDataDiversity,influencesPrecept,Unfairness,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",Unfairness is usually attributed to a lack of diverse representation in speech training data
31,RacialBiases,constrainsAgent,FemaleSpeakers,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4","Findings confirm that racial biases exist, particularly for female speakers with accented speech."
32,FearOfExclusion,constrainsAgent,Participants,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",participants expected that using AI technologies could lead to overuse or exclude communities
33,LackOfCulutralKnowledge,constrainsAgent,VoiceTechnologies,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know
34,VoiceTechnologies,hasProducedArtifact,Misunderstanding,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",voice technologies to misunderstand community-specific cultural and regional knowledge
35,CodeSwitching,constrainsAgent,Participants,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",P12 described how she “code switches” or alternates between English that sounds more “white” than Black.
36,PoorFeaturePerformance,constrainsAgent,Participants,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",Current features that are intended to improve natural language processing did not work well for participants.
37,ExpectationOfDifferentialPerformance,constrainsAgent,Participants,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",participants expect voice technologies to favor certain speech patterns over others.
38,ConcernsOfOverreliance,constrainsAgent,Participants,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",participants echoed concerns about becoming overly reliant on AI technologies
39,ConcernsAboutHumanOsbolescence,constrainsAgent,Participants,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16).
40,ConfusionAboutTechnology,constrainsAgent,Participants,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI.
41,ConernAboutExclusion,constrainsAgent,Participants,"Most ASR and voice technologies have been deemed to be “unfair” simply because they do not operate in equal ways for all users[39]. These systems have higher error rates for certain groups. Unfairness is usually attributed to a lack of diverse representation in speech training data, which could be an ethical and a legal concern[17]. Researchers agree that demographic group affiliation should not change the accuracy of ASR. Prior work uses a counterfactual fairness approach where researchers trained ASR systems to achieve“equivalent output label distributions” for speakers whose voices had different protected attributes [39]. Systems are often trained on datasets that are not inclusive of variations of English. Most datasets include “American English”or “Standard American English”, but few have considered American English (AAE) or African American Vernacular English (AAVE) [33]. In one study, researchers tested whether a novel attacker could exploit racial and gender biases in ASR, considering Standard American English, Korean, and Nigerian with varying genders [47]. Findings confirm that racial biases exist, particularly for female speakers with accented speech. Other fairness literature confirms gender speech bias in [17]. Much of this work explores fairness by measuring accuracy to calculate disparity, or manip-ulating datasets to weight underrepresented data or collect more data [36]. Most of these findings highlight gender and racial bias but neglect age. In this paper, we take a sociotechnical approach to voice technology bias, particularly at the intersection of race and age. [...] During workshops, participants described how they expected to interact with voice technologies based on their current design. Inmuch of this discussion, they did not expect voice technologies to understand cultural knowledge or their voices. At a higher level, participants expected that using AI technologies could lead to overuse or exclude communities, engaging in discussions about community boundaries and equitable participation in voice and AI technology design. During the design workshops, several participants (n = 5) described how they did not expect the current state of voice technologies to understand cultural knowledge. One participant explicitly stated, “there’s certain cultural knowledge or ethnic knowledge or something that we wouldn’t necessarily expect a voice assistant to know” (P1). This knowledge included information about holidays important to the African American community, common sayings, and regional knowledge. In describing a scenario where a user might ask for cultural information, one participant details how she envisions the system responding:“In our scenario, we assumed—we decided that Alexa might not know all the cultural stuff. . . She asked, “Whatis Juneteenth?” Alexa said, “I did not understand. Could you please rephrase?” She hollered, “What is Juneteenth?”Alexa said, “Juneteenth is the name of a Richard Wrightnovel...Is Juneteenth a national holiday? Then Alexa  could figure it out.” (P1)P1 acknowledged how voice technologies can answer factual questions common to mainstream audiences, such as information about books. However, she did not expect voice technologies to know “all the cultural stuff ” related to a holiday celebrating Black heritage such as Juneteenth, which might include its historical significance or celebratory traditions. She attributed this to community size, saying that voice technologies are “not programmed to know, simply because they are niche information for smaller communities”(P1). Implicitly, this statement reveals an assumption that voice and AI technologies only know information for historically represented communities.Similarly, other participants (n = 5) described how voice technologies might not have appropriate regional knowledge to answer questions about locations or local holiday celebrations. For example,one participant described the storyboard scenario they created:“Our question was, she asked, “How large is Bell Isle?”I’m like, “How do you spell that?” Then she said, “I-S-L-E.” Then I said, “How long is Bell Island?” Then she was like, “No, Isle. I-S-L-E.” It was kind of like when you’re hearing “isle” and then “island,” playing off those kind of words [sic] (P9)”“Because it’s so used to the common I-S-L-A-N-D. (P12)”Here, P9 and P12 show how they expected voice technologies to understand certain location-related information based on word popularity (isle vs. island). In these quotes, participants describe how words and phrases that are important to their community maybe ignored by other communities. As such, they expected voice technologies to misunderstand community-specific cultural and regional knowledge. Often, participants described how the technologies did not understand their voice (n = 9). These types of interactions led to participants expecting that the voice assistant would not understand them with their initial voice query.For example, P11 described an experience with needing to rephrase a query several times for deleting a song from a playlist. She was unsure why her voice assistant did not understand her initial query.In another example, P12 felt confident that ethnicity affected how the system understood her voice:“When I speak to my voice assistant, I’m like maybeI’m using too much Ebonics. Let me speak using myking’s English. Let me use my white girl voice. ‘I said Iwould like to—’ sometimes you have to be specific and articulate with it. Your eloquence of speech matters it seems because oftentimes, it’ll say, ‘I don’t understand what you said.’ I’m like, ‘Maybe you don’t like the wayI’m talking.’”Similar to prior work [23], P12 described how she “code switches” or alternates between English that sounds more “white” than Black.This quote suggests that there is an expectation that white-soundingvoices are understood better than Black-sounding voices. Voice technologies often have training processes where the user can fine-tune speech recognition according to specific voice features. Current features that are intended to improve natural language processing did not work well for participants. Two participants were aware of these training features. For example:“It doesn’t recognize every voice. The cell phone even tells you to train your voice to the phone. I did get a notification on this device that told me, ‘Train your voice to the phone.’ I know I did that when I got the phone, but it asked me to do it again, so I did it again. (P13)” These quotes show that participants expect voice technologies to favor certain speech patterns over others. While it may be unclear what type of speech leads to better responses, others suspect eth-nicity can play a role. Moreover, current strategies for mitigating speech recognition errors have yet to work well for participants. These findings raise questions about how to mitigate speech-related bias and other forms of bias with AI technologies. As voice technologies are one type of AI technology, we also engaged participants in higher-level discussions about AI expectations. Their discussions unpacked concerns about historically underrepresented communities being excluded from AI systems and design processes, which could nega-tively impact data representation. Several participants echoed concerns about becoming overly reliant on AI technologies (n = 7). Concerns about reliance and dependence were part of larger conversations about humans becoming “obsolete” (P2) and AI technologies feeling “impersonal” (P16). For example, P4 asked, “What is the need for humans? Because artificial intelligence becomes [sic] so human, do they need us?” Although researchers continue to develop models that can better mimic human behavior and speech (e.g., with natural language processing), participants were concerned that machines would replace humans. Other participants such as P13 envisioned a more robotic world with the future of AI, saying, “Now what they got to do is puta chip in us. And everywhere we go, you can scan the chip, scan yourself.” To participants, AI is, and will continue to be, inauthentic .The workshop coordinators observed how such conversations reflected participants’ confusion about AI technology boundaries or how to describe AI. This confusion connected with P16’s concerns when she said, “My concern is that everything is going to artificial identification or artificial intelligence. And I am just so afraid that a lot of blacks are going to be left behind.” During the design workshops,3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design. For example, P5 wanted “more people from diverse backgrounds having input into the programming process” as this could mitigate exclusion and lead to more equitable voice and AI futures. These participants did not want developers and researchers to exclude underrepresented communities.","380-1, 383-4",3 participants were concerned that researchers would exclude Black people from the future of AI and voice technology design
