,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{ma2023yousounddepressed,
    author = {Anna Ma and Elizabeth Patitsas and Jonathan Sterne},
    title = {You Sound Depressed: A Case Study on Sonde Health’s Diagnostic Use of Voice Analysis AI},
    year = 2023
}
", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-12, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,VoiceAnalyisAI,Agent,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,voice analysis-based artificial intelligence (AI) for monitoring mental health,
10,Researchers,Agent,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,we,
11,TextAnalysisOfDocumentation,Artifact,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,"a textual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression",
12,ComplexPsychometrics,Agent,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,complex psychometrics ,
13,DepressionDetection,Perceived_Problem,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,"for monitoring mental health, such as depression detection",
14,SocietalContext,Causal_Theory,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,the societal consequences of such technologie,
15,CriticalDisabilityLens,Causal_Theory,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,"Using a critical disability studies lens,",
16,FallacyOfFlattening,Causal_Theory,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,fallacy that complex psychometrics can be meaningfully flattened into a single encompassing score,
17,EugenicsLegacy,Causal_Theory,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,the legacy of eugenics,
18, , , , , ,
19, , , , , ,
20, , , , , ,
21, , , , , ,
22,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
23,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
24,DepressionDetection,constrainsAgent,VoiceAnalyisAI,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,"voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection"
25,SocietalContext,constrainsAgent,VoiceAnalyisAI,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,"However, insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems."
26,CriticalDisabilityLens,constrainsAgent,Researchers,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,"Using a critical disability studies lens, we"
27,Researchers,hasProducedArtifact,TextAnalysisOfDocumentation,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,"we conducted a textual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression."
28,FallacyOfFlattening,constrainsAgent,ComplexPsychometrics,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,fallacy that complex psychometrics can be meaningfully flattened into a single encompassing score
29,EugenicsLegacy,constrainsAgent,VoiceAnalyisAI,"There is growing interest within the medical sector about the diag-nostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However,insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study ofSonde Health, a Boston-based startup that purports to offer “objec-tive” depression detection and monitoring via its Mental Fitness App that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a tex-tual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizingSonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychomet-rics can be meaningfully flattened into a single encompassing score,the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care",639,We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care.
