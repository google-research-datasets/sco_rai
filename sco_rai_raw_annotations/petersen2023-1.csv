,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{petersen2023, author = {Petersen, Eike and Ganz, Melanie and Holm, Sune and Feragen, Aasa},
title = {On (Assessing) the Fairness of Risk Score Models},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,ProvisionOfSimilarEpistemicValue,Perceived_Problem,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such deci-sions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk Models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and mean-ingful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings –in two case studies, one in recidivism risk prediction, and one risk of major depressive disorder (MDD) prediction [...] We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia. We predict the risk of recidivism within five years after completion of the educa-tional program, based on subject age at time of the crime and at time of program completion, sex, area of origin, province of residence,the number of prior criminal records, the province in which the ed-ucational program was executed, crime category and specific type,and the type of educational program. We use the pre processing described by Fuglsang-Damgaard and Zinck [30], available online.8The processed dataset contains 4652 samples (34% recidivism rate),and we use 70% / 10% / 20% for the training / validation / test sets,stratifying on the recidivism outcome. We consider subject sex, age group at time of crime, area of origin, as well as the province of residence as sensitive variables for our fairness analyses, and we use a minimum test set group size of 100 for our fairness analyses,leaving a total of 41 groups to be analyzed.Figure 3 shows the results of our analyses. Overall model per-formance is reasonable (AUROC > 0.7, DRMSCE < 0.05) given the limited amount of input information. As expected, metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible.","817, 822",the provision of similar epistemic valu,
10,FairRiskScores,Artifact,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such deci-sions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk Models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and mean-ingful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings –in two case studies, one in recidivism risk prediction, and one risk of major depressive disorder (MDD) prediction [...] We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia. We predict the risk of recidivism within five years after completion of the educa-tional program, based on subject age at time of the crime and at time of program completion, sex, area of origin, province of residence,the number of prior criminal records, the province in which the ed-ucational program was executed, crime category and specific type,and the type of educational program. We use the pre processing described by Fuglsang-Damgaard and Zinck [30], available online.8The processed dataset contains 4652 samples (34% recidivism rate),and we use 70% / 10% / 20% for the training / validation / test sets,stratifying on the recidivism outcome. We consider subject sex, age group at time of crime, area of origin, as well as the province of residence as sensitive variables for our fairness analyses, and we use a minimum test set group size of 100 for our fairness analyses,leaving a total of 41 groups to be analyzed.Figure 3 shows the results of our analyses. Overall model per-formance is reasonable (AUROC > 0.7, DRMSCE < 0.05) given the limited amount of input information. As expected, metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible.","817, 822",fair risk scores,
11,CatalonianJuveniles,Agent,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such deci-sions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk Models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and mean-ingful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings –in two case studies, one in recidivism risk prediction, and one risk of major depressive disorder (MDD) prediction [...] We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia. We predict the risk of recidivism within five years after completion of the educa-tional program, based on subject age at time of the crime and at time of program completion, sex, area of origin, province of residence,the number of prior criminal records, the province in which the ed-ucational program was executed, crime category and specific type,and the type of educational program. We use the pre processing described by Fuglsang-Damgaard and Zinck [30], available online.8The processed dataset contains 4652 samples (34% recidivism rate),and we use 70% / 10% / 20% for the training / validation / test sets,stratifying on the recidivism outcome. We consider subject sex, age group at time of crime, area of origin, as well as the province of residence as sensitive variables for our fairness analyses, and we use a minimum test set group size of 100 for our fairness analyses,leaving a total of 41 groups to be analyzed.Figure 3 shows the results of our analyses. Overall model per-formance is reasonable (AUROC > 0.7, DRMSCE < 0.05) given the limited amount of input information. As expected, metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible.","817, 822",juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia,
12,MetricUncertainty,Perceived_Problem,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such deci-sions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk Models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and mean-ingful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings –in two case studies, one in recidivism risk prediction, and one risk of major depressive disorder (MDD) prediction [...] We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia. We predict the risk of recidivism within five years after completion of the educa-tional program, based on subject age at time of the crime and at time of program completion, sex, area of origin, province of residence,the number of prior criminal records, the province in which the ed-ucational program was executed, crime category and specific type,and the type of educational program. We use the pre processing described by Fuglsang-Damgaard and Zinck [30], available online.8The processed dataset contains 4652 samples (34% recidivism rate),and we use 70% / 10% / 20% for the training / validation / test sets,stratifying on the recidivism outcome. We consider subject sex, age group at time of crime, area of origin, as well as the province of residence as sensitive variables for our fairness analyses, and we use a minimum test set group size of 100 for our fairness analyses,leaving a total of 41 groups to be analyzed.Figure 3 shows the results of our analyses. Overall model per-formance is reasonable (AUROC > 0.7, DRMSCE < 0.05) given the limited amount of input information. As expected, metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible.","817, 822",metric uncertainty,
13,UnfairRiskBasedRankings,Perceived_Need,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such deci-sions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk Models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and mean-ingful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings –in two case studies, one in recidivism risk prediction, and one risk of major depressive disorder (MDD) prediction [...] We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia. We predict the risk of recidivism within five years after completion of the educa-tional program, based on subject age at time of the crime and at time of program completion, sex, area of origin, province of residence,the number of prior criminal records, the province in which the ed-ucational program was executed, crime category and specific type,and the type of educational program. We use the pre processing described by Fuglsang-Damgaard and Zinck [30], available online.8The processed dataset contains 4652 samples (34% recidivism rate),and we use 70% / 10% / 20% for the training / validation / test sets,stratifying on the recidivism outcome. We consider subject sex, age group at time of crime, area of origin, as well as the province of residence as sensitive variables for our fairness analyses, and we use a minimum test set group size of 100 for our fairness analyses,leaving a total of 41 groups to be analyzed.Figure 3 shows the results of our analyses. Overall model per-formance is reasonable (AUROC > 0.7, DRMSCE < 0.05) given the limited amount of input information. As expected, metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible.","817, 822",unfair risk-based rankings,
14,RecidivismDatset,Artifact,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such deci-sions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk Models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and mean-ingful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings –in two case studies, one in recidivism risk prediction, and one risk of major depressive disorder (MDD) prediction [...] We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia. We predict the risk of recidivism within five years after completion of the educa-tional program, based on subject age at time of the crime and at time of program completion, sex, area of origin, province of residence,the number of prior criminal records, the province in which the ed-ucational program was executed, crime category and specific type,and the type of educational program. We use the pre processing described by Fuglsang-Damgaard and Zinck [30], available online.8The processed dataset contains 4652 samples (34% recidivism rate),and we use 70% / 10% / 20% for the training / validation / test sets,stratifying on the recidivism outcome. We consider subject sex, age group at time of crime, area of origin, as well as the province of residence as sensitive variables for our fairness analyses, and we use a minimum test set group size of 100 for our fairness analyses,leaving a total of 41 groups to be analyzed.Figure 3 shows the results of our analyses. Overall model per-formance is reasonable (AUROC > 0.7, DRMSCE < 0.05) given the limited amount of input information. As expected, metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible.","817, 822",a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,
15,TestGroups,Agent,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such deci-sions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk Models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and mean-ingful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings –in two case studies, one in recidivism risk prediction, and one risk of major depressive disorder (MDD) prediction [...] We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia. We predict the risk of recidivism within five years after completion of the educa-tional program, based on subject age at time of the crime and at time of program completion, sex, area of origin, province of residence,the number of prior criminal records, the province in which the ed-ucational program was executed, crime category and specific type,and the type of educational program. We use the pre processing described by Fuglsang-Damgaard and Zinck [30], available online.8The processed dataset contains 4652 samples (34% recidivism rate),and we use 70% / 10% / 20% for the training / validation / test sets,stratifying on the recidivism outcome. We consider subject sex, age group at time of crime, area of origin, as well as the province of residence as sensitive variables for our fairness analyses, and we use a minimum test set group size of 100 for our fairness analyses,leaving a total of 41 groups to be analyzed.Figure 3 shows the results of our analyses. Overall model per-formance is reasonable (AUROC > 0.7, DRMSCE < 0.05) given the limited amount of input information. As expected, metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible.","817, 822",small test groups,
16, , , , , ,
17,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
18,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
19,FairRiskScores,reflectsPrecept,ProvisionOfSimilarEpistemicValue,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such deci-sions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk Models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and mean-ingful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings –in two case studies, one in recidivism risk prediction, and one risk of major depressive disorder (MDD) prediction [...] We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia. We predict the risk of recidivism within five years after completion of the educa-tional program, based on subject age at time of the crime and at time of program completion, sex, area of origin, province of residence,the number of prior criminal records, the province in which the ed-ucational program was executed, crime category and specific type,and the type of educational program. We use the pre processing described by Fuglsang-Damgaard and Zinck [30], available online.8The processed dataset contains 4652 samples (34% recidivism rate),and we use 70% / 10% / 20% for the training / validation / test sets,stratifying on the recidivism outcome. We consider subject sex, age group at time of crime, area of origin, as well as the province of residence as sensitive variables for our fairness analyses, and we use a minimum test set group size of 100 for our fairness analyses,leaving a total of 41 groups to be analyzed.Figure 3 shows the results of our analyses. Overall model per-formance is reasonable (AUROC > 0.7, DRMSCE < 0.05) given the limited amount of input information. As expected, metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible.","817, 822",We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness
20,FairRiskScores,influencesPrecept,UnfairRiskBasedRankings,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such deci-sions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk Models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and mean-ingful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings –in two case studies, one in recidivism risk prediction, and one risk of major depressive disorder (MDD) prediction [...] We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia. We predict the risk of recidivism within five years after completion of the educa-tional program, based on subject age at time of the crime and at time of program completion, sex, area of origin, province of residence,the number of prior criminal records, the province in which the ed-ucational program was executed, crime category and specific type,and the type of educational program. We use the pre processing described by Fuglsang-Damgaard and Zinck [30], available online.8The processed dataset contains 4652 samples (34% recidivism rate),and we use 70% / 10% / 20% for the training / validation / test sets,stratifying on the recidivism outcome. We consider subject sex, age group at time of crime, area of origin, as well as the province of residence as sensitive variables for our fairness analyses, and we use a minimum test set group size of 100 for our fairness analyses,leaving a total of 41 groups to be analyzed.Figure 3 shows the results of our analyses. Overall model per-formance is reasonable (AUROC > 0.7, DRMSCE < 0.05) given the limited amount of input information. As expected, metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible.","817, 822",we show how even fair risk scores can lead to unfair risk-based rankings.
21,CatalonianJuveniles,hasProducedArtifact,RecidivismDatset,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such deci-sions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk Models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and mean-ingful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings –in two case studies, one in recidivism risk prediction, and one risk of major depressive disorder (MDD) prediction [...] We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia. We predict the risk of recidivism within five years after completion of the educa-tional program, based on subject age at time of the crime and at time of program completion, sex, area of origin, province of residence,the number of prior criminal records, the province in which the ed-ucational program was executed, crime category and specific type,and the type of educational program. We use the pre processing described by Fuglsang-Damgaard and Zinck [30], available online.8The processed dataset contains 4652 samples (34% recidivism rate),and we use 70% / 10% / 20% for the training / validation / test sets,stratifying on the recidivism outcome. We consider subject sex, age group at time of crime, area of origin, as well as the province of residence as sensitive variables for our fairness analyses, and we use a minimum test set group size of 100 for our fairness analyses,leaving a total of 41 groups to be analyzed.Figure 3 shows the results of our analyses. Overall model per-formance is reasonable (AUROC > 0.7, DRMSCE < 0.05) given the limited amount of input information. As expected, metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible.","817, 822","We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia"
22,MetricUncertainty,constrainsAgent,TestGroups,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such deci-sions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk Models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and mean-ingful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings –in two case studies, one in recidivism risk prediction, and one risk of major depressive disorder (MDD) prediction [...] We use a dataset on juvenile recidivism provided by the Centre forLegal Studies and Specialized Training (CEJFE) within the Depart-ment of Justice of the Government of Catalonia,7 analyses of which have been previously published by Tolan et al. [91] and Fuglsang-Damgaard and Zinck [30]. The dataset provides information about juvenile subjects who have participated in an educational program following a conviction of a criminal act in Catalonia. We predict the risk of recidivism within five years after completion of the educa-tional program, based on subject age at time of the crime and at time of program completion, sex, area of origin, province of residence,the number of prior criminal records, the province in which the ed-ucational program was executed, crime category and specific type,and the type of educational program. We use the pre processing described by Fuglsang-Damgaard and Zinck [30], available online.8The processed dataset contains 4652 samples (34% recidivism rate),and we use 70% / 10% / 20% for the training / validation / test sets,stratifying on the recidivism outcome. We consider subject sex, age group at time of crime, area of origin, as well as the province of residence as sensitive variables for our fairness analyses, and we use a minimum test set group size of 100 for our fairness analyses,leaving a total of 41 groups to be analyzed.Figure 3 shows the results of our analyses. Overall model per-formance is reasonable (AUROC > 0.7, DRMSCE < 0.05) given the limited amount of input information. As expected, metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible.","817, 822","metric uncertainty in small test groups is very high, making reliable statements about the fairness or unfairness of the model near-impossible."
